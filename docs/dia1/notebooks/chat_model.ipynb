{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cb0be8",
   "metadata": {},
   "source": [
    "# Prompting y Langchain básico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c79b73",
   "metadata": {},
   "source": [
    "Importación y instalación de los paquetes requeridos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5afca013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import pprint\n",
    "from typing import List\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2172d63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d58cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-openai langchain-groq langchain-community langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d57f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b61bac",
   "metadata": {},
   "source": [
    "## Chat simple basado en LLM\n",
    "\n",
    "Para Groq, crear una cuenta y un API KEY at https://groq.com/ \n",
    "\n",
    "Lee el nombre del modelo desde la variable de entorno, crea una instancia de ChatOpenAI con temperatura baja (menos aleatoriedad), envía un prompt directo al LLM y muestra la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d9942cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moonshotai/kimi-k2-instruct\n",
      "Why don’t data scientists ever get invited to parties?\n",
      "\n",
      "Because they always bring their own *standard deviations* and kill the *mean* mood!\n"
     ]
    }
   ],
   "source": [
    "llm_model = os.environ[\"OPENAI_MODEL\"]\n",
    "llm_model = \"moonshotai/kimi-k2-instruct\"\n",
    "print(llm_model)\n",
    "\n",
    "llm = ChatOpenAI(model=llm_model, temperature=0.1)\n",
    "llm = ChatGroq(model=llm_model, temperature=0.1)\n",
    "\n",
    "response = llm.invoke(\"Tell me a joke about data scientists\")\n",
    "print(response.text()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be39da",
   "metadata": {},
   "source": [
    "Los chat models se componen de roles y mensajes intercalados. \n",
    "\n",
    "Crea un template con un SystemMessage fijo y un MessagesPlaceholder llamado \"messages\", y luego encadena ese template con el LLM. Al invocar la cadena, se pasa un diccionario con la clave \"messages\" y una lista de objetos de mensaje (HumanMessage, AIMessage, ...). El resultado ai_msg es un objeto de mensaje que se muestra como respuesta.\n",
    "\n",
    "El orden y tipo de mensajes importa (contexto, historial, ejemplos). La variable definida en el placeholder debe coincidir luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e869ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I said: “J’adore la programmation,” which is French for “I love programming.”\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm # Chaining the prompt and the llm call\n",
    "\n",
    "ai_msg = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(ai_msg.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1baea",
   "metadata": {},
   "source": [
    "Este es un ejemplo de una chain *zero-shot*, porque el LLM contesta en base a su pre-entrenamiento.\n",
    "\n",
    "Se crea un chat con un mensaje de sistema fijo (SYSTEM_PROMPT) y un template que espera una variable \"question\" en el rol \"human\". ChatPromptTemplate.from_messages() acepta distintas representaciones de mensajes y devuelve un template para componer con un LLM.\n",
    "\n",
    "La qa_chain, al invocarse, llenará las variables del prompt, ejecutará el modelo y devolverá la salida combinada. Al final, se renderizala salida como Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c64fd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Proxy Design Pattern – Quick-Reference Card  \n",
       "(what it is: a surrogate / placeholder that controls access to another object)\n",
       "\n",
       "-------------------------------------------------\n",
       "PROS – why you reach for it\n",
       "-------------------------------------------------\n",
       "1. Transparent protection  \n",
       "   Client code talks to the Proxy through the exact same interface it would use for the real subject → no changes in calling code.\n",
       "\n",
       "2. Lazy (virtual) instantiation  \n",
       "   Heavy or network objects are created only when the first real method is invoked → faster start-up and lower memory footprint.\n",
       "\n",
       "3. Security gatekeeper  \n",
       "   You can centralise permission checks in the Proxy instead of scattering them through business code.\n",
       "\n",
       "4. Remote communication stub (RMI, gRPC, SOAP, …)  \n",
       "   The Proxy serialises the call and hides all the plumbing (marshalling, retries, network failures).\n",
       "\n",
       "5. Thread-safe or cached access  \n",
       "   Synchronisation, pooling, result caching, request coalescing, circuit-breaker, etc. can be added without touching the real object.\n",
       "\n",
       "6. Instrumentation / AOP hook  \n",
       "   Add logging, metrics, transaction boundaries, profiling, throttling, … in one place.\n",
       "\n",
       "7. Future-proofing & hot swapping  \n",
       "   Replace the real subject with a different implementation (mock, decorator, cloud service) at run-time.\n",
       "\n",
       "-------------------------------------------------\n",
       "CONS – where it can bite you\n",
       "-------------------------------------------------\n",
       "1. One more abstraction layer  \n",
       "   Extra classes/interfaces → steeper learning curve for newcomers, more code to navigate.\n",
       "\n",
       "2. Performance tax  \n",
       "   Every call goes through an indirection; for very fine-grained objects (e.g., graph nodes, game entities) the overhead can dominate.\n",
       "\n",
       "3. Lifecycle complexity  \n",
       "   You now have two objects to manage: proxy and real subject.  \n",
       "   – Who creates whom?  \n",
       "   – Who owns the expensive resources?  \n",
       "   – When can the real subject be disposed?  \n",
       "   Reference-counting or weak references may be needed.\n",
       "\n",
       "4. Concurrency pitfalls  \n",
       "   If the proxy is shared between threads, its own state (e.g., cached reference to the real subject) must be thread-safe, duplicating synchronisation effort.\n",
       "\n",
       "5. Debugging pain  \n",
       "   Stack traces are deeper; breakpoints in the proxy can hide the real culprit; logging may show “proxy” instead of the actual class name unless you toString() carefully.\n",
       "\n",
       "6. Interface coupling  \n",
       "   If the real subject’s interface evolves you must update the proxy (and all specialised proxies: virtual, remote, cache, security, …).  \n",
       "   Dynamic proxies (java.lang.reflect.Proxy, CGLib, ByteBuddy) mitigate this but add byte-code magic that not every team is comfortable with.\n",
       "\n",
       "7. Over-engineering risk  \n",
       "   A simple “if” in the service layer is sometimes enough; wrapping every repository call in a proxy can turn into a maze of tiny classes.\n",
       "\n",
       "-------------------------------------------------\n",
       "Rule-of-thumb\n",
       "-------------------------------------------------\n",
       "Use Proxy when the cost of creating/accessing the real object is high, or when you need to add a systemic, cross-cutting behaviour (security, caching, remoting) that must stay orthogonal to business logic.  \n",
       "Avoid it when the object is cheap, calls are extremely frequent, or the added indirection obscures more than it helps."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an experienced software architect that assists a novice developer \n",
    "to design a system. \"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", SYSTEM_PROMPT),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    ")\n",
    "\n",
    "qa_chain = qa_prompt | llm\n",
    "\n",
    "query = \"What are the pros and cons of the Proxy design pattern?\"\n",
    "result = qa_chain.invoke(input=query)\n",
    "# print(result.content)\n",
    "\n",
    "display(Markdown(result.content)) # Result in Markdown format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071cf33",
   "metadata": {},
   "source": [
    "## Gestión de Memoria (como parte de un chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa52eff",
   "metadata": {},
   "source": [
    "La memoria es una lista de (pares de) mensajes previos entre el humano y la IA, que proporciona *contexto* para la siguiente interacción.\n",
    "\n",
    "La cadena define un prompt de reescritura para convertir una pregunta dependiente del historial en una pregunta independiente: se busca crear un template con un SystemMessage y un MessagesPlaceholder \"chat_history\", y luego componerla con el LLM, sumando además un StrOutputParser para la salida.\n",
    "\n",
    "MessagesPlaceholder espera explícitamente una lista de mensajes en la entrada.\n",
    "\n",
    "ChatMessageHistory actúa como buffer. Pasar chat_history.messages al invoke() proporciona la estructura que MessagesPlaceholder espera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ead4946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes—combine the Proxy pattern with other patterns whenever you need to layer responsibilities that don’t belong in the real subject. Typical pairings:\n",
      "\n",
      "- Decorator – Proxy adds control/access; Decorator adds behaviour. You can wrap a Proxy around a Decorator (or vice-versa) to get both caching and, say, compression.  \n",
      "- Adapter – Use an Adapter inside a Remote Proxy to translate the real object’s interface into the one clients expect over the wire.  \n",
      "- Flyweight – A Virtual Proxy can postpone creation of the heavy flyweight object until it’s actually needed.  \n",
      "- Singleton – A Proxy can be the single entry point that lazily creates and guards the only instance.  \n",
      "- Observer – A Protection Proxy can filter or veto subscription requests; a Remote Proxy can re-broadcast events across JVMs.  \n",
      "- Factory / Abstract Factory – Instantiate the correct Proxy subclass (CachingProxy, SecurityProxy, …) without clients knowing.  \n",
      "- Command – A Remote Proxy can turn each method call into a Command object that is queued, logged, or undone.  \n",
      "- Bridge – Use a Proxy as the implementation-side reference so the abstraction never knows whether it’s talking to a local or remote implementation.  \n",
      "- Cache-Aside / Repository – A Caching Proxy sits in front of the repository to intercept and serve repeated queries.  \n",
      "- Circuit-Breaker – Wrap the Proxy with (or implement it as) a circuit-breaker to fail fast when the real service is down.\n",
      "\n",
      "In short, Proxy is rarely used alone; it’s a glue pattern that plays well with almost any other pattern when you need transparent control or access mediation.\n"
     ]
    }
   ],
   "source": [
    "CONTEXTUALIZED_PROMPT = \"\"\"Given a chat history and the latest developer's question\n",
    "    which might reference context in the chat history, formulate a standalone question\n",
    "    that can be understood without the chat history. Do NOT answer the question,\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualized_qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", CONTEXTUALIZED_PROMPT),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# This is a possible chain to keep (and compress) past interactions\n",
    "# It's a form of rewriting\n",
    "contextualized_qa_chain = contextualized_qa_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# A buffer to store messages from user and assistant\n",
    "chat_history = ChatMessageHistory()\n",
    "chat_history.add_user_message(query)\n",
    "chat_history.add_ai_message(result)\n",
    "\n",
    "\n",
    "query = \"Can I combine the pattern with other patterns?\"\n",
    "ai_msg = contextualized_qa_chain.invoke(\n",
    "    {\n",
    "        'question': query, \n",
    "        'chat_history': chat_history.messages\n",
    "    }\n",
    ")\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09ce73",
   "metadata": {},
   "source": [
    "Y ahora se puede usar la pregunta contextualizada y que el LLM responda la pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a198b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Exactly.  \n",
       "Think of the Proxy as the “on-ramp” to the object; once traffic is flowing through that on-ramp you can stack as many extra lanes (concerns) as you need without touching the original object.  \n",
       "Below are the most common combinations you’ll run into, the problem each one solves, and a minimal code sketch so you can see the seams.\n",
       "\n",
       "-------------------------------------------------\n",
       "1. Proxy + Caching (Flyweight or simple Map)\n",
       "-------------------------------------------------\n",
       "Problem: The real service is CPU- or I/O-heavy and the same arguments are requested over and over.\n",
       "\n",
       "Key idea: Let the proxy hold a lightweight cache; only delegate to the real object on a miss.\n",
       "\n",
       "Java example (interface already exists):\n",
       "\n",
       "public interface DataService { String fetch(String key); }\n",
       "\n",
       "public class CachingProxy implements DataService {\n",
       "    private final DataService real;\n",
       "    private final Map<String,String> cache = new ConcurrentHashMap<>();\n",
       "\n",
       "    public CachingProxy(DataService real) { this.real = real; }\n",
       "\n",
       "    @Override\n",
       "    public String fetch(String key) {\n",
       "        return cache.computeIfAbsent(key, real::fetch);\n",
       "    }\n",
       "}\n",
       "\n",
       "Usage:\n",
       "DataService data = new CachingProxy(new RemoteDataService());\n",
       "\n",
       "-------------------------------------------------\n",
       "2. Proxy + Security (Protection Proxy)\n",
       "-------------------------------------------------\n",
       "Problem: You must check roles/permissions before every call.\n",
       "\n",
       "Key idea: Intercept the call, ask an AccessManager, throw if denied, forward if allowed.\n",
       "\n",
       "public class SecureProxy implements DataService {\n",
       "    private final DataService real;\n",
       "    private final AccessManager access;\n",
       "\n",
       "    public SecureProxy(DataService real, AccessManager access) {\n",
       "        this.real = real; this.access = access;\n",
       "    }\n",
       "\n",
       "    @Override\n",
       "    public String fetch(String key) {\n",
       "        if (!access.canRead(key))\n",
       "            throw new ForbiddenException();\n",
       "        return real.fetch(key);\n",
       "    }\n",
       "}\n",
       "\n",
       "-------------------------------------------------\n",
       "3. Proxy + Remote Communication (Remote Proxy / Adapter)\n",
       "-------------------------------------------------\n",
       "Problem: Real object lives on another process/machine.\n",
       "\n",
       "Key idea: Proxy hides serialization, network stubs, retries, circuit-breaker, etc.\n",
       "\n",
       "Spring example (RMI or HTTP):\n",
       "\n",
       "@FeignClient(name = \"data-service\")\n",
       "public interface RemoteDataService extends DataService {}\n",
       "\n",
       "Spring generates the proxy at runtime; you just autowire DataService and never notice the network.\n",
       "\n",
       "-------------------------------------------------\n",
       "4. Proxy + Lazy Loading (Virtual Proxy)\n",
       "-------------------------------------------------\n",
       "Problem: Object graph is expensive to create/load.\n",
       "\n",
       "Key idea: Proxy holds a reference but doesn’t materialize the real object until the first method call.\n",
       "\n",
       "public class VirtualProxy implements HeavyObject {\n",
       "    private HeavyObject real;   // null until needed\n",
       "    @Override\n",
       "    public void heavyOp() {\n",
       "        if (real == null) {\n",
       "            real = new HeavyObjectImpl(); // costly\n",
       "        }\n",
       "        real.heavyOp();\n",
       "    }\n",
       "}\n",
       "\n",
       "-------------------------------------------------\n",
       "5. Proxy + Metrics / Logging (Decorator flavour)\n",
       "-------------------------------------------------\n",
       "Problem: Ops wants latency counters, logs, tracing.\n",
       "\n",
       "Key idea: Proxy wraps and times, then delegates.\n",
       "\n",
       "public class MetricsProxy implements DataService {\n",
       "    private final DataService real;\n",
       "    private final MeterRegistry registry;\n",
       "\n",
       "    @Override\n",
       "    public String fetch(String key) {\n",
       "        return registry.timer(\"ds.fetch\", \"key\", key)\n",
       "                       .recordCallable(() -> real.fetch(key));\n",
       "    }\n",
       "}\n",
       "\n",
       "-------------------------------------------------\n",
       "6. Chaining them together\n",
       "-------------------------------------------------\n",
       "Because every proxy implements the same interface you can stack:\n",
       "\n",
       "DataService service =\n",
       "    new MetricsProxy(\n",
       "        new SecureProxy(\n",
       "            new CachingProxy(\n",
       "                new RemoteDataService())));\n",
       "\n",
       "The call order becomes:\n",
       "Metrics → Security → Cache → Network.\n",
       "\n",
       "-------------------------------------------------\n",
       "7. When to stop\n",
       "-------------------------------------------------\n",
       "- Keep the interface stable; every new concern is a new wrapper.  \n",
       "- If the stack becomes hard to reason about, switch to a single interceptor pipeline (e.g., Spring AOP, Jakarta interceptors, .NET DispatchProxy) so ordering is explicit.  \n",
       "- Document the chain in README or a diagram—future you will thank you.\n",
       "\n",
       "-------------------------------------------------\n",
       "Rule of thumb\n",
       "-------------------------------------------------\n",
       "“Add a new proxy when the concern is orthogonal to business logic and you can express it as ‘before/after’ the real call.”  \n",
       "Everything else probably belongs inside the service itself."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def contextualized_question(input: dict):\n",
    "        if input.get(\"chat_history\"):\n",
    "            return contextualized_qa_chain\n",
    "        else:\n",
    "            return input[\"question\"]\n",
    "\n",
    "# A new QA chain that reuses the previous chain\n",
    "qa_chain_with_memory = (\n",
    "         RunnablePassthrough.assign(\n",
    "            context=contextualized_question | qa_prompt | llm\n",
    "        )\n",
    "    )\n",
    "\n",
    "result = qa_chain_with_memory.invoke(\n",
    "    {\n",
    "        'question': query,  \n",
    "        'chat_history': chat_history.messages\n",
    "    }\n",
    ")\n",
    "\n",
    "display(Markdown(result['context'].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86e91488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat_history': [HumanMessage(content='What are the pros and cons of the Proxy design pattern?', additional_kwargs={}, response_metadata={}),\n",
      "                  AIMessage(content='Proxy Design Pattern – Quick-Reference Card  \\n(what it is: a surrogate / placeholder that controls access to another object)\\n\\n-------------------------------------------------\\nPROS – why you reach for it\\n-------------------------------------------------\\n1. Transparent protection  \\n   Client code talks to the Proxy through the exact same interface it would use for the real subject → no changes in calling code.\\n\\n2. Lazy (virtual) instantiation  \\n   Heavy or network objects are created only when the first real method is invoked → faster start-up and lower memory footprint.\\n\\n3. Security gatekeeper  \\n   You can centralise permission checks in the Proxy instead of scattering them through business code.\\n\\n4. Remote communication stub (RMI, gRPC, SOAP, …)  \\n   The Proxy serialises the call and hides all the plumbing (marshalling, retries, network failures).\\n\\n5. Thread-safe or cached access  \\n   Synchronisation, pooling, result caching, request coalescing, circuit-breaker, etc. can be added without touching the real object.\\n\\n6. Instrumentation / AOP hook  \\n   Add logging, metrics, transaction boundaries, profiling, throttling, … in one place.\\n\\n7. Future-proofing & hot swapping  \\n   Replace the real subject with a different implementation (mock, decorator, cloud service) at run-time.\\n\\n-------------------------------------------------\\nCONS – where it can bite you\\n-------------------------------------------------\\n1. One more abstraction layer  \\n   Extra classes/interfaces → steeper learning curve for newcomers, more code to navigate.\\n\\n2. Performance tax  \\n   Every call goes through an indirection; for very fine-grained objects (e.g., graph nodes, game entities) the overhead can dominate.\\n\\n3. Lifecycle complexity  \\n   You now have two objects to manage: proxy and real subject.  \\n   – Who creates whom?  \\n   – Who owns the expensive resources?  \\n   – When can the real subject be disposed?  \\n   Reference-counting or weak references may be needed.\\n\\n4. Concurrency pitfalls  \\n   If the proxy is shared between threads, its own state (e.g., cached reference to the real subject) must be thread-safe, duplicating synchronisation effort.\\n\\n5. Debugging pain  \\n   Stack traces are deeper; breakpoints in the proxy can hide the real culprit; logging may show “proxy” instead of the actual class name unless you toString() carefully.\\n\\n6. Interface coupling  \\n   If the real subject’s interface evolves you must update the proxy (and all specialised proxies: virtual, remote, cache, security, …).  \\n   Dynamic proxies (java.lang.reflect.Proxy, CGLib, ByteBuddy) mitigate this but add byte-code magic that not every team is comfortable with.\\n\\n7. Over-engineering risk  \\n   A simple “if” in the service layer is sometimes enough; wrapping every repository call in a proxy can turn into a maze of tiny classes.\\n\\n-------------------------------------------------\\nRule-of-thumb\\n-------------------------------------------------\\nUse Proxy when the cost of creating/accessing the real object is high, or when you need to add a systemic, cross-cutting behaviour (security, caching, remoting) that must stay orthogonal to business logic.  \\nAvoid it when the object is cheap, calls are extremely frequent, or the added indirection obscures more than it helps.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 650, 'prompt_tokens': 41, 'total_tokens': 691, 'completion_time': 3.682818728, 'prompt_time': 0.010490069, 'queue_time': 0.1472479, 'total_time': 3.6933087970000003}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_5fe129dff6', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--81f9d70e-3e20-4a3e-919e-29e0d9cf940c-0', usage_metadata={'input_tokens': 41, 'output_tokens': 650, 'total_tokens': 691})],\n",
      " 'context': AIMessage(content='Exactly.  \\nThink of the Proxy as the “on-ramp” to the object; once traffic is flowing through that on-ramp you can stack as many extra lanes (concerns) as you need without touching the original object.  \\nBelow are the most common combinations you’ll run into, the problem each one solves, and a minimal code sketch so you can see the seams.\\n\\n-------------------------------------------------\\n1. Proxy + Caching (Flyweight or simple Map)\\n-------------------------------------------------\\nProblem: The real service is CPU- or I/O-heavy and the same arguments are requested over and over.\\n\\nKey idea: Let the proxy hold a lightweight cache; only delegate to the real object on a miss.\\n\\nJava example (interface already exists):\\n\\npublic interface DataService { String fetch(String key); }\\n\\npublic class CachingProxy implements DataService {\\n    private final DataService real;\\n    private final Map<String,String> cache = new ConcurrentHashMap<>();\\n\\n    public CachingProxy(DataService real) { this.real = real; }\\n\\n    @Override\\n    public String fetch(String key) {\\n        return cache.computeIfAbsent(key, real::fetch);\\n    }\\n}\\n\\nUsage:\\nDataService data = new CachingProxy(new RemoteDataService());\\n\\n-------------------------------------------------\\n2. Proxy + Security (Protection Proxy)\\n-------------------------------------------------\\nProblem: You must check roles/permissions before every call.\\n\\nKey idea: Intercept the call, ask an AccessManager, throw if denied, forward if allowed.\\n\\npublic class SecureProxy implements DataService {\\n    private final DataService real;\\n    private final AccessManager access;\\n\\n    public SecureProxy(DataService real, AccessManager access) {\\n        this.real = real; this.access = access;\\n    }\\n\\n    @Override\\n    public String fetch(String key) {\\n        if (!access.canRead(key))\\n            throw new ForbiddenException();\\n        return real.fetch(key);\\n    }\\n}\\n\\n-------------------------------------------------\\n3. Proxy + Remote Communication (Remote Proxy / Adapter)\\n-------------------------------------------------\\nProblem: Real object lives on another process/machine.\\n\\nKey idea: Proxy hides serialization, network stubs, retries, circuit-breaker, etc.\\n\\nSpring example (RMI or HTTP):\\n\\n@FeignClient(name = \"data-service\")\\npublic interface RemoteDataService extends DataService {}\\n\\nSpring generates the proxy at runtime; you just autowire DataService and never notice the network.\\n\\n-------------------------------------------------\\n4. Proxy + Lazy Loading (Virtual Proxy)\\n-------------------------------------------------\\nProblem: Object graph is expensive to create/load.\\n\\nKey idea: Proxy holds a reference but doesn’t materialize the real object until the first method call.\\n\\npublic class VirtualProxy implements HeavyObject {\\n    private HeavyObject real;   // null until needed\\n    @Override\\n    public void heavyOp() {\\n        if (real == null) {\\n            real = new HeavyObjectImpl(); // costly\\n        }\\n        real.heavyOp();\\n    }\\n}\\n\\n-------------------------------------------------\\n5. Proxy + Metrics / Logging (Decorator flavour)\\n-------------------------------------------------\\nProblem: Ops wants latency counters, logs, tracing.\\n\\nKey idea: Proxy wraps and times, then delegates.\\n\\npublic class MetricsProxy implements DataService {\\n    private final DataService real;\\n    private final MeterRegistry registry;\\n\\n    @Override\\n    public String fetch(String key) {\\n        return registry.timer(\"ds.fetch\", \"key\", key)\\n                       .recordCallable(() -> real.fetch(key));\\n    }\\n}\\n\\n-------------------------------------------------\\n6. Chaining them together\\n-------------------------------------------------\\nBecause every proxy implements the same interface you can stack:\\n\\nDataService service =\\n    new MetricsProxy(\\n        new SecureProxy(\\n            new CachingProxy(\\n                new RemoteDataService())));\\n\\nThe call order becomes:\\nMetrics → Security → Cache → Network.\\n\\n-------------------------------------------------\\n7. When to stop\\n-------------------------------------------------\\n- Keep the interface stable; every new concern is a new wrapper.  \\n- If the stack becomes hard to reason about, switch to a single interceptor pipeline (e.g., Spring AOP, Jakarta interceptors, .NET DispatchProxy) so ordering is explicit.  \\n- Document the chain in README or a diagram—future you will thank you.\\n\\n-------------------------------------------------\\nRule of thumb\\n-------------------------------------------------\\n“Add a new proxy when the concern is orthogonal to business logic and you can express it as ‘before/after’ the real call.”  \\nEverything else probably belongs inside the service itself.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 855, 'prompt_tokens': 64, 'total_tokens': 919, 'completion_time': 3.461216384, 'prompt_time': 0.01227876, 'queue_time': 0.147729928, 'total_time': 3.473495144}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_5fe129dff6', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ce44a953-a544-4fcc-ad65-01a4427b54c2-0', usage_metadata={'input_tokens': 64, 'output_tokens': 855, 'total_tokens': 919}),\n",
      " 'question': 'Can I combine the pattern with other patterns?'}\n"
     ]
    }
   ],
   "source": [
    "# How the whole result looks like\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5df46e",
   "metadata": {},
   "source": [
    "## Few-shots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd3c1a",
   "metadata": {},
   "source": [
    "Crea un template que espera una variable llamada \"input\" y contiene un prompt que pide el antónimo y una explicación. Luego se arma un chain que compone el prompt con los valores de entrada y lo envía al model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a463766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonym:  \n",
      "“I am overjoyed and utterly without hope.”\n",
      "\n",
      "Explanation:  \n",
      "The original sentence pairs deep sadness (“very sad”) with the retention of hope. Its opposite must reverse both emotional valence and outlook. “Overjoyed” is the direct affective opposite of “very sad,” while “utterly without hope” negates the residual optimism. The conjunction “and” replaces “but” because the two reversed states now align in the same negative-positive direction, forming a coherent antithetical state.\n"
     ]
    }
   ],
   "source": [
    "# Template inicial\n",
    "zero_shot_prompt = PromptTemplate(\n",
    "    input_variables=['input'], validate_template=True,\n",
    "    template=\"\"\"Return the antonym of the input given along with an explanation.\n",
    "    Input: {input}\n",
    "    Output:\n",
    "    Explanation:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Zero-shot chain (por ahora)\n",
    "zero_shot_chain = zero_shot_prompt | llm\n",
    "\n",
    "query = 'I am very sad but still have hope'\n",
    "result = zero_shot_chain.invoke(input=query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5945e6b",
   "metadata": {},
   "source": [
    "Ahora se supone que hay una base de ejemplos de antonimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7305a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos de tareas para crear antonimos\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d7944",
   "metadata": {},
   "source": [
    "Se configura un flujo few‑shot que selecciona dinámicamente ejemplos semánticamente similares y renderiza un prompt listo para enviar al LLM. Se usan los embeddings (de OpenAI, u otros) para convertir ejemplos a vectores y luego Chroma como based de almacenamiento. El parametro k=1 indica que se devolverá 1 ejemplo cercano por consulta.\n",
    "\n",
    "Al ejecutar el FewShotsPromptTemplate, el selector busca el ejemplo semánticamente más cercano, y lo inserta para generar el prompt final, que debiera usarse con el LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a839c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return the antonym of the given input along with an explanation. \n",
      "\n",
      "Example(s):\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: cloudy\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings, #OpenAIEmbeddings(),\n",
    "    Chroma, # The database to store the examples with their embeddings\n",
    "    # The number of examples to produce.\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Return the antonym of the given input along with an explanation. \\n\\nExample(s):\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "print(similar_prompt.format(adjective=\"cloudy\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8f9b4",
   "metadata": {},
   "source": [
    "Y solo resta usar el prompt en una chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb4ec565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return the antonym of the given input along with an explanation. \n",
      "\n",
      "Example(s):\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: rainy\n",
      "Output:\n",
      "\n",
      "Output: sunny  \n",
      "Explanation: “Rainy” describes weather characterized by rain, while “sunny” describes weather with clear skies and sunshine—essentially the opposite condition.\n"
     ]
    }
   ],
   "source": [
    "# Few-shot chain\n",
    "few_shot_chain = similar_prompt | llm\n",
    "\n",
    "query = 'rainy' # 'I am very sad but still have hope'\n",
    "print(similar_prompt.format(adjective=query))\n",
    "print()\n",
    "\n",
    "result = few_shot_chain.invoke(input=query)\n",
    "print(result.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed851dcd",
   "metadata": {},
   "source": [
    "Alternativamente, se le puede pedir al LLM que genere un objeto JSON como respuesta (salida estructurada) via Pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c22ca2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FormattedAntonym(antonym='sunny', explanation='The antonym of \"rainy\" is \"sunny\" because \"rainy\" describes weather characterized by rain and overcast skies, whereas \"sunny\" describes clear skies and sunshine, representing opposite weather conditions.', additional_clarifications=[])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descripción de la estructura de salida usando Pydantic\n",
    "class FormattedAntonym(BaseModel):\n",
    "    antonym: str = Field(description=\"An antonym for the input word or phrase.\", default=\"\")\n",
    "    explanation: str = Field(description=\"A short explanation of how the antonym was generated\")\n",
    "    additional_clarifications: List[str] = Field(description=\"A list of questions that the LLM needs to clarify in order to ...\", default=[])\n",
    "\n",
    "\n",
    "# Wrapper para la salida estructurada\n",
    "llm_with_structure = llm.with_structured_output(FormattedAntonym)\n",
    "\n",
    "few_shot_chain1 = similar_prompt | llm_with_structure\n",
    "result = few_shot_chain1.invoke(input=query)\n",
    "result # The Pydantic (object) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16621a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sunny'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.antonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "651e9c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_clarifications': [],\n",
      " 'antonym': 'sunny',\n",
      " 'explanation': 'The antonym of \"rainy\" is \"sunny\" because \"rainy\" describes '\n",
      "                'weather characterized by rain and overcast skies, whereas '\n",
      "                '\"sunny\" describes clear skies and sunshine, representing '\n",
      "                'opposite weather conditions.'}\n"
     ]
    }
   ],
   "source": [
    "# print(result.model_dump_json(indent=2))\n",
    "pprint.pprint(result.model_dump()) # This is a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca44fb",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_desde_cero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
