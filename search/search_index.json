{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenidos al Curso!","text":""},{"location":"#contenidos-del-curso","title":"Contenidos del Curso","text":"<p>Clases D\u00eda 1</p><p>Fundamentos y primeros bloques</p> <p>Clases D\u00eda 2</p><p>Workflows, Agentes y LangGraph</p> <p>Clases D\u00eda 3</p><p>Protocolos y desaf\u00edos avanzados</p>"},{"location":"__init__/","title":"init","text":""},{"location":"dia1/clases/","title":"Fundamentos y primeros bloques","text":""},{"location":"dia1/notebooks/chat_model/","title":"Prompting y Langchain b\u00e1sico","text":"<p>Importaci\u00f3n y instalaci\u00f3n de los paquetes requeridos</p> In\u00a0[1]: Copied! <pre>import os\nimport json\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nimport pprint\nfrom typing import List\n\nfrom IPython.display import display, Markdown\n</pre> import os import json from pathlib import Path from dotenv import load_dotenv import pprint from typing import List  from IPython.display import display, Markdown In\u00a0[2]: Copied! <pre># Load environment variables from .env file\nload_dotenv()\n</pre> # Load environment variables from .env file load_dotenv() Out[2]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre># %pip install langchain langchain-openai langchain-groq langchain-community langchain-chroma\n</pre> # %pip install langchain langchain-openai langchain-groq langchain-community langchain-chroma In\u00a0[3]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom langchain_groq import ChatGroq\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.memory import ChatMessageHistory\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.example_selectors import SemanticSimilarityExampleSelector\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom pydantic import BaseModel, Field\n</pre> from langchain_openai import ChatOpenAI from langchain_groq import ChatGroq from langchain_core.messages import AIMessage, HumanMessage, SystemMessage from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.output_parsers import StrOutputParser from langchain.memory import ChatMessageHistory from langchain_core.runnables import RunnablePassthrough from langchain_core.example_selectors import SemanticSimilarityExampleSelector from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate from langchain_openai import OpenAIEmbeddings from langchain_huggingface import HuggingFaceEmbeddings from langchain_chroma import Chroma from pydantic import BaseModel, Field  In\u00a0[36]: Copied! <pre>llm_model = os.environ[\"OPENAI_MODEL\"]\nllm_model = \"moonshotai/kimi-k2-instruct\"\nprint(llm_model)\n\nllm = ChatOpenAI(model=llm_model, temperature=0.1)\nllm = ChatGroq(model=llm_model, temperature=0.1)\n\nresponse = llm.invoke(\"Tell me a joke about data scientists\")\nprint(response.text()) \n</pre> llm_model = os.environ[\"OPENAI_MODEL\"] llm_model = \"moonshotai/kimi-k2-instruct\" print(llm_model)  llm = ChatOpenAI(model=llm_model, temperature=0.1) llm = ChatGroq(model=llm_model, temperature=0.1)  response = llm.invoke(\"Tell me a joke about data scientists\") print(response.text())  <pre>moonshotai/kimi-k2-instruct\nWhy don\u2019t data scientists ever get invited to parties?\n\nBecause they always bring their own *standard deviations* and kill the *mean* mood!\n</pre> <p>Los chat models se componen de roles y mensajes intercalados.</p> <p>Crea un template con un SystemMessage fijo y un MessagesPlaceholder llamado \"messages\", y luego encadena ese template con el LLM. Al invocar la cadena, se pasa un diccionario con la clave \"messages\" y una lista de objetos de mensaje (HumanMessage, AIMessage, ...). El resultado ai_msg es un objeto de mensaje que se muestra como respuesta.</p> <p>El orden y tipo de mensajes importa (contexto, historial, ejemplos). La variable definida en el placeholder debe coincidir luego.</p> In\u00a0[37]: Copied! <pre>prompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | llm # Chaining the prompt and the llm call\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.text())\n</pre> prompt = ChatPromptTemplate.from_messages(     [         SystemMessage(             content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"         ),         MessagesPlaceholder(variable_name=\"messages\"),     ] )  chain = prompt | llm # Chaining the prompt and the llm call  ai_msg = chain.invoke(     {         \"messages\": [             HumanMessage(                 content=\"Translate from English to French: I love programming.\"             ),             AIMessage(content=\"J'adore la programmation.\"),             HumanMessage(content=\"What did you just say?\"),         ],     } ) print(ai_msg.text()) <pre>I said: \u201cJ\u2019adore la programmation,\u201d which is French for \u201cI love programming.\u201d\n</pre> <p>Este es un ejemplo de una chain zero-shot, porque el LLM contesta en base a su pre-entrenamiento.</p> <p>Se crea un chat con un mensaje de sistema fijo (SYSTEM_PROMPT) y un template que espera una variable \"question\" en el rol \"human\". ChatPromptTemplate.from_messages() acepta distintas representaciones de mensajes y devuelve un template para componer con un LLM.</p> <p>La qa_chain, al invocarse, llenar\u00e1 las variables del prompt, ejecutar\u00e1 el modelo y devolver\u00e1 la salida combinada. Al final, se renderizala salida como Markdown.</p> In\u00a0[38]: Copied! <pre>SYSTEM_PROMPT = \"\"\"You are an experienced software architect that assists a novice developer \nto design a system. \"\"\"\n\nqa_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", SYSTEM_PROMPT),\n            (\"human\", \"{question}\"),\n        ]\n)\n\nqa_chain = qa_prompt | llm\n\nquery = \"What are the pros and cons of the Proxy design pattern?\"\nresult = qa_chain.invoke(input=query)\n#\u00a0print(result.content)\n\ndisplay(Markdown(result.content)) # Result in Markdown format\n</pre> SYSTEM_PROMPT = \"\"\"You are an experienced software architect that assists a novice developer  to design a system. \"\"\"  qa_prompt = ChatPromptTemplate.from_messages(         [             (\"system\", SYSTEM_PROMPT),             (\"human\", \"{question}\"),         ] )  qa_chain = qa_prompt | llm  query = \"What are the pros and cons of the Proxy design pattern?\" result = qa_chain.invoke(input=query) #\u00a0print(result.content)  display(Markdown(result.content)) # Result in Markdown format  <p>Proxy Design Pattern \u2013 Quick-Reference Card (what it is: a surrogate / placeholder that controls access to another object)</p> <p>La memoria es una lista de (pares de) mensajes previos entre el humano y la IA, que proporciona contexto para la siguiente interacci\u00f3n.</p> <p>La cadena define un prompt de reescritura para convertir una pregunta dependiente del historial en una pregunta independiente: se busca crear un template con un SystemMessage y un MessagesPlaceholder \"chat_history\", y luego componerla con el LLM, sumando adem\u00e1s un StrOutputParser para la salida.</p> <p>MessagesPlaceholder espera expl\u00edcitamente una lista de mensajes en la entrada.</p> <p>ChatMessageHistory act\u00faa como buffer. Pasar chat_history.messages al invoke() proporciona la estructura que MessagesPlaceholder espera.</p> In\u00a0[39]: Copied! <pre>CONTEXTUALIZED_PROMPT = \"\"\"Given a chat history and the latest developer's question\n    which might reference context in the chat history, formulate a standalone question\n    that can be understood without the chat history. Do NOT answer the question,\n    just reformulate it if needed and otherwise return it as is.\"\"\"\n\ncontextualized_qa_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", CONTEXTUALIZED_PROMPT),\n            MessagesPlaceholder(variable_name=\"chat_history\"),\n            (\"human\", \"{question}\"),\n        ]\n    )\n\n# This is a possible chain to keep (and compress) past interactions\n# It's a form of rewriting\ncontextualized_qa_chain = contextualized_qa_prompt | llm | StrOutputParser()\n\n\n# A buffer to store messages from user and assistant\nchat_history = ChatMessageHistory()\nchat_history.add_user_message(query)\nchat_history.add_ai_message(result)\n\n\nquery = \"Can I combine the pattern with other patterns?\"\nai_msg = contextualized_qa_chain.invoke(\n    {\n        'question': query, \n        'chat_history': chat_history.messages\n    }\n)\nprint(ai_msg)\n</pre> CONTEXTUALIZED_PROMPT = \"\"\"Given a chat history and the latest developer's question     which might reference context in the chat history, formulate a standalone question     that can be understood without the chat history. Do NOT answer the question,     just reformulate it if needed and otherwise return it as is.\"\"\"  contextualized_qa_prompt = ChatPromptTemplate.from_messages(         [             (\"system\", CONTEXTUALIZED_PROMPT),             MessagesPlaceholder(variable_name=\"chat_history\"),             (\"human\", \"{question}\"),         ]     )  # This is a possible chain to keep (and compress) past interactions # It's a form of rewriting contextualized_qa_chain = contextualized_qa_prompt | llm | StrOutputParser()   # A buffer to store messages from user and assistant chat_history = ChatMessageHistory() chat_history.add_user_message(query) chat_history.add_ai_message(result)   query = \"Can I combine the pattern with other patterns?\" ai_msg = contextualized_qa_chain.invoke(     {         'question': query,          'chat_history': chat_history.messages     } ) print(ai_msg) <pre>Yes\u2014combine the Proxy pattern with other patterns whenever you need to layer responsibilities that don\u2019t belong in the real subject. Typical pairings:\n\n- Decorator \u2013 Proxy adds control/access; Decorator adds behaviour. You can wrap a Proxy around a Decorator (or vice-versa) to get both caching and, say, compression.  \n- Adapter \u2013 Use an Adapter inside a Remote Proxy to translate the real object\u2019s interface into the one clients expect over the wire.  \n- Flyweight \u2013 A Virtual Proxy can postpone creation of the heavy flyweight object until it\u2019s actually needed.  \n- Singleton \u2013 A Proxy can be the single entry point that lazily creates and guards the only instance.  \n- Observer \u2013 A Protection Proxy can filter or veto subscription requests; a Remote Proxy can re-broadcast events across JVMs.  \n- Factory / Abstract Factory \u2013 Instantiate the correct Proxy subclass (CachingProxy, SecurityProxy, \u2026) without clients knowing.  \n- Command \u2013 A Remote Proxy can turn each method call into a Command object that is queued, logged, or undone.  \n- Bridge \u2013 Use a Proxy as the implementation-side reference so the abstraction never knows whether it\u2019s talking to a local or remote implementation.  \n- Cache-Aside / Repository \u2013 A Caching Proxy sits in front of the repository to intercept and serve repeated queries.  \n- Circuit-Breaker \u2013 Wrap the Proxy with (or implement it as) a circuit-breaker to fail fast when the real service is down.\n\nIn short, Proxy is rarely used alone; it\u2019s a glue pattern that plays well with almost any other pattern when you need transparent control or access mediation.\n</pre> <p>Y ahora se puede usar la pregunta contextualizada y que el LLM responda la pregunta.</p> In\u00a0[40]: Copied! <pre>def contextualized_question(input: dict):\n        if input.get(\"chat_history\"):\n            return contextualized_qa_chain\n        else:\n            return input[\"question\"]\n\n# A new QA chain that reuses the previous chain\nqa_chain_with_memory = (\n         RunnablePassthrough.assign(\n            context=contextualized_question | qa_prompt | llm\n        )\n    )\n\nresult = qa_chain_with_memory.invoke(\n    {\n        'question': query,  \n        'chat_history': chat_history.messages\n    }\n)\n\ndisplay(Markdown(result['context'].content))\n</pre> def contextualized_question(input: dict):         if input.get(\"chat_history\"):             return contextualized_qa_chain         else:             return input[\"question\"]  # A new QA chain that reuses the previous chain qa_chain_with_memory = (          RunnablePassthrough.assign(             context=contextualized_question | qa_prompt | llm         )     )  result = qa_chain_with_memory.invoke(     {         'question': query,           'chat_history': chat_history.messages     } )  display(Markdown(result['context'].content)) <p>Exactly. Think of the Proxy as the \u201con-ramp\u201d to the object; once traffic is flowing through that on-ramp you can stack as many extra lanes (concerns) as you need without touching the original object. Below are the most common combinations you\u2019ll run into, the problem each one solves, and a minimal code sketch so you can see the seams.</p> <ol> <li>Proxy + Caching (Flyweight or simple Map)</li> </ol> <p>Problem: The real service is CPU- or I/O-heavy and the same arguments are requested over and over.</p> <p>Key idea: Let the proxy hold a lightweight cache; only delegate to the real object on a miss.</p> <p>Java example (interface already exists):</p> <p>public interface DataService { String fetch(String key); }</p> <p>public class CachingProxy implements DataService { private final DataService real; private final Map&lt;String,String&gt; cache = new ConcurrentHashMap&lt;&gt;();</p> <pre><code>public CachingProxy(DataService real) { this.real = real; }\n\n@Override\npublic String fetch(String key) {\n    return cache.computeIfAbsent(key, real::fetch);\n}</code></pre> <p>}</p> <p>Usage: DataService data = new CachingProxy(new RemoteDataService());</p> <ol> <li>Proxy + Security (Protection Proxy)</li> </ol> <p>Problem: You must check roles/permissions before every call.</p> <p>Key idea: Intercept the call, ask an AccessManager, throw if denied, forward if allowed.</p> <p>public class SecureProxy implements DataService { private final DataService real; private final AccessManager access;</p> <pre><code>public SecureProxy(DataService real, AccessManager access) {\n    this.real = real; this.access = access;\n}\n\n@Override\npublic String fetch(String key) {\n    if (!access.canRead(key))\n        throw new ForbiddenException();\n    return real.fetch(key);\n}</code></pre> <p>}</p> <ol> <li>Proxy + Remote Communication (Remote Proxy / Adapter)</li> </ol> <p>Problem: Real object lives on another process/machine.</p> <p>Key idea: Proxy hides serialization, network stubs, retries, circuit-breaker, etc.</p> <p>Spring example (RMI or HTTP):</p> <p>@FeignClient(name = \"data-service\") public interface RemoteDataService extends DataService {}</p> <p>Spring generates the proxy at runtime; you just autowire DataService and never notice the network.</p> <ol> <li>Proxy + Lazy Loading (Virtual Proxy)</li> </ol> <p>Problem: Object graph is expensive to create/load.</p> <p>Key idea: Proxy holds a reference but doesn\u2019t materialize the real object until the first method call.</p> <p>public class VirtualProxy implements HeavyObject { private HeavyObject real;   // null until needed @Override public void heavyOp() { if (real == null) { real = new HeavyObjectImpl(); // costly } real.heavyOp(); } }</p> <ol> <li>Proxy + Metrics / Logging (Decorator flavour)</li> </ol> <p>Problem: Ops wants latency counters, logs, tracing.</p> <p>Key idea: Proxy wraps and times, then delegates.</p> <p>public class MetricsProxy implements DataService { private final DataService real; private final MeterRegistry registry;</p> <pre><code>@Override\npublic String fetch(String key) {\n    return registry.timer(\"ds.fetch\", \"key\", key)\n                   .recordCallable(() -&gt; real.fetch(key));\n}</code></pre> <p>}</p> <ol> <li>Chaining them together</li> </ol> <p>Because every proxy implements the same interface you can stack:</p> <p>DataService service = new MetricsProxy( new SecureProxy( new CachingProxy( new RemoteDataService())));</p> <p>The call order becomes: Metrics \u2192 Security \u2192 Cache \u2192 Network.</p> <ol> <li>When to stop</li> </ol> <ul> <li>Keep the interface stable; every new concern is a new wrapper.</li> <li>If the stack becomes hard to reason about, switch to a single interceptor pipeline (e.g., Spring AOP, Jakarta interceptors, .NET DispatchProxy) so ordering is explicit.</li> <li>Document the chain in README or a diagram\u2014future you will thank you.</li> </ul> In\u00a0[41]: Copied! <pre># How the whole result looks like\npprint.pprint(result)\n</pre> # How the whole result looks like pprint.pprint(result) <pre>{'chat_history': [HumanMessage(content='What are the pros and cons of the Proxy design pattern?', additional_kwargs={}, response_metadata={}),\n                  AIMessage(content='Proxy Design Pattern \u2013 Quick-Reference Card  \\n(what it is: a surrogate / placeholder that controls access to another object)\\n\\n-------------------------------------------------\\nPROS \u2013 why you reach for it\\n-------------------------------------------------\\n1. Transparent protection  \\n   Client code talks to the Proxy through the exact same interface it would use for the real subject \u2192 no changes in calling code.\\n\\n2. Lazy (virtual) instantiation  \\n   Heavy or network objects are created only when the first real method is invoked \u2192 faster start-up and lower memory footprint.\\n\\n3. Security gatekeeper  \\n   You can centralise permission checks in the Proxy instead of scattering them through business code.\\n\\n4. Remote communication stub (RMI, gRPC, SOAP, \u2026)  \\n   The Proxy serialises the call and hides all the plumbing (marshalling, retries, network failures).\\n\\n5. Thread-safe or cached access  \\n   Synchronisation, pooling, result caching, request coalescing, circuit-breaker, etc. can be added without touching the real object.\\n\\n6. Instrumentation / AOP hook  \\n   Add logging, metrics, transaction boundaries, profiling, throttling, \u2026 in one place.\\n\\n7. Future-proofing &amp; hot swapping  \\n   Replace the real subject with a different implementation (mock, decorator, cloud service) at run-time.\\n\\n-------------------------------------------------\\nCONS \u2013 where it can bite you\\n-------------------------------------------------\\n1. One more abstraction layer  \\n   Extra classes/interfaces \u2192 steeper learning curve for newcomers, more code to navigate.\\n\\n2. Performance tax  \\n   Every call goes through an indirection; for very fine-grained objects (e.g., graph nodes, game entities) the overhead can dominate.\\n\\n3. Lifecycle complexity  \\n   You now have two objects to manage: proxy and real subject.  \\n   \u2013 Who creates whom?  \\n   \u2013 Who owns the expensive resources?  \\n   \u2013 When can the real subject be disposed?  \\n   Reference-counting or weak references may be needed.\\n\\n4. Concurrency pitfalls  \\n   If the proxy is shared between threads, its own state (e.g., cached reference to the real subject) must be thread-safe, duplicating synchronisation effort.\\n\\n5. Debugging pain  \\n   Stack traces are deeper; breakpoints in the proxy can hide the real culprit; logging may show \u201cproxy\u201d instead of the actual class name unless you toString() carefully.\\n\\n6. Interface coupling  \\n   If the real subject\u2019s interface evolves you must update the proxy (and all specialised proxies: virtual, remote, cache, security, \u2026).  \\n   Dynamic proxies (java.lang.reflect.Proxy, CGLib, ByteBuddy) mitigate this but add byte-code magic that not every team is comfortable with.\\n\\n7. Over-engineering risk  \\n   A simple \u201cif\u201d in the service layer is sometimes enough; wrapping every repository call in a proxy can turn into a maze of tiny classes.\\n\\n-------------------------------------------------\\nRule-of-thumb\\n-------------------------------------------------\\nUse Proxy when the cost of creating/accessing the real object is high, or when you need to add a systemic, cross-cutting behaviour (security, caching, remoting) that must stay orthogonal to business logic.  \\nAvoid it when the object is cheap, calls are extremely frequent, or the added indirection obscures more than it helps.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 650, 'prompt_tokens': 41, 'total_tokens': 691, 'completion_time': 3.682818728, 'prompt_time': 0.010490069, 'queue_time': 0.1472479, 'total_time': 3.6933087970000003}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_5fe129dff6', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--81f9d70e-3e20-4a3e-919e-29e0d9cf940c-0', usage_metadata={'input_tokens': 41, 'output_tokens': 650, 'total_tokens': 691})],\n 'context': AIMessage(content='Exactly.  \\nThink of the Proxy as the \u201con-ramp\u201d to the object; once traffic is flowing through that on-ramp you can stack as many extra lanes (concerns) as you need without touching the original object.  \\nBelow are the most common combinations you\u2019ll run into, the problem each one solves, and a minimal code sketch so you can see the seams.\\n\\n-------------------------------------------------\\n1. Proxy + Caching (Flyweight or simple Map)\\n-------------------------------------------------\\nProblem: The real service is CPU- or I/O-heavy and the same arguments are requested over and over.\\n\\nKey idea: Let the proxy hold a lightweight cache; only delegate to the real object on a miss.\\n\\nJava example (interface already exists):\\n\\npublic interface DataService { String fetch(String key); }\\n\\npublic class CachingProxy implements DataService {\\n    private final DataService real;\\n    private final Map&lt;String,String&gt; cache = new ConcurrentHashMap&lt;&gt;();\\n\\n    public CachingProxy(DataService real) { this.real = real; }\\n\\n    @Override\\n    public String fetch(String key) {\\n        return cache.computeIfAbsent(key, real::fetch);\\n    }\\n}\\n\\nUsage:\\nDataService data = new CachingProxy(new RemoteDataService());\\n\\n-------------------------------------------------\\n2. Proxy + Security (Protection Proxy)\\n-------------------------------------------------\\nProblem: You must check roles/permissions before every call.\\n\\nKey idea: Intercept the call, ask an AccessManager, throw if denied, forward if allowed.\\n\\npublic class SecureProxy implements DataService {\\n    private final DataService real;\\n    private final AccessManager access;\\n\\n    public SecureProxy(DataService real, AccessManager access) {\\n        this.real = real; this.access = access;\\n    }\\n\\n    @Override\\n    public String fetch(String key) {\\n        if (!access.canRead(key))\\n            throw new ForbiddenException();\\n        return real.fetch(key);\\n    }\\n}\\n\\n-------------------------------------------------\\n3. Proxy + Remote Communication (Remote Proxy / Adapter)\\n-------------------------------------------------\\nProblem: Real object lives on another process/machine.\\n\\nKey idea: Proxy hides serialization, network stubs, retries, circuit-breaker, etc.\\n\\nSpring example (RMI or HTTP):\\n\\n@FeignClient(name = \"data-service\")\\npublic interface RemoteDataService extends DataService {}\\n\\nSpring generates the proxy at runtime; you just autowire DataService and never notice the network.\\n\\n-------------------------------------------------\\n4. Proxy + Lazy Loading (Virtual Proxy)\\n-------------------------------------------------\\nProblem: Object graph is expensive to create/load.\\n\\nKey idea: Proxy holds a reference but doesn\u2019t materialize the real object until the first method call.\\n\\npublic class VirtualProxy implements HeavyObject {\\n    private HeavyObject real;   // null until needed\\n    @Override\\n    public void heavyOp() {\\n        if (real == null) {\\n            real = new HeavyObjectImpl(); // costly\\n        }\\n        real.heavyOp();\\n    }\\n}\\n\\n-------------------------------------------------\\n5. Proxy + Metrics / Logging (Decorator flavour)\\n-------------------------------------------------\\nProblem: Ops wants latency counters, logs, tracing.\\n\\nKey idea: Proxy wraps and times, then delegates.\\n\\npublic class MetricsProxy implements DataService {\\n    private final DataService real;\\n    private final MeterRegistry registry;\\n\\n    @Override\\n    public String fetch(String key) {\\n        return registry.timer(\"ds.fetch\", \"key\", key)\\n                       .recordCallable(() -&gt; real.fetch(key));\\n    }\\n}\\n\\n-------------------------------------------------\\n6. Chaining them together\\n-------------------------------------------------\\nBecause every proxy implements the same interface you can stack:\\n\\nDataService service =\\n    new MetricsProxy(\\n        new SecureProxy(\\n            new CachingProxy(\\n                new RemoteDataService())));\\n\\nThe call order becomes:\\nMetrics \u2192 Security \u2192 Cache \u2192 Network.\\n\\n-------------------------------------------------\\n7. When to stop\\n-------------------------------------------------\\n- Keep the interface stable; every new concern is a new wrapper.  \\n- If the stack becomes hard to reason about, switch to a single interceptor pipeline (e.g., Spring AOP, Jakarta interceptors, .NET DispatchProxy) so ordering is explicit.  \\n- Document the chain in README or a diagram\u2014future you will thank you.\\n\\n-------------------------------------------------\\nRule of thumb\\n-------------------------------------------------\\n\u201cAdd a new proxy when the concern is orthogonal to business logic and you can express it as \u2018before/after\u2019 the real call.\u201d  \\nEverything else probably belongs inside the service itself.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 855, 'prompt_tokens': 64, 'total_tokens': 919, 'completion_time': 3.461216384, 'prompt_time': 0.01227876, 'queue_time': 0.147729928, 'total_time': 3.473495144}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_5fe129dff6', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ce44a953-a544-4fcc-ad65-01a4427b54c2-0', usage_metadata={'input_tokens': 64, 'output_tokens': 855, 'total_tokens': 919}),\n 'question': 'Can I combine the pattern with other patterns?'}\n</pre> <p>Crea un template que espera una variable llamada \"input\" y contiene un prompt que pide el ant\u00f3nimo y una explicaci\u00f3n. Luego se arma un chain que compone el prompt con los valores de entrada y lo env\u00eda al model.</p> In\u00a0[42]: Copied! <pre># Template inicial\nzero_shot_prompt = PromptTemplate(\n    input_variables=['input'], validate_template=True,\n    template=\"\"\"Return the antonym of the input given along with an explanation.\n    Input: {input}\n    Output:\n    Explanation:\n    \"\"\"\n)\n\n# Zero-shot chain (por ahora)\nzero_shot_chain = zero_shot_prompt | llm\n\nquery = 'I am very sad but still have hope'\nresult = zero_shot_chain.invoke(input=query)\nprint(result.content)\n</pre> # Template inicial zero_shot_prompt = PromptTemplate(     input_variables=['input'], validate_template=True,     template=\"\"\"Return the antonym of the input given along with an explanation.     Input: {input}     Output:     Explanation:     \"\"\" )  # Zero-shot chain (por ahora) zero_shot_chain = zero_shot_prompt | llm  query = 'I am very sad but still have hope' result = zero_shot_chain.invoke(input=query) print(result.content) <pre>Antonym:  \n\u201cI am overjoyed and utterly without hope.\u201d\n\nExplanation:  \nThe original sentence pairs deep sadness (\u201cvery sad\u201d) with the retention of hope. Its opposite must reverse both emotional valence and outlook. \u201cOverjoyed\u201d is the direct affective opposite of \u201cvery sad,\u201d while \u201cutterly without hope\u201d negates the residual optimism. The conjunction \u201cand\u201d replaces \u201cbut\u201d because the two reversed states now align in the same negative-positive direction, forming a coherent antithetical state.\n</pre> <p>Ahora se supone que hay una base de ejemplos de antonimos.</p> In\u00a0[43]: Copied! <pre># Ejemplos de tareas para crear antonimos\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=\"Input: {input}\\nOutput: {output}\",\n)\n\nexamples = [\n    {\"input\": \"happy\", \"output\": \"sad\"},\n    {\"input\": \"tall\", \"output\": \"short\"},\n    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n    {\"input\": \"windy\", \"output\": \"calm\"},\n]\n</pre> # Ejemplos de tareas para crear antonimos example_prompt = PromptTemplate(     input_variables=[\"input\", \"output\"],     template=\"Input: {input}\\nOutput: {output}\", )  examples = [     {\"input\": \"happy\", \"output\": \"sad\"},     {\"input\": \"tall\", \"output\": \"short\"},     {\"input\": \"energetic\", \"output\": \"lethargic\"},     {\"input\": \"sunny\", \"output\": \"gloomy\"},     {\"input\": \"windy\", \"output\": \"calm\"}, ] <p>Se configura un flujo few\u2011shot que selecciona din\u00e1micamente ejemplos sem\u00e1nticamente similares y renderiza un prompt listo para enviar al LLM. Se usan los embeddings (de OpenAI, u otros) para convertir ejemplos a vectores y luego Chroma como based de almacenamiento. El parametro k=1 indica que se devolver\u00e1 1 ejemplo cercano por consulta.</p> <p>Al ejecutar el FewShotsPromptTemplate, el selector busca el ejemplo sem\u00e1nticamente m\u00e1s cercano, y lo inserta para generar el prompt final, que debiera usarse con el LLM.</p> In\u00a0[44]: Copied! <pre>embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\nexample_selector = SemanticSimilarityExampleSelector.from_examples(\n    # The list of examples available to select from.\n    examples,\n    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n    embeddings, #OpenAIEmbeddings(),\n    Chroma, # The database to store the examples with their embeddings\n    # The number of examples to produce.\n    k=2,\n)\n\nsimilar_prompt = FewShotPromptTemplate(\n    # We provide an ExampleSelector instead of examples.\n    example_selector=example_selector,\n    example_prompt=example_prompt,\n    prefix=\"Return the antonym of the given input along with an explanation. \\n\\nExample(s):\",\n    suffix=\"Input: {adjective}\\nOutput:\",\n    input_variables=[\"adjective\"],\n)\n\nprint(similar_prompt.format(adjective=\"cloudy\"))\n</pre> embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  example_selector = SemanticSimilarityExampleSelector.from_examples(     # The list of examples available to select from.     examples,     # The embedding class used to produce embeddings which are used to measure semantic similarity.     embeddings, #OpenAIEmbeddings(),     Chroma, # The database to store the examples with their embeddings     # The number of examples to produce.     k=2, )  similar_prompt = FewShotPromptTemplate(     # We provide an ExampleSelector instead of examples.     example_selector=example_selector,     example_prompt=example_prompt,     prefix=\"Return the antonym of the given input along with an explanation. \\n\\nExample(s):\",     suffix=\"Input: {adjective}\\nOutput:\",     input_variables=[\"adjective\"], )  print(similar_prompt.format(adjective=\"cloudy\"))  <pre>Return the antonym of the given input along with an explanation. \n\nExample(s):\n\nInput: sunny\nOutput: gloomy\n\nInput: sunny\nOutput: gloomy\n\nInput: cloudy\nOutput:\n</pre> <p>Y solo resta usar el prompt en una chain</p> In\u00a0[45]: Copied! <pre># Few-shot chain\nfew_shot_chain = similar_prompt | llm\n\nquery = 'rainy' #\u00a0'I am very sad but still have hope'\nprint(similar_prompt.format(adjective=query))\nprint()\n\nresult = few_shot_chain.invoke(input=query)\nprint(result.text())\n</pre> # Few-shot chain few_shot_chain = similar_prompt | llm  query = 'rainy' #\u00a0'I am very sad but still have hope' print(similar_prompt.format(adjective=query)) print()  result = few_shot_chain.invoke(input=query) print(result.text()) <pre>Return the antonym of the given input along with an explanation. \n\nExample(s):\n\nInput: sunny\nOutput: gloomy\n\nInput: sunny\nOutput: gloomy\n\nInput: rainy\nOutput:\n\nOutput: sunny  \nExplanation: \u201cRainy\u201d describes weather characterized by rain, while \u201csunny\u201d describes weather with clear skies and sunshine\u2014essentially the opposite condition.\n</pre> <p>Alternativamente, se le puede pedir al LLM que genere un objeto JSON como respuesta (salida estructurada) via Pydantic.</p> In\u00a0[46]: Copied! <pre># Descripci\u00f3n de la estructura de salida usando Pydantic\nclass FormattedAntonym(BaseModel):\n    antonym: str = Field(description=\"An antonym for the input word or phrase.\", default=\"\")\n    explanation: str = Field(description=\"A short explanation of how the antonym was generated\")\n    additional_clarifications: List[str] = Field(description=\"A list of questions that the LLM needs to clarify in order to ...\", default=[])\n\n\n# Wrapper para la salida estructurada\nllm_with_structure = llm.with_structured_output(FormattedAntonym)\n\nfew_shot_chain1 = similar_prompt | llm_with_structure\nresult = few_shot_chain1.invoke(input=query)\nresult # The Pydantic (object) format\n</pre> # Descripci\u00f3n de la estructura de salida usando Pydantic class FormattedAntonym(BaseModel):     antonym: str = Field(description=\"An antonym for the input word or phrase.\", default=\"\")     explanation: str = Field(description=\"A short explanation of how the antonym was generated\")     additional_clarifications: List[str] = Field(description=\"A list of questions that the LLM needs to clarify in order to ...\", default=[])   # Wrapper para la salida estructurada llm_with_structure = llm.with_structured_output(FormattedAntonym)  few_shot_chain1 = similar_prompt | llm_with_structure result = few_shot_chain1.invoke(input=query) result # The Pydantic (object) format Out[46]: <pre>FormattedAntonym(antonym='sunny', explanation='The antonym of \"rainy\" is \"sunny\" because \"rainy\" describes weather characterized by rain and overcast skies, whereas \"sunny\" describes clear skies and sunshine, representing opposite weather conditions.', additional_clarifications=[])</pre> In\u00a0[47]: Copied! <pre>result.antonym\n</pre> result.antonym Out[47]: <pre>'sunny'</pre> In\u00a0[48]: Copied! <pre># print(result.model_dump_json(indent=2))\npprint.pprint(result.model_dump()) # This is a dict\n</pre> # print(result.model_dump_json(indent=2)) pprint.pprint(result.model_dump()) # This is a dict <pre>{'additional_clarifications': [],\n 'antonym': 'sunny',\n 'explanation': 'The antonym of \"rainy\" is \"sunny\" because \"rainy\" describes '\n                'weather characterized by rain and overcast skies, whereas '\n                '\"sunny\" describes clear skies and sunshine, representing '\n                'opposite weather conditions.'}\n</pre>"},{"location":"dia1/notebooks/chat_model/#prompting-y-langchain-basico","title":"Prompting y Langchain b\u00e1sico\u00b6","text":""},{"location":"dia1/notebooks/chat_model/#chat-simple-basado-en-llm","title":"Chat simple basado en LLM\u00b6","text":"<p>Para Groq, crear una cuenta y un API KEY at https://groq.com/</p> <p>Lee el nombre del modelo desde la variable de entorno, crea una instancia de ChatOpenAI con temperatura baja (menos aleatoriedad), env\u00eda un prompt directo al LLM y muestra la respuesta.</p>"},{"location":"dia1/notebooks/chat_model/#pros-why-you-reach-for-it","title":"PROS \u2013 why you reach for it\u00b6","text":"<ol> <li><p>Transparent protection Client code talks to the Proxy through the exact same interface it would use for the real subject \u2192 no changes in calling code.</p> </li> <li><p>Lazy (virtual) instantiation Heavy or network objects are created only when the first real method is invoked \u2192 faster start-up and lower memory footprint.</p> </li> <li><p>Security gatekeeper You can centralise permission checks in the Proxy instead of scattering them through business code.</p> </li> <li><p>Remote communication stub (RMI, gRPC, SOAP, \u2026) The Proxy serialises the call and hides all the plumbing (marshalling, retries, network failures).</p> </li> <li><p>Thread-safe or cached access Synchronisation, pooling, result caching, request coalescing, circuit-breaker, etc. can be added without touching the real object.</p> </li> <li><p>Instrumentation / AOP hook Add logging, metrics, transaction boundaries, profiling, throttling, \u2026 in one place.</p> </li> <li><p>Future-proofing &amp; hot swapping Replace the real subject with a different implementation (mock, decorator, cloud service) at run-time.</p> </li> </ol>"},{"location":"dia1/notebooks/chat_model/#cons-where-it-can-bite-you","title":"CONS \u2013 where it can bite you\u00b6","text":"<ol> <li><p>One more abstraction layer Extra classes/interfaces \u2192 steeper learning curve for newcomers, more code to navigate.</p> </li> <li><p>Performance tax Every call goes through an indirection; for very fine-grained objects (e.g., graph nodes, game entities) the overhead can dominate.</p> </li> <li><p>Lifecycle complexity You now have two objects to manage: proxy and real subject. \u2013 Who creates whom? \u2013 Who owns the expensive resources? \u2013 When can the real subject be disposed? Reference-counting or weak references may be needed.</p> </li> <li><p>Concurrency pitfalls If the proxy is shared between threads, its own state (e.g., cached reference to the real subject) must be thread-safe, duplicating synchronisation effort.</p> </li> <li><p>Debugging pain Stack traces are deeper; breakpoints in the proxy can hide the real culprit; logging may show \u201cproxy\u201d instead of the actual class name unless you toString() carefully.</p> </li> <li><p>Interface coupling If the real subject\u2019s interface evolves you must update the proxy (and all specialised proxies: virtual, remote, cache, security, \u2026). Dynamic proxies (java.lang.reflect.Proxy, CGLib, ByteBuddy) mitigate this but add byte-code magic that not every team is comfortable with.</p> </li> <li><p>Over-engineering risk A simple \u201cif\u201d in the service layer is sometimes enough; wrapping every repository call in a proxy can turn into a maze of tiny classes.</p> </li> </ol>"},{"location":"dia1/notebooks/chat_model/#rule-of-thumb","title":"Rule-of-thumb\u00b6","text":"<p>Use Proxy when the cost of creating/accessing the real object is high, or when you need to add a systemic, cross-cutting behaviour (security, caching, remoting) that must stay orthogonal to business logic. Avoid it when the object is cheap, calls are extremely frequent, or the added indirection obscures more than it helps.</p>"},{"location":"dia1/notebooks/chat_model/#gestion-de-memoria-como-parte-de-un-chat","title":"Gesti\u00f3n de Memoria (como parte de un chat)\u00b6","text":""},{"location":"dia1/notebooks/chat_model/#rule-of-thumb","title":"Rule of thumb\u00b6","text":"<p>\u201cAdd a new proxy when the concern is orthogonal to business logic and you can express it as \u2018before/after\u2019 the real call.\u201d Everything else probably belongs inside the service itself.</p>"},{"location":"dia1/notebooks/chat_model/#few-shots","title":"Few-shots\u00b6","text":""},{"location":"dia1/notebooks/langchain-tool-call/","title":"LangChain Tool Calling","text":"In\u00a0[13]: Copied! <pre>#\u00a0%pip install pydantic python-dotenv langchain langchain-openai langchain-community\n</pre> #\u00a0%pip install pydantic python-dotenv langchain langchain-openai langchain-community In\u00a0[14]: Copied! <pre>from dotenv import load_dotenv\n\nload_dotenv()\n</pre> from dotenv import load_dotenv  load_dotenv() Out[14]: <pre>True</pre> In\u00a0[15]: Copied! <pre>from langchain_core.tools import tool\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"\n    Generate a weather message for a given location.\n\n    Args:\n        location (str): The location for which to generate the weather report.\n\n    Returns:\n        str: A weather message for the specified location.\n    \"\"\"\n    return f\"The weather in {location} is sunny with a chance of clouds.\"\n</pre> from langchain_core.tools import tool  @tool def get_weather(location: str) -&gt; str:     \"\"\"     Generate a weather message for a given location.      Args:         location (str): The location for which to generate the weather report.      Returns:         str: A weather message for the specified location.     \"\"\"     return f\"The weather in {location} is sunny with a chance of clouds.\" In\u00a0[16]: Copied! <pre># Tools conform to the Runnable interface, which means you can run a tool using the invoke method\n\nget_weather.invoke(\"Tandil\")\n</pre> # Tools conform to the Runnable interface, which means you can run a tool using the invoke method  get_weather.invoke(\"Tandil\") Out[16]: <pre>'The weather in Tandil is sunny with a chance of clouds.'</pre> In\u00a0[17]: Copied! <pre>tool_call = {\n    \"type\": \"tool_call\",\n    \"id\": \"1\",\n    \"args\": { \"location\": \"Mar del Plata\" }\n}\n\nget_weather.invoke(tool_call) # returns a ToolMessage object\n</pre> tool_call = {     \"type\": \"tool_call\",     \"id\": \"1\",     \"args\": { \"location\": \"Mar del Plata\" } }  get_weather.invoke(tool_call) # returns a ToolMessage object Out[17]: <pre>ToolMessage(content='The weather in Mar del Plata is sunny with a chance of clouds.', name='get_weather', tool_call_id='1')</pre> In\u00a0[18]: Copied! <pre>from langchain.chat_models import init_chat_model\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\nSYSTEM_PROMPT = \"\"\"\nYou report the weather of a given location using emojis. \n\nUse the following format for your answers:\n1. Tandil: \u2600\ufe0f - \ud83c\udf21\n2. London: \ufe0f\ud83c\udf27\ufe0f - \u2744\ufe0f\n\"\"\"\n\nmodel = init_chat_model(model=\"gpt-4.1-nano\", temperature=0)\n\nchat_template = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(content=SYSTEM_PROMPT),\n        HumanMessage(content=\"What's the weather like in Paris today?\"),\n    ]\n)\n\n# Bind schema to model\nmodel_with_tools = model.bind_tools([get_weather])\n\n# Invoke the model to produce structured output that matches the schema\nresponse = model_with_tools.invoke(chat_template.format_messages())\n</pre> from langchain.chat_models import init_chat_model from langchain_core.prompts import ChatPromptTemplate from langchain_core.messages import SystemMessage, HumanMessage  SYSTEM_PROMPT = \"\"\" You report the weather of a given location using emojis.   Use the following format for your answers: 1. Tandil: \u2600\ufe0f - \ud83c\udf21 2. London: \ufe0f\ud83c\udf27\ufe0f - \u2744\ufe0f \"\"\"  model = init_chat_model(model=\"gpt-4.1-nano\", temperature=0)  chat_template = ChatPromptTemplate.from_messages(     [         SystemMessage(content=SYSTEM_PROMPT),         HumanMessage(content=\"What's the weather like in Paris today?\"),     ] )  # Bind schema to model model_with_tools = model.bind_tools([get_weather])  # Invoke the model to produce structured output that matches the schema response = model_with_tools.invoke(chat_template.format_messages())  In\u00a0[19]: Copied! <pre>print(response.tool_calls)\n</pre> print(response.tool_calls) <pre>[{'name': 'get_weather', 'args': {'location': 'Paris'}, 'id': 'call_01AXm3pVsMd5DOjFFYP9PtJB', 'type': 'tool_call'}]\n</pre> In\u00a0[20]: Copied! <pre>tool_call = response.tool_calls[0]\n\ntool_return = get_weather.invoke(tool_call)\n</pre> tool_call = response.tool_calls[0]  tool_return = get_weather.invoke(tool_call) In\u00a0[21]: Copied! <pre>tool_return\n</pre> tool_return Out[21]: <pre>ToolMessage(content='The weather in Paris is sunny with a chance of clouds.', name='get_weather', tool_call_id='call_01AXm3pVsMd5DOjFFYP9PtJB')</pre> In\u00a0[22]: Copied! <pre>msgs = chat_template.format_messages() + [response] + [tool_return]\n\nmsgs\n</pre> msgs = chat_template.format_messages() + [response] + [tool_return]  msgs Out[22]: <pre>[SystemMessage(content='\\nYou report the weather of a given location using emojis. \\n\\nUse the following format for your answers:\\n1. Tandil: \u2600\ufe0f - \ud83c\udf21\\n2. London: \ufe0f\ud83c\udf27\ufe0f - \u2744\ufe0f\\n', additional_kwargs={}, response_metadata={}),\n HumanMessage(content=\"What's the weather like in Paris today?\", additional_kwargs={}, response_metadata={}),\n AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_01AXm3pVsMd5DOjFFYP9PtJB', 'function': {'arguments': '{\"location\":\"Paris\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 129, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_950f36939b', 'id': 'chatcmpl-CVT1Q7RJQULYpGtAVj6tu0a46bPOe', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d73c2480-79bb-410a-b4d0-2f3a29a9172e-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Paris'}, 'id': 'call_01AXm3pVsMd5DOjFFYP9PtJB', 'type': 'tool_call'}], usage_metadata={'input_tokens': 129, 'output_tokens': 14, 'total_tokens': 143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n ToolMessage(content='The weather in Paris is sunny with a chance of clouds.', name='get_weather', tool_call_id='call_01AXm3pVsMd5DOjFFYP9PtJB')]</pre> In\u00a0[23]: Copied! <pre>model_with_tools.invoke(msgs).content\n</pre> model_with_tools.invoke(msgs).content Out[23]: <pre>'Paris: \u2600\ufe0f - \u2601\ufe0f'</pre>"},{"location":"dia1/notebooks/langchain-tool-call/#langchain-tool-calling","title":"LangChain Tool Calling\u00b6","text":"<p>For more information: https://langchain-ai.github.io/langgraph/how-tos/tool-calling/</p>"},{"location":"dia1/notebooks/rag_langchain/","title":"Retrieval Augmented Generation (RAG)","text":"<p>Importaci\u00f3n de bibliotecas e instalaci\u00f3n de paquetes</p> In\u00a0[1]: Copied! <pre>from dotenv import load_dotenv\nimport pandas as pd\nfrom pathlib import Path\nimport json\nfrom dotenv import load_dotenv\nimport os \nimport shutil \nfrom IPython.display import display, Markdown\nimport pprint\n</pre> from dotenv import load_dotenv import pandas as pd from pathlib import Path import json from dotenv import load_dotenv import os  import shutil  from IPython.display import display, Markdown import pprint In\u00a0[\u00a0]: Copied! <pre>from langchain.document_loaders.pdf import PyPDFDirectoryLoader \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \nfrom langchain_openai import OpenAIEmbeddings \nfrom langchain.schema import Document \nfrom langchain_chroma import Chroma # This is a Chroma wrapper from Langchain\nfrom langchain_openai import ChatOpenAI # Import OpenAI LLM\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain import hub\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.document_loaders import PyPDFLoader\n\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\nfrom langchain_huggingface import HuggingFaceEmbeddings\n</pre> from langchain.document_loaders.pdf import PyPDFDirectoryLoader  from langchain.text_splitter import RecursiveCharacterTextSplitter  from langchain_openai import OpenAIEmbeddings  from langchain.schema import Document  from langchain_chroma import Chroma # This is a Chroma wrapper from Langchain from langchain_openai import ChatOpenAI # Import OpenAI LLM from langchain_core.vectorstores import InMemoryVectorStore from langchain import hub from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser from langchain.document_loaders import PyPDFLoader  from langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import CrossEncoderReranker from langchain_community.cross_encoders import HuggingFaceCrossEncoder from langchain_huggingface import HuggingFaceEmbeddings  In\u00a0[3]: Copied! <pre># %pip install pypdf langchain-huggingface sentence-transformers\n</pre> # %pip install pypdf langchain-huggingface sentence-transformers In\u00a0[\u00a0]: Copied! <pre># Carga de variables de ambiente desde el archivo .env\nload_dotenv()\n</pre> # Carga de variables de ambiente desde el archivo .env load_dotenv() Out[\u00a0]: <pre>True</pre> In\u00a0[3]: Copied! <pre># Get the same dataset as in the other notebook\ninput_datapath = \"../semantic-search/dataset.json\"\n\nwith open(input_datapath, 'r') as f:\n    movie_data = json.load(f)\n\ndf = pd.DataFrame(movie_data)\nprint(df.shape)\ndf.head()\n</pre> # Get the same dataset as in the other notebook input_datapath = \"../semantic-search/dataset.json\"  with open(input_datapath, 'r') as f:     movie_data = json.load(f)  df = pd.DataFrame(movie_data) print(df.shape) df.head() <pre>(10, 10)\n</pre> Out[3]: title release_date genres original_language vote_average overview tagline combined n_tokens embedding 0 The Pope's Exorcist 2023-04-05 ['Horror', 'Mystery', 'Thriller'] English 7.4 Father Gabriele Amorth, Chief Exorcist of the ... Inspired by the actual files of Father Gabriel... The Pope's Exorcist Father Gabriele Amorth, Ch... 67 [0.0099146804, -0.0019374829, -0.0009720114, -... 1 Ant-Man and the Wasp: Quantumania 2023-02-15 ['Action', 'Adventure', 'Science Fiction'] English 6.6 Super-Hero partners Scott Lang and Hope van Dy... Witness the beginning of a new dynasty. Ant-Man and the Wasp: Quantumania Super-Hero p... 84 [0.0057371012, -0.017788643, 0.0122131966, -0.... 2 Ghosted 2023-04-18 ['Action', 'Comedy', 'Romance'] English 7.2 Salt-of-the-earth Cole falls head over heels f... Finding that special someone can be a real adv... Ghosted Salt-of-the-earth Cole falls head over... 65 [0.0011030367, -0.018147951, 0.0101844044, -0.... 3 Shazam! Fury of the Gods 2023-03-15 ['Action', 'Comedy', 'Fantasy', 'Adventure'] English 6.8 Billy Batson and his foster siblings, who tran... Oh. My. Gods. Shazam! Fury of the Gods Billy Batson and his ... 62 [-0.0110116974, -0.0404475406, -0.0210291967, ... 4 Avatar: The Way of Water 2022-12-14 ['Science Fiction', 'Adventure', 'Action'] English 7.7 Set more than a decade after the events of the... Return to Pandora. Avatar: The Way of Water Set more than a decad... 72 [-0.0010760396, -0.0292616803, -0.0164514501, ... <p>Se crea un \"documento\" por pel\u00edcula, y tambi\u00e9n se agrega metadata</p> In\u00a0[4]: Copied! <pre>import ast\n\ndocuments = []\nfor index, row in df.iterrows():\n    genres = ast.literal_eval(row['genres'])\n    md_dict = {\n        \"language\": row['original_language'], \n        \"genre\": genres[0], \n        \"release_date\": row['release_date'],\n        \"source\": index\n    }\n    doc = Document(id=index, page_content=row['title']+\" - \"+row['overview'], metadata=md_dict)\n    documents.append(doc)\nprint(len(documents), \"documents\")\n</pre> import ast  documents = [] for index, row in df.iterrows():     genres = ast.literal_eval(row['genres'])     md_dict = {         \"language\": row['original_language'],          \"genre\": genres[0],          \"release_date\": row['release_date'],         \"source\": index     }     doc = Document(id=index, page_content=row['title']+\" - \"+row['overview'], metadata=md_dict)     documents.append(doc) print(len(documents), \"documents\") <pre>10 documents\n</pre> In\u00a0[97]: Copied! <pre>documents\n</pre> documents Out[97]: <pre>[Document(id='0', metadata={'language': 'English', 'genre': 'Horror', 'release_date': '2023-04-05', 'source': 0}, page_content=\"The Pope's Exorcist - Father Gabriele Amorth, Chief Exorcist of the Vatican, investigates a young boy's terrifying possession and ends up uncovering a centuries-old conspiracy the Vatican has desperately tried to keep hidden.\"),\n Document(id='1', metadata={'language': 'English', 'genre': 'Action', 'release_date': '2023-02-15', 'source': 1}, page_content=\"Ant-Man and the Wasp: Quantumania - Super-Hero partners Scott Lang and Hope van Dyne, along with with Hope's parents Janet van Dyne and Hank Pym, and Scott's daughter Cassie Lang, find themselves exploring the Quantum Realm, interacting with strange new creatures and embarking on an adventure that will push them beyond the limits of what they thought possible.\"),\n Document(id='2', metadata={'language': 'English', 'genre': 'Action', 'release_date': '2023-04-18', 'source': 2}, page_content='Ghosted - Salt-of-the-earth Cole falls head over heels for enigmatic Sadie \u2014 but then makes the shocking discovery that she\u2019s a secret agent. Before they can decide on a second date, Cole and Sadie are swept away on an international adventure to save the world.'),\n Document(id='3', metadata={'language': 'English', 'genre': 'Action', 'release_date': '2023-03-15', 'source': 3}, page_content='Shazam! Fury of the Gods - Billy Batson and his foster siblings, who transform into superheroes by saying \"Shazam!\", are forced to get back into action and fight the Daughters of Atlas, who they must stop from using a weapon that could destroy the world.'),\n Document(id='4', metadata={'language': 'English', 'genre': 'Science Fiction', 'release_date': '2022-12-14', 'source': 4}, page_content='Avatar: The Way of Water - Set more than a decade after the events of the first film, learn the story of the Sully family (Jake, Neytiri, and their kids), the trouble that follows them, the lengths they go to keep each other safe, the battles they fight to stay alive, and the tragedies they endure.'),\n Document(id='5', metadata={'language': 'English', 'genre': 'Science Fiction', 'release_date': '2023-05-03', 'source': 5}, page_content='Guardians of the Galaxy Volume 3 - Peter Quill, still reeling from the loss of Gamora, must rally his team around him to defend the universe along with protecting one of their own. A mission that, if not completed successfully, could quite possibly lead to the end of the Guardians as we know them.'),\n Document(id='6', metadata={'language': 'English', 'genre': 'Horror', 'release_date': '2023-03-08', 'source': 6}, page_content='Scream VI - Following the latest Ghostface killings, the four survivors leave Woodsboro behind and start a fresh chapter.'),\n Document(id='7', metadata={'language': 'English', 'genre': 'Drama', 'release_date': '2023-03-01', 'source': 7}, page_content='Creed III - After dominating the boxing world, Adonis Creed has been thriving in both his career and family life. When a childhood friend and former boxing prodigy, Damian Anderson, resurfaces after serving a long sentence in prison, he is eager to prove that he deserves his shot in the ring. The face-off between former friends is more than just a fight. To settle the score, Adonis must put his future on the line to battle Damian \u2014 a fighter who has nothing to lose.'),\n Document(id='8', metadata={'language': 'English', 'genre': 'Adventure', 'release_date': '2023-03-23', 'source': 8}, page_content='Dungeons &amp; Dragons: Honor Among Thieves - A charming thief and a band of unlikely adventurers undertake an epic heist to retrieve a lost relic, but things go dangerously awry when they run afoul of the wrong people.'),\n Document(id='9', metadata={'language': 'English', 'genre': 'Family', 'release_date': '2023-04-20', 'source': 9}, page_content='Peter Pan &amp; Wendy - Wendy Darling, a young girl afraid to leave her childhood home behind, meets Peter Pan, a boy who refuses to grow up. Alongside her brothers and a tiny fairy, Tinker Bell, she travels with Peter to the magical world of Neverland. There, she encounters an evil pirate captain, Captain Hook, and embarks on a thrilling adventure that will change her life forever.')]</pre> <p>Por simplificidad, se almacenan las pel\u00edculas en una base vectorial en memoria (aunque podr\u00eda usarse cualquier otra base vectorial en disco o en cloud). La base vectorial requiere una estrategia de embeddings para los documentos a ingestar.</p> In\u00a0[\u00a0]: Copied! <pre>embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Local model\n\n# vectorstore = InMemoryVectorStore(OpenAIEmbeddings())\nvectorstore = InMemoryVectorStore(embeddings)\n_ = vectorstore.add_documents(documents=documents)\n</pre> embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Local model  # vectorstore = InMemoryVectorStore(OpenAIEmbeddings()) vectorstore = InMemoryVectorStore(embeddings) _ = vectorstore.add_documents(documents=documents) <p>Funcionalidad de b\u00fasqueda sem\u00e1ntica (k vecinos m\u00e1s cercanos al query). Pueden implementarse b\u00fasquedas m\u00e1s complejas, que aprovechen la metadata o que usen un umbral de similitud.</p> In\u00a0[9]: Copied! <pre>def _filter_function(doc: Document) -&gt; bool:\n    return doc.metadata.get(\"genre\") == 'Horror'\n\n# Alternative ways of performing a semantic search\n\nquery = \"Something about religion\"\n#\u00a0results = vectorstore.similarity_search(query, k=2)\nresults = vectorstore.similarity_search_with_score(query, k=2)\n#\u00a0results = vectorstore.similarity_search_with_score(query, k=2, filter=_filter_function)\n\nfor r in results:\n    print(r)\n</pre> def _filter_function(doc: Document) -&gt; bool:     return doc.metadata.get(\"genre\") == 'Horror'  # Alternative ways of performing a semantic search  query = \"Something about religion\" #\u00a0results = vectorstore.similarity_search(query, k=2) results = vectorstore.similarity_search_with_score(query, k=2) #\u00a0results = vectorstore.similarity_search_with_score(query, k=2, filter=_filter_function)  for r in results:     print(r) <pre>(Document(id='0', metadata={'language': 'English', 'genre': 'Horror', 'release_date': '2023-04-05', 'source': 0}, page_content=\"The Pope's Exorcist - Father Gabriele Amorth, Chief Exorcist of the Vatican, investigates a young boy's terrifying possession and ends up uncovering a centuries-old conspiracy the Vatican has desperately tried to keep hidden.\"), 0.22907333452485987)\n(Document(id='4', metadata={'language': 'English', 'genre': 'Science Fiction', 'release_date': '2022-12-14', 'source': 4}, page_content='Avatar: The Way of Water - Set more than a decade after the events of the first film, learn the story of the Sully family (Jake, Neytiri, and their kids), the trouble that follows them, the lengths they go to keep each other safe, the battles they fight to stay alive, and the tragedies they endure.'), 0.1761204551123968)\n</pre> <p>En Langchain, se usa un retriever sobre la base vectorial</p> In\u00a0[10]: Copied! <pre>retriever = vectorstore.as_retriever(\n    search_kwargs={\n        'k': 3\n    }\n)\n\nretriever.invoke(input=query)\n</pre> retriever = vectorstore.as_retriever(     search_kwargs={         'k': 3     } )  retriever.invoke(input=query) Out[10]: <pre>[Document(id='0', metadata={'language': 'English', 'genre': 'Horror', 'release_date': '2023-04-05', 'source': 0}, page_content=\"The Pope's Exorcist - Father Gabriele Amorth, Chief Exorcist of the Vatican, investigates a young boy's terrifying possession and ends up uncovering a centuries-old conspiracy the Vatican has desperately tried to keep hidden.\"),\n Document(id='4', metadata={'language': 'English', 'genre': 'Science Fiction', 'release_date': '2022-12-14', 'source': 4}, page_content='Avatar: The Way of Water - Set more than a decade after the events of the first film, learn the story of the Sully family (Jake, Neytiri, and their kids), the trouble that follows them, the lengths they go to keep each other safe, the battles they fight to stay alive, and the tragedies they endure.'),\n Document(id='8', metadata={'language': 'English', 'genre': 'Adventure', 'release_date': '2023-03-23', 'source': 8}, page_content='Dungeons &amp; Dragons: Honor Among Thieves - A charming thief and a band of unlikely adventurers undertake an epic heist to retrieve a lost relic, but things go dangerously awry when they run afoul of the wrong people.')]</pre> In\u00a0[11]: Copied! <pre>llm_model = os.environ[\"OPENAI_MODEL\"]\nprint(llm_model)\nllm = ChatOpenAI(model=llm_model, temperature=0.1)\n</pre> llm_model = os.environ[\"OPENAI_MODEL\"] print(llm_model) llm = ChatOpenAI(model=llm_model, temperature=0.1) <pre>gpt-4o-mini\n</pre> <p>El patron de prompt t\u00edpico de RAG considera una pregunta y un context</p> In\u00a0[21]: Copied! <pre># Example for a public prompt (https://smith.langchain.com/hub/rlm/rag-prompt)\nrag_prompt = hub.pull(\"rlm/rag-prompt\", include_model=True)\nrag_prompt_template = rag_prompt.messages[0].prompt\nrag_prompt_template.model_dump() # Pydantic object in JSON format\n</pre> # Example for a public prompt (https://smith.langchain.com/hub/rlm/rag-prompt) rag_prompt = hub.pull(\"rlm/rag-prompt\", include_model=True) rag_prompt_template = rag_prompt.messages[0].prompt rag_prompt_template.model_dump() # Pydantic object in JSON format <pre>/Users/adiazpace/opt/anaconda3/envs/ia_desde_cero/lib/python3.11/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n  warnings.warn(\n</pre> Out[21]: <pre>{'name': None,\n 'input_variables': ['context', 'question'],\n 'optional_variables': [],\n 'output_parser': None,\n 'partial_variables': {},\n 'metadata': None,\n 'tags': None,\n 'template': \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\",\n 'template_format': 'f-string',\n 'validate_template': False}</pre> In\u00a0[22]: Copied! <pre>pprint.pprint(rag_prompt_template.template) # El prompt en s\u00ed del RAG\n</pre> pprint.pprint(rag_prompt_template.template) # El prompt en s\u00ed del RAG <pre>('You are an assistant for question-answering tasks. Use the following pieces '\n \"of retrieved context to answer the question. If you don't know the answer, \"\n \"just say that you don't know. Use three sentences maximum and keep the \"\n 'answer concise.\\n'\n 'Question: {question} \\n'\n 'Context: {context} \\n'\n 'Answer:')\n</pre> <p>Si se quisiera, se puede hacer un prompt m\u00e1s custom, siempre respetando la estructura b\u00e1sica y la variable context</p> In\u00a0[27]: Copied! <pre># Conexi\u00f3n entre el retriever y el LLM usando el prompt de RAG\nrag_chain = {\"context\": retriever,  \"question\": RunnablePassthrough()} | rag_prompt | llm | StrOutputParser()\n\nquery = \"I want to get a movie about religion\"\nresult = rag_chain.invoke(input=query)\n#\u00a0pprint.pprint(result)\ndisplay(Markdown(result))\n</pre> # Conexi\u00f3n entre el retriever y el LLM usando el prompt de RAG rag_chain = {\"context\": retriever,  \"question\": RunnablePassthrough()} | rag_prompt | llm | StrOutputParser()  query = \"I want to get a movie about religion\" result = rag_chain.invoke(input=query) #\u00a0pprint.pprint(result) display(Markdown(result)) <p>You might consider watching \"The Pope's Exorcist,\" which involves themes of religion as it follows Father Gabriele Amorth investigating a young boy's possession and uncovering a Vatican conspiracy.</p> In\u00a0[60]: Copied! <pre># We consider a large PDF file\npdf_path = \"./data/Understanding_Climate_Change.pdf\"\n\nloader = PyPDFLoader(pdf_path) # Tool to load and process a PDF file\npdf_documents = loader.load() # Each document corresponds actually to a page\nprint(len(pdf_documents), \"loaded\")\n</pre> # We consider a large PDF file pdf_path = \"./data/Understanding_Climate_Change.pdf\"  loader = PyPDFLoader(pdf_path) # Tool to load and process a PDF file pdf_documents = loader.load() # Each document corresponds actually to a page print(len(pdf_documents), \"loaded\") <pre>33 loaded\n</pre> <p>Cada documento (p\u00e1gina del PDF) se particiona en chunks. La estrategia RecursiveCharacterTextSplitter es sint\u00e1ctica, y aplica una heur\u00edstica por caracteres para ver p\u00e1rrafos y frases mirando los separadores (salto de l\u00ednea, espacio, etc.). Pueden utilizarse tambi\u00e9n otras estrategias (por ej., jer\u00e1rquica, sem\u00e1ntica, etc.)</p> In\u00a0[98]: Copied! <pre>from uuid import uuid4\n\ndef preprocess(list_of_documents):\n    for idx, doc in enumerate(list_of_documents):\n        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n        doc.id = str(idx) #str(uuid4())\n    return list_of_documents\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=100)\n\ntexts = text_splitter.split_documents(pdf_documents)\ncleaned_texts = preprocess(texts)\nprint(len(cleaned_texts), \"chunks\")\n</pre> from uuid import uuid4  def preprocess(list_of_documents):     for idx, doc in enumerate(list_of_documents):         doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces         doc.id = str(idx) #str(uuid4())     return list_of_documents  # Split documents into chunks text_splitter = RecursiveCharacterTextSplitter(     chunk_size=1000, chunk_overlap=100)  texts = text_splitter.split_documents(pdf_documents) cleaned_texts = preprocess(texts) print(len(cleaned_texts), \"chunks\") <pre>97 chunks\n</pre> In\u00a0[77]: Copied! <pre>#\u00a0cleaned_texts\n</pre> #\u00a0cleaned_texts <p>Se utiliza ac\u00e1 una base vectorial en disco (ChromaDB) desde el wrapper que proporciona Langchain (no es igual que trabajar directamente con ChromaDB). Luego se obtiene un retriever con k=5. La configuraci\u00f3n adecuada del par\u00e1metro k es dependiente del sistema.</p> In\u00a0[108]: Copied! <pre># We use a vector store for the chunks\n\n#\u00a0ids = [d.id for d in cleaned_texts]\n\nvector_store = Chroma(\n        collection_name=\"my_documents_collection\",\n        embedding_function=embeddings,\n        persist_directory=\"./chroma_db\" # Optional: specify a directory to persist your data\n    )\nvector_store.add_documents(cleaned_texts)\n\nmy_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})\n</pre> # We use a vector store for the chunks  #\u00a0ids = [d.id for d in cleaned_texts]  vector_store = Chroma(         collection_name=\"my_documents_collection\",         embedding_function=embeddings,         persist_directory=\"./chroma_db\" # Optional: specify a directory to persist your data     ) vector_store.add_documents(cleaned_texts)  my_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7}) In\u00a0[109]: Copied! <pre># Helper function for printing docs\ndef pretty_print_docs(docs):\n    print(\n        f\"\\n{'-' * 100}\\n\".join(\n            [f\"{i + 1}. Document {d.id}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n        )\n    )\n\ntest_query = \"What is the main cause of climate change?\"\n\n# Solamente se est\u00e1 haciendo recuperaci\u00f3n\ncontext_docs =  my_retriever.invoke(input=test_query)\npretty_print_docs(context_docs)\n</pre> # Helper function for printing docs def pretty_print_docs(docs):     print(         f\"\\n{'-' * 100}\\n\".join(             [f\"{i + 1}. Document {d.id}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]         )     )  test_query = \"What is the main cause of climate change?\"  # Solamente se est\u00e1 haciendo recuperaci\u00f3n context_docs =  my_retriever.invoke(input=test_query) pretty_print_docs(context_docs) <pre>1. Document 1:\n\nchange the amount of solar energy our planet receives. During the Holocene epoch, which \nbegan at the end of the last ice age, human societies flourished, but the industrial era has seen \nunprecedented changes. \nModern Observations \nModern scientific observations indicate a rapid increase in global temperatures, sea levels, \nand extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \ndocumented these changes extensively. Ice core samples, tree rings, and ocean sediments \nprovide a historical record that scientists use to understand past climate conditions and \npredict future trends. The evidence overwhelmingly shows that recent changes are primarily \ndriven by human activities, particularly the emission of greenhouse gases. \nChapter 2: Causes of Climate Change \nGreenhouse Gases \nThe primary cause of recent climate change is the increase in greenhouse gases in the \natmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous\n----------------------------------------------------------------------------------------------------\n2. Document 7:\n\nagricultural sector's carbon footprint. \nChapter 3: Effects of Climate Change \nThe effects of climate change are already being felt around the world and are projected to \nintensify in the coming decades. These effects include: \nRising Temperatures \nGlobal temperatures have risen by about 1.2 degrees Celsius (2.2 degrees Fahrenheit) since \nthe late 19th century. This warming is not uniform, with some regions experiencing more \nsignificant increases than others. \nHeatwaves \nHeatwaves are becoming more frequent and severe, posing risks to human health, agriculture, \nand infrastructure. Cities are particularly vulnerable due to the \"urban heat island\" effect. \nHeatwaves can lead to heat-related illnesses and exacerbate existing health conditions. \nChanging Seasons \nClimate change is altering the timing and length of seasons, affecting ecosystems and human \nactivities. For example, spring is arriving earlier, and winters are becoming shorter and\n----------------------------------------------------------------------------------------------------\n3. Document 0:\n\nUnderstanding Climate Change \nChapter 1: Introduction to Climate Change \nClimate change refers to significant, long-term changes in the global climate. The term \n\"global climate\" encompasses the planet's overall weather patterns, including temperature, \nprecipitation, and wind patterns, over an extended period. Over the past century, human \nactivities, particularly the burning of fossil fuels and deforestation, have significantly \ncontributed to climate change. \nHistorical Context \nThe Earth's climate has changed throughout history. Over the past 650,000 years, there have \nbeen seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n11,700 years ago marking the beginning of the modern climate era and human civilization. \nMost of these climate changes are attributed to very small variations in Earth's orbit that \nchange the amount of solar energy our planet receives. During the Holocene epoch, which\n----------------------------------------------------------------------------------------------------\n4. Document 8:\n\nactivities. For example, spring is arriving earlier, and winters are becoming shorter and \nmilder in many regions. This shift disrupts plant and animal life cycles and agricultural \npractices. \nMelting Ice and Rising Sea Levels \nWarmer temperatures are causing polar ice caps and glaciers to melt, contributing to rising \nsea levels. Sea levels have risen by about 20 centimeters (8 inches) in the past century, \nthreatening coastal communities and ecosystems. \nPolar Ice Melt\n----------------------------------------------------------------------------------------------------\n5. Document 9:\n\nThe Arctic is warming at more than twice the global average rate, leading to significant ice \nloss. Antarctic ice sheets are also losing mass, contributing to sea level rise. This melting \naffects global ocean currents and weather patterns. \nGlacial Retreat \nGlaciers around the world are retreating, affecting water supplies for millions of people. \nRegions dependent on glacial meltwater, such as the Himalayas and the Andes, face \nparticular risks. Glacial melt also impacts hydropower generation and agriculture. \nCoastal Erosion \nRising sea levels and increased storm surges are accelerating coastal erosion, threatening \nhomes, infrastructure, and ecosystems. Low-lying islands and coastal regions are especially \nvulnerable. Coastal communities must invest in adaptation measures like sea walls and \nmanaged retreats. \nExtreme Weather Events \nClimate change is linked to an increase in the frequency and severity of extreme weather\n----------------------------------------------------------------------------------------------------\n6. Document 2:\n\natmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \noxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \nfor life on Earth, as it keeps the planet warm enough to support life. However, human \nactivities have intensified this natural process, leading to a warmer climate. \nFossil Fuels \nBurning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \nnatural gas used for electricity, heating, and transportation. The industrial revolution marked \nthe beginning of a significant increase in fossil fuel consumption, which continues to rise \ntoday. \nCoal\n----------------------------------------------------------------------------------------------------\n7. Document 38:\n\nexperiencing shifts in plant and animal species composition. These changes can lead to a loss \nof biodiversity and disrupt ecological balance. \nMarine Ecosystems \nMarine ecosystems are highly vulnerable to climate change. Rising sea temperatures, ocean \nacidification, and changing currents affect marine biodiversity, from coral reefs to deep-sea \nhabitats. Species migration and changes in reproductive cycles can disrupt marine food webs \nand fisheries.\n</pre> In\u00a0[102]: Copied! <pre>#\u00a0context_docs\n</pre> #\u00a0context_docs In\u00a0[110]: Copied! <pre># Se vuelve a definir la chain RAG - porque cambi\u00f3 el retriever, que ahora esta asociado a ChromaDB.\nrag_chain = (\n    {\"context\": my_retriever,  \"question\": RunnablePassthrough()} \n    | rag_prompt \n    | llm\n    | StrOutputParser()\n)\n\ntest_query = \"What is the main cause of climate change?\" \n#\u00a0test_query = \"What was the latest storm on Earth?\"\nresult = rag_chain.invoke(test_query)\n#\u00a0pprint.pprint(result)\ndisplay(Markdown(result))\n</pre> # Se vuelve a definir la chain RAG - porque cambi\u00f3 el retriever, que ahora esta asociado a ChromaDB. rag_chain = (     {\"context\": my_retriever,  \"question\": RunnablePassthrough()}      | rag_prompt      | llm     | StrOutputParser() )  test_query = \"What is the main cause of climate change?\"  #\u00a0test_query = \"What was the latest storm on Earth?\" result = rag_chain.invoke(test_query) #\u00a0pprint.pprint(result) display(Markdown(result)) <p>The main cause of climate change is the increase in greenhouse gases in the atmosphere, primarily due to human activities such as burning fossil fuels and deforestation. These gases, including carbon dioxide, methane, and nitrous oxide, trap heat from the sun, leading to a warming climate. This intensified greenhouse effect is largely a result of industrialization and modern energy consumption.</p> <p>Es un paso adicional para intentar maximizar la chance de recuperar chunks relevantes a la pregunta, que pueden haber quedado m\u00e1s abajo en el ranking original del retriever. En este ejemplo se usa una estrategia de cross-encoding (de HuggingFace), pero existen otras estrategias e incluso mediante un prompt al LLM.</p> In\u00a0[111]: Copied! <pre># Initialize the cross encoder\nmodel = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n\n# Create a reranker compressor\ncompressor = CrossEncoderReranker(model=model, top_n=3)\n\n# Wrap your base retriever with the compression retriever\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=my_retriever\n)\n</pre> # Initialize the cross encoder model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  # Create a reranker compressor compressor = CrossEncoderReranker(model=model, top_n=3)  # Wrap your base retriever with the compression retriever compression_retriever = ContextualCompressionRetriever(     base_compressor=compressor,     base_retriever=my_retriever ) In\u00a0[112]: Copied! <pre># Use the compression retriever\ncompressed_docs = compression_retriever.invoke(test_query)\npretty_print_docs(compressed_docs)\n</pre> # Use the compression retriever compressed_docs = compression_retriever.invoke(test_query) pretty_print_docs(compressed_docs) <pre>1. Document 1:\n\nchange the amount of solar energy our planet receives. During the Holocene epoch, which \nbegan at the end of the last ice age, human societies flourished, but the industrial era has seen \nunprecedented changes. \nModern Observations \nModern scientific observations indicate a rapid increase in global temperatures, sea levels, \nand extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \ndocumented these changes extensively. Ice core samples, tree rings, and ocean sediments \nprovide a historical record that scientists use to understand past climate conditions and \npredict future trends. The evidence overwhelmingly shows that recent changes are primarily \ndriven by human activities, particularly the emission of greenhouse gases. \nChapter 2: Causes of Climate Change \nGreenhouse Gases \nThe primary cause of recent climate change is the increase in greenhouse gases in the \natmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous\n----------------------------------------------------------------------------------------------------\n2. Document 0:\n\nUnderstanding Climate Change \nChapter 1: Introduction to Climate Change \nClimate change refers to significant, long-term changes in the global climate. The term \n\"global climate\" encompasses the planet's overall weather patterns, including temperature, \nprecipitation, and wind patterns, over an extended period. Over the past century, human \nactivities, particularly the burning of fossil fuels and deforestation, have significantly \ncontributed to climate change. \nHistorical Context \nThe Earth's climate has changed throughout history. Over the past 650,000 years, there have \nbeen seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n11,700 years ago marking the beginning of the modern climate era and human civilization. \nMost of these climate changes are attributed to very small variations in Earth's orbit that \nchange the amount of solar energy our planet receives. During the Holocene epoch, which\n----------------------------------------------------------------------------------------------------\n3. Document 7:\n\nagricultural sector's carbon footprint. \nChapter 3: Effects of Climate Change \nThe effects of climate change are already being felt around the world and are projected to \nintensify in the coming decades. These effects include: \nRising Temperatures \nGlobal temperatures have risen by about 1.2 degrees Celsius (2.2 degrees Fahrenheit) since \nthe late 19th century. This warming is not uniform, with some regions experiencing more \nsignificant increases than others. \nHeatwaves \nHeatwaves are becoming more frequent and severe, posing risks to human health, agriculture, \nand infrastructure. Cities are particularly vulnerable due to the \"urban heat island\" effect. \nHeatwaves can lead to heat-related illnesses and exacerbate existing health conditions. \nChanging Seasons \nClimate change is altering the timing and length of seasons, affecting ecosystems and human \nactivities. For example, spring is arriving earlier, and winters are becoming shorter and\n</pre> In\u00a0[113]: Copied! <pre>rag_chain1 = (\n    {\"context\": compression_retriever,  \"question\": RunnablePassthrough()} \n    | rag_prompt \n    | llm\n    | StrOutputParser()\n)\n\nresult = rag_chain.invoke(test_query)\n#\u00a0pprint.pprint(result)\ndisplay(Markdown(result))\n</pre> rag_chain1 = (     {\"context\": compression_retriever,  \"question\": RunnablePassthrough()}      | rag_prompt      | llm     | StrOutputParser() )  result = rag_chain.invoke(test_query) #\u00a0pprint.pprint(result) display(Markdown(result)) <p>The main cause of climate change is the increase in greenhouse gases in the atmosphere, primarily due to human activities such as burning fossil fuels and deforestation. These gases, including carbon dioxide, methane, and nitrous oxide, trap heat from the sun, leading to a warming climate. This intensified greenhouse effect has been documented extensively by scientific observations.</p> <p>https://github.com/gabrielchua/RAGxplorer</p> In\u00a0[21]: Copied! <pre># %pip install ragexplorer nbformat\n</pre> # %pip install ragexplorer nbformat In\u00a0[114]: Copied! <pre>from ragxplorer import RAGxplorer\n\n#\u00a0os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n\nclient_openai = RAGxplorer(embedding_model=\"text-embedding-3-small\")\nclient_openai.load_pdf(\n    document_path=pdf_path, \n    chunk_size=1000,\n    chunk_overlap=100,\n    verbose=True\n)\n</pre> from ragxplorer import RAGxplorer  #\u00a0os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')  client_openai = RAGxplorer(embedding_model=\"text-embedding-3-small\") client_openai.load_pdf(     document_path=pdf_path,      chunk_size=1000,     chunk_overlap=100,     verbose=True ) <pre> ~ Building the vector database...\nCompleted Building Vector Database \u2713\n ~ Reducing the dimensionality of embeddings...\n</pre> <pre>OMP: Info #270: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97/97 [00:01&lt;00:00, 69.35it/s] </pre> <pre>Completed reducing dimensionality of embeddings \u2713\n</pre> <pre>\n</pre> In\u00a0[115]: Copied! <pre>client_openai.visualize_query(\n    query=test_query, \n    retrieval_method=\"HyDE\", \n    top_k=6, \n    query_shape_size=10\n)\n</pre> client_openai.visualize_query(     query=test_query,      retrieval_method=\"HyDE\",      top_k=6,      query_shape_size=10 ) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 108.33it/s]\n</pre>"},{"location":"dia1/notebooks/rag_langchain/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)\u00b6","text":""},{"location":"dia1/notebooks/rag_langchain/#semantic-search-sobre-peliculas","title":"Semantic Search (sobre pel\u00edculas)\u00b6","text":"<p>Para empezar, las pel\u00edculas se toman del archivo JSON y se cargan en un dataframe. Nota: la descripci\u00f3n de cada pel\u00edcula tiene pre-computado su embedding.</p>"},{"location":"dia1/notebooks/rag_langchain/#rag-chain","title":"RAG Chain\u00b6","text":""},{"location":"dia1/notebooks/rag_langchain/#ingestion-chunks-y-rag","title":"Ingestion (chunks) y RAG\u00b6","text":"<p>En este caso hay un documento de entrada, y se quiere preguntar sobre eso</p>"},{"location":"dia1/notebooks/rag_langchain/#re-ranking","title":"Re-Ranking\u00b6","text":""},{"location":"dia1/notebooks/rag_langchain/#bonus-visualizacion-de-chunks-y-query","title":"Bonus: Visualizacion de Chunks y Query\u00b6","text":""},{"location":"dia1/notebooks/structured-outputs/","title":"Extra: Structured Outputs con OpenAI y Langchain","text":"In\u00a0[1]: Copied! <pre># %pip install pydantic python-dotenv langchain langchain-openai langchain-community\n</pre> # %pip install pydantic python-dotenv langchain langchain-openai langchain-community In\u00a0[1]: Copied! <pre>from dotenv import load_dotenv\n\nload_dotenv()\n</pre> from dotenv import load_dotenv  load_dotenv() Out[1]: <pre>True</pre> In\u00a0[4]: Copied! <pre>from pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\ncompletion = client.chat.completions.parse(\n    model=\"gpt-4.1-nano\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\nprint(event)\n</pre> from pydantic import BaseModel from openai import OpenAI  client = OpenAI()  class CalendarEvent(BaseModel):     name: str     date: str     participants: list[str]  completion = client.chat.completions.parse(     model=\"gpt-4.1-nano\",     messages=[         {\"role\": \"system\", \"content\": \"Extract the event information.\"},         {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},     ],     response_format=CalendarEvent, )  event = completion.choices[0].message.parsed print(event)  <pre>name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n</pre> In\u00a0[2]: Copied! <pre>from pydantic import BaseModel\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\nmodel = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n\nchat_template = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(content=\"Extract the event information.\"),\n        HumanMessage(content=\"Alice and Bob are going to a science fair on Friday.\"),\n    ]\n)\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\n# Bind schema to model\nmodel_with_tools = model.with_structured_output(CalendarEvent)\n\n# Invoke the model to produce structured output that matches the schema\nresponse = model_with_tools.invoke(chat_template.format_messages())\n</pre> from pydantic import BaseModel  from langchain_openai import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate from langchain_core.messages import SystemMessage, HumanMessage  model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)  chat_template = ChatPromptTemplate.from_messages(     [         SystemMessage(content=\"Extract the event information.\"),         HumanMessage(content=\"Alice and Bob are going to a science fair on Friday.\"),     ] )  class CalendarEvent(BaseModel):     name: str     date: str     participants: list[str]  # Bind schema to model model_with_tools = model.with_structured_output(CalendarEvent)  # Invoke the model to produce structured output that matches the schema response = model_with_tools.invoke(chat_template.format_messages())  In\u00a0[3]: Copied! <pre>print(response)\n</pre> print(response) <pre>name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n</pre>"},{"location":"dia1/notebooks/structured-outputs/#extra-structured-outputs-con-openai-y-langchain","title":"Extra: Structured Outputs con OpenAI y Langchain\u00b6","text":"<p>Para mayor informaci\u00f3n: https://python.langchain.com/docs/concepts/structured_outputs/</p>"},{"location":"dia1/notebooks/structured-outputs/#openai","title":"OpenAI\u00b6","text":""},{"location":"dia1/notebooks/structured-outputs/#langchain-sobre-openai","title":"Langchain (sobre OpenAI)\u00b6","text":""},{"location":"dia2/clases/","title":"Workflows, Agentes y LangGraph","text":""},{"location":"dia2/notebooks/agentic-rag/","title":"Agentic RAG","text":"<p>It involves a series of RAG-related steps in which decisions need to be made, with the help of an LLM. In this case, we want to decide whether to retrieve context from a vector store or respond to the user directly.</p> <p></p> In\u00a0[1]: Copied! <pre>from dotenv import load_dotenv\nimport pandas as pd\nfrom pathlib import Path\nimport json\nfrom dotenv import load_dotenv\nimport os \nfrom IPython.display import Image, display, Markdown\nimport pprint\n</pre> from dotenv import load_dotenv import pandas as pd from pathlib import Path import json from dotenv import load_dotenv import os  from IPython.display import Image, display, Markdown import pprint <p>Import the necessary classes from Langchain and Langgraph</p> In\u00a0[2]: Copied! <pre>from langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.tools.retriever import create_retriever_tool\nfrom langgraph.graph import MessagesState\nfrom langchain.chat_models import init_chat_model\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nfrom langchain_core.messages import convert_to_messages\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.prebuilt import tools_condition\nfrom langchain_core.messages import BaseMessage, HumanMessage\n</pre> from langchain.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter  from langchain_core.vectorstores import InMemoryVectorStore from langchain_openai import OpenAIEmbeddings from langchain.tools.retriever import create_retriever_tool from langgraph.graph import MessagesState from langchain.chat_models import init_chat_model from pydantic import BaseModel, Field from typing import Literal from langchain_core.messages import convert_to_messages from langgraph.graph import StateGraph, START, END from langgraph.prebuilt import ToolNode from langgraph.prebuilt import tools_condition from langchain_core.messages import BaseMessage, HumanMessage  In\u00a0[3]: Copied! <pre># %pip install pypdf langgraph\n</pre> # %pip install pypdf langgraph In\u00a0[3]: Copied! <pre># Load environment variables from .env file\nload_dotenv()\n</pre> # Load environment variables from .env file load_dotenv() Out[3]: <pre>True</pre> In\u00a0[4]: Copied! <pre># We consider a large PDF file\npdf_path = \"./data/Understanding_Climate_Change.pdf\"\n\nloader = PyPDFLoader(pdf_path)\npdf_documents = loader.load() # Each document corresponds actually to a page\nprint(len(pdf_documents), \"loaded\")\n</pre> # We consider a large PDF file pdf_path = \"./data/Understanding_Climate_Change.pdf\"  loader = PyPDFLoader(pdf_path) pdf_documents = loader.load() # Each document corresponds actually to a page print(len(pdf_documents), \"loaded\") <pre>33 loaded\n</pre> In\u00a0[5]: Copied! <pre>def replace_t_with_space(list_of_documents):\n    for doc in list_of_documents:\n        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n    return list_of_documents\n\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=100)\n\ntexts = text_splitter.split_documents(pdf_documents)\ndoc_splits = replace_t_with_space(texts)\nprint(len(doc_splits), \"chunks\")\n</pre> def replace_t_with_space(list_of_documents):     for doc in list_of_documents:         doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces     return list_of_documents   # Split documents into chunks text_splitter = RecursiveCharacterTextSplitter(     chunk_size=1000, chunk_overlap=100)  texts = text_splitter.split_documents(pdf_documents) doc_splits = replace_t_with_space(texts) print(len(doc_splits), \"chunks\") <pre>97 chunks\n</pre> <p>For simplicity, let's use an in-memory vector store with a retriever</p> In\u00a0[6]: Copied! <pre>vectorstore = InMemoryVectorStore.from_documents(\n    documents=doc_splits, embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n</pre> vectorstore = InMemoryVectorStore.from_documents(     documents=doc_splits, embedding=OpenAIEmbeddings() ) retriever = vectorstore.as_retriever() <p>The retriever is adapted as a tool for the workflow</p> In\u00a0[7]: Copied! <pre>retriever_tool = create_retriever_tool(\n    retriever,\n    \"retrieve_doc_info\",\n    \"Search and return information about climate change.\",\n)\n</pre> retriever_tool = create_retriever_tool(     retriever,     \"retrieve_doc_info\",     \"Search and return information about climate change.\", ) In\u00a0[8]: Copied! <pre>query = \"what are the main reasons for climate change?\"\nresult = retriever_tool.invoke({\"query\": query})\npprint.pprint(result)\n</pre> query = \"what are the main reasons for climate change?\" result = retriever_tool.invoke({\"query\": query}) pprint.pprint(result)  <pre>('change the amount of solar energy our planet receives. During the Holocene '\n 'epoch, which \\n'\n 'began at the end of the last ice age, human societies flourished, but the '\n 'industrial era has seen \\n'\n 'unprecedented changes. \\n'\n 'Modern Observations \\n'\n 'Modern scientific observations indicate a rapid increase in global '\n 'temperatures, sea levels, \\n'\n 'and extreme weather events. The Intergovernmental Panel on Climate Change '\n '(IPCC) has \\n'\n 'documented these changes extensively. Ice core samples, tree rings, and '\n 'ocean sediments \\n'\n 'provide a historical record that scientists use to understand past climate '\n 'conditions and \\n'\n 'predict future trends. The evidence overwhelmingly shows that recent changes '\n 'are primarily \\n'\n 'driven by human activities, particularly the emission of greenhouse gases. \\n'\n 'Chapter 2: Causes of Climate Change \\n'\n 'Greenhouse Gases \\n'\n 'The primary cause of recent climate change is the increase in greenhouse '\n 'gases in the \\n'\n 'atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), '\n 'and nitrous\\n'\n '\\n'\n 'Climate change is linked to an increase in the frequency and severity of '\n 'extreme weather \\n'\n 'events, such as hurricanes, heatwaves, droughts, and heavy rainfall. These '\n 'events can have \\n'\n 'devastating impacts on communities, economies, and ecosystems. \\n'\n 'Hurricanes and Typhoons \\n'\n 'Warmer ocean temperatures can intensify hurricanes and typhoons, leading to '\n 'more \\n'\n 'destructive storms. Coastal regions are at heightened risk of storm surge '\n 'and flooding. Early \\n'\n 'warning systems and resilient infrastructure are critical for mitigating '\n 'these risks. \\n'\n 'Droughts \\n'\n 'Increased temperatures and changing precipitation patterns are contributing '\n 'to more frequent \\n'\n 'and severe droughts. This affects agriculture, water supply, and ecosystems, '\n 'particularly in \\n'\n 'arid and semi-arid regions. Droughts can lead to food and water shortages '\n 'and exacerbate \\n'\n 'conflicts. \\n'\n 'Flooding \\n'\n 'Heavy rainfall events are becoming more common, leading to increased '\n 'flooding. Urban\\n'\n '\\n'\n \"agricultural sector's carbon footprint. \\n\"\n 'Chapter 3: Effects of Climate Change \\n'\n 'The effects of climate change are already being felt around the world and '\n 'are projected to \\n'\n 'intensify in the coming decades. These effects include: \\n'\n 'Rising Temperatures \\n'\n 'Global temperatures have risen by about 1.2 degrees Celsius (2.2 degrees '\n 'Fahrenheit) since \\n'\n 'the late 19th century. This warming is not uniform, with some regions '\n 'experiencing more \\n'\n 'significant increases than others. \\n'\n 'Heatwaves \\n'\n 'Heatwaves are becoming more frequent and severe, posing risks to human '\n 'health, agriculture, \\n'\n 'and infrastructure. Cities are particularly vulnerable due to the \"urban '\n 'heat island\" effect. \\n'\n 'Heatwaves can lead to heat-related illnesses and exacerbate existing health '\n 'conditions. \\n'\n 'Changing Seasons \\n'\n 'Climate change is altering the timing and length of seasons, affecting '\n 'ecosystems and human \\n'\n 'activities. For example, spring is arriving earlier, and winters are '\n 'becoming shorter and\\n'\n '\\n'\n 'Understanding Climate Change \\n'\n 'Chapter 1: Introduction to Climate Change \\n'\n 'Climate change refers to significant, long-term changes in the global '\n 'climate. The term \\n'\n '\"global climate\" encompasses the planet\\'s overall weather patterns, '\n 'including temperature, \\n'\n 'precipitation, and wind patterns, over an extended period. Over the past '\n 'century, human \\n'\n 'activities, particularly the burning of fossil fuels and deforestation, have '\n 'significantly \\n'\n 'contributed to climate change. \\n'\n 'Historical Context \\n'\n \"The Earth's climate has changed throughout history. Over the past 650,000 \"\n 'years, there have \\n'\n 'been seven cycles of glacial advance and retreat, with the abrupt end of the '\n 'last ice age about \\n'\n '11,700 years ago marking the beginning of the modern climate era and human '\n 'civilization. \\n'\n 'Most of these climate changes are attributed to very small variations in '\n \"Earth's orbit that \\n\"\n 'change the amount of solar energy our planet receives. During the Holocene '\n 'epoch, which')\n</pre> <p>All the nodes will work on a MessageState, which contains a key and a list of chat messages</p> In\u00a0[9]: Copied! <pre>llm_model = \"openai:\"+os.getenv(\"OPENAI_MODEL\")\nprint(llm_model)\nresponse_model = init_chat_model(llm_model, temperature=0)\n\ndef generate_query_or_respond(state: MessagesState):\n    \"\"\"Call the model to generate a response based on the current state. Given\n    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n    \"\"\"\n    response = (\n        response_model\n        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n    )\n    return {\"messages\": [response]}\n</pre> llm_model = \"openai:\"+os.getenv(\"OPENAI_MODEL\") print(llm_model) response_model = init_chat_model(llm_model, temperature=0)  def generate_query_or_respond(state: MessagesState):     \"\"\"Call the model to generate a response based on the current state. Given     the question, it will decide to retrieve using the retriever tool, or simply respond to the user.     \"\"\"     response = (         response_model         .bind_tools([retriever_tool]).invoke(state[\"messages\"])     )     return {\"messages\": [response]} <pre>openai:gpt-4o-mini\n</pre> <p>Testing the generation (node) with different inputs</p> In\u00a0[10]: Copied! <pre>input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]}\ngenerate_query_or_respond(input)[\"messages\"][-1].pretty_print()\n</pre> input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]} generate_query_or_respond(input)[\"messages\"][-1].pretty_print() <pre>================================== Ai Message ==================================\n\nHello! How can I assist you today?\n</pre> In\u00a0[11]: Copied! <pre>input = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        }\n    ]\n}\ngenerate_query_or_respond(input)[\"messages\"][-1].pretty_print()\n</pre> input = {     \"messages\": [         {             \"role\": \"user\",             \"content\": query,         }     ] } generate_query_or_respond(input)[\"messages\"][-1].pretty_print() <pre>================================== Ai Message ==================================\nTool Calls:\n  retrieve_doc_info (call_q7W3w8P4xW3joh1Y4y1XWvY9)\n Call ID: call_q7W3w8P4xW3joh1Y4y1XWvY9\n  Args:\n    query: main reasons for climate change\n</pre> <p>This is s a conditional edge (in the graph) to determine whether the retrieved documents are relevant to the question.</p> <p>We use a Pydantic model to record the result of the LLM decision.</p> In\u00a0[12]: Copied! <pre>GRADE_PROMPT = (\n    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n    \"Here is the user question: {question} \\n\"\n    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n)\n\nclass GradeDocuments(BaseModel):\n    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n    )\n\ngrader_model = init_chat_model(llm_model, temperature=0)\n</pre> GRADE_PROMPT = (     \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"     \"Here is the retrieved document: \\n\\n {context} \\n\\n\"     \"Here is the user question: {question} \\n\"     \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"     \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\" )  class GradeDocuments(BaseModel):     \"\"\"Grade documents using a binary score for relevance check.\"\"\"      binary_score: str = Field(         description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"     )  grader_model = init_chat_model(llm_model, temperature=0) In\u00a0[13]: Copied! <pre>def grade_documents(\n    state: MessagesState,\n) -&gt; Literal[\"generate_answer\", \"rewrite_question\"]:\n    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n    question = state[\"messages\"][0].content\n    context = state[\"messages\"][-1].content\n\n    prompt = GRADE_PROMPT.format(question=question, context=context)\n    response = (\n        grader_model\n        .with_structured_output(GradeDocuments).invoke(\n            [{\"role\": \"user\", \"content\": prompt}]\n        )\n    )\n    score = response.binary_score\n\n    if score == \"yes\":\n        return \"generate_answer\"\n    else:\n        return \"rewrite_question\"\n</pre> def grade_documents(     state: MessagesState, ) -&gt; Literal[\"generate_answer\", \"rewrite_question\"]:     \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"     question = state[\"messages\"][0].content     context = state[\"messages\"][-1].content      prompt = GRADE_PROMPT.format(question=question, context=context)     response = (         grader_model         .with_structured_output(GradeDocuments).invoke(             [{\"role\": \"user\", \"content\": prompt}]         )     )     score = response.binary_score      if score == \"yes\":         return \"generate_answer\"     else:         return \"rewrite_question\" <p>Testing the grading with different inputs</p> In\u00a0[14]: Copied! <pre>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": query,\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_doc_info\",\n                        \"args\": {\"query\": \"types of climate adverse effects\"},\n                    }\n                ],\n            },\n            {\"role\": \"tool\", \"content\": \"hacking news\", \"tool_call_id\": \"1\"},\n        ]\n    )\n}\ngrade_documents(input)\n</pre> input = {     \"messages\": convert_to_messages(         [             {                 \"role\": \"user\",                 \"content\": query,             },             {                 \"role\": \"assistant\",                 \"content\": \"\",                 \"tool_calls\": [                     {                         \"id\": \"1\",                         \"name\": \"retrieve_doc_info\",                         \"args\": {\"query\": \"types of climate adverse effects\"},                     }                 ],             },             {\"role\": \"tool\", \"content\": \"hacking news\", \"tool_call_id\": \"1\"},         ]     ) } grade_documents(input) Out[14]: <pre>'rewrite_question'</pre> In\u00a0[15]: Copied! <pre>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": query,\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_doc_info\",\n                        \"args\": {\"query\": \"is climate change a threat to humanity?\"},\n                    }\n                ],\n            },\n            {\n                \"role\": \"tool\",\n                \"content\": \"one of the reasons for climate change is deforestation\",\n                \"tool_call_id\": \"1\",\n            },\n        ]\n    )\n}\ngrade_documents(input)\n</pre> input = {     \"messages\": convert_to_messages(         [             {                 \"role\": \"user\",                 \"content\": query,             },             {                 \"role\": \"assistant\",                 \"content\": \"\",                 \"tool_calls\": [                     {                         \"id\": \"1\",                         \"name\": \"retrieve_doc_info\",                         \"args\": {\"query\": \"is climate change a threat to humanity?\"},                     }                 ],             },             {                 \"role\": \"tool\",                 \"content\": \"one of the reasons for climate change is deforestation\",                 \"tool_call_id\": \"1\",             },         ]     ) } grade_documents(input) Out[15]: <pre>'generate_answer'</pre> <p>Since the retrieval node can return irrelevant documents, one way of improving the retrieval is to re-write the original question</p> In\u00a0[16]: Copied! <pre>REWRITE_PROMPT = (\n    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n    \"Here is the initial question:\"\n    \"\\n ------- \\n\"\n    \"{question}\"\n    \"\\n ------- \\n\"\n    \"Formulate an improved question:\"\n)\n</pre> REWRITE_PROMPT = (     \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"     \"Here is the initial question:\"     \"\\n ------- \\n\"     \"{question}\"     \"\\n ------- \\n\"     \"Formulate an improved question:\" ) In\u00a0[17]: Copied! <pre>def rewrite_question(state: MessagesState):\n    \"\"\"Rewrite the original user question.\"\"\"\n    messages = state[\"messages\"]\n    question = messages[0].content\n    prompt = REWRITE_PROMPT.format(question=question)\n    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}\n</pre> def rewrite_question(state: MessagesState):     \"\"\"Rewrite the original user question.\"\"\"     messages = state[\"messages\"]     question = messages[0].content     prompt = REWRITE_PROMPT.format(question=question)     response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])     return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]} <p>Testing the node with different inputs</p> In\u00a0[18]: Copied! <pre>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": query,\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_doc_info\",\n                        \"args\": {\"query\": \"is climate change a threat to humanity?\"},\n                    }\n                ],\n            },\n            {\"role\": \"tool\", \"content\": \"deforestation\", \"tool_call_id\": \"1\"},\n        ]\n    )\n}\n\nresponse = rewrite_question(input)\nprint(response[\"messages\"][-1][\"content\"])\n</pre> input = {     \"messages\": convert_to_messages(         [             {                 \"role\": \"user\",                 \"content\": query,             },             {                 \"role\": \"assistant\",                 \"content\": \"\",                 \"tool_calls\": [                     {                         \"id\": \"1\",                         \"name\": \"retrieve_doc_info\",                         \"args\": {\"query\": \"is climate change a threat to humanity?\"},                     }                 ],             },             {\"role\": \"tool\", \"content\": \"deforestation\", \"tool_call_id\": \"1\"},         ]     ) }  response = rewrite_question(input) print(response[\"messages\"][-1][\"content\"]) <pre>What are the primary factors contributing to climate change?\n</pre> <p>Finally, we ask the LLM to generate a response as a normal RAG procedure.</p> In\u00a0[19]: Copied! <pre>GENERATE_PROMPT = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer the question. \"\n    \"If you don't know the answer, just say that you don't know. \"\n    \"Use three sentences maximum and keep the answer concise.\\n\"\n    \"Question: {question} \\n\"\n    \"Context: {context}\"\n)\n\n\ndef generate_answer(state: MessagesState):\n    \"\"\"Generate an answer.\"\"\"\n    question = state[\"messages\"][0].content\n    context = state[\"messages\"][-1].content\n    prompt = GENERATE_PROMPT.format(question=question, context=context)\n    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n    return {\"messages\": [response]}\n</pre> GENERATE_PROMPT = (     \"You are an assistant for question-answering tasks. \"     \"Use the following pieces of retrieved context to answer the question. \"     \"If you don't know the answer, just say that you don't know. \"     \"Use three sentences maximum and keep the answer concise.\\n\"     \"Question: {question} \\n\"     \"Context: {context}\" )   def generate_answer(state: MessagesState):     \"\"\"Generate an answer.\"\"\"     question = state[\"messages\"][0].content     context = state[\"messages\"][-1].content     prompt = GENERATE_PROMPT.format(question=question, context=context)     response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])     return {\"messages\": [response]} In\u00a0[20]: Copied! <pre>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": query,\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_doc_info\",\n                        \"args\": {\"query\": \"is climate change a threat to humanity?\"},\n                    }\n                ],\n            },\n            {\n                \"role\": \"tool\",\n                \"content\": \"primary factors contributing to climate change include deforestation, and others such as ...\",\n                \"tool_call_id\": \"1\",\n            },\n        ]\n    )\n}\n\nresponse = generate_answer(input)\nresponse[\"messages\"][-1].pretty_print()\n</pre> input = {     \"messages\": convert_to_messages(         [             {                 \"role\": \"user\",                 \"content\": query,             },             {                 \"role\": \"assistant\",                 \"content\": \"\",                 \"tool_calls\": [                     {                         \"id\": \"1\",                         \"name\": \"retrieve_doc_info\",                         \"args\": {\"query\": \"is climate change a threat to humanity?\"},                     }                 ],             },             {                 \"role\": \"tool\",                 \"content\": \"primary factors contributing to climate change include deforestation, and others such as ...\",                 \"tool_call_id\": \"1\",             },         ]     ) }  response = generate_answer(input) response[\"messages\"][-1].pretty_print() <pre>================================== Ai Message ==================================\n\nThe main reasons for climate change include deforestation, greenhouse gas emissions from fossil fuel combustion, and industrial activities. Additionally, agricultural practices and waste management also contribute significantly. These factors lead to an increase in atmospheric carbon dioxide and other greenhouse gases, driving global warming.\n</pre> In\u00a0[21]: Copied! <pre>workflow = StateGraph(MessagesState)\n\n# Define the nodes we will cycle between\nworkflow.add_node(generate_query_or_respond)\nworkflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\nworkflow.add_node(rewrite_question)\nworkflow.add_node(generate_answer)\n\nworkflow.add_edge(START, \"generate_query_or_respond\")\n\n# Decide whether to retrieve\nworkflow.add_conditional_edges(\n    \"generate_query_or_respond\",\n    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n    tools_condition,\n    {\n        # Translate the condition outputs to nodes in our graph\n        \"tools\": \"retrieve\",\n        END: END,\n    },\n)\n\n# Edges taken after the `action` node is called.\nworkflow.add_conditional_edges(\n    \"retrieve\",\n    # Assess agent decision\n    grade_documents,\n)\nworkflow.add_edge(\"generate_answer\", END)\nworkflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n\n# Compile\ngraph = workflow.compile()\n</pre> workflow = StateGraph(MessagesState)  # Define the nodes we will cycle between workflow.add_node(generate_query_or_respond) workflow.add_node(\"retrieve\", ToolNode([retriever_tool])) workflow.add_node(rewrite_question) workflow.add_node(generate_answer)  workflow.add_edge(START, \"generate_query_or_respond\")  # Decide whether to retrieve workflow.add_conditional_edges(     \"generate_query_or_respond\",     # Assess LLM decision (call `retriever_tool` tool or respond to the user)     tools_condition,     {         # Translate the condition outputs to nodes in our graph         \"tools\": \"retrieve\",         END: END,     }, )  # Edges taken after the `action` node is called. workflow.add_conditional_edges(     \"retrieve\",     # Assess agent decision     grade_documents, ) workflow.add_edge(\"generate_answer\", END) workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")  # Compile graph = workflow.compile() In\u00a0[22]: Copied! <pre>display(Image(graph.get_graph().draw_mermaid_png()))\n</pre> display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[23]: Copied! <pre>for chunk in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": query,\n            }\n        ]\n    }\n):\n    for node, update in chunk.items():\n        print(\"Update from node\", node)\n        update[\"messages\"][-1].pretty_print()\n        print(\"\\n\\n\")\n</pre> for chunk in graph.stream(     {         \"messages\": [             {                 \"role\": \"user\",                 \"content\": query,             }         ]     } ):     for node, update in chunk.items():         print(\"Update from node\", node)         update[\"messages\"][-1].pretty_print()         print(\"\\n\\n\") <pre>Update from node generate_query_or_respond\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_doc_info (call_DkBlcT6WozFMVb5BHJ2zbq0N)\n Call ID: call_DkBlcT6WozFMVb5BHJ2zbq0N\n  Args:\n    query: main reasons for climate change\n\n\n\nUpdate from node retrieve\n================================= Tool Message =================================\nName: retrieve_doc_info\n\nClimate change is linked to an increase in the frequency and severity of extreme weather \nevents, such as hurricanes, heatwaves, droughts, and heavy rainfall. These events can have \ndevastating impacts on communities, economies, and ecosystems. \nHurricanes and Typhoons \nWarmer ocean temperatures can intensify hurricanes and typhoons, leading to more \ndestructive storms. Coastal regions are at heightened risk of storm surge and flooding. Early \nwarning systems and resilient infrastructure are critical for mitigating these risks. \nDroughts \nIncreased temperatures and changing precipitation patterns are contributing to more frequent \nand severe droughts. This affects agriculture, water supply, and ecosystems, particularly in \narid and semi-arid regions. Droughts can lead to food and water shortages and exacerbate \nconflicts. \nFlooding \nHeavy rainfall events are becoming more common, leading to increased flooding. Urban\n\nchange the amount of solar energy our planet receives. During the Holocene epoch, which \nbegan at the end of the last ice age, human societies flourished, but the industrial era has seen \nunprecedented changes. \nModern Observations \nModern scientific observations indicate a rapid increase in global temperatures, sea levels, \nand extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \ndocumented these changes extensively. Ice core samples, tree rings, and ocean sediments \nprovide a historical record that scientists use to understand past climate conditions and \npredict future trends. The evidence overwhelmingly shows that recent changes are primarily \ndriven by human activities, particularly the emission of greenhouse gases. \nChapter 2: Causes of Climate Change \nGreenhouse Gases \nThe primary cause of recent climate change is the increase in greenhouse gases in the \natmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous\n\nto habitat loss and species extinction. \nBoreal Forests \nBoreal forests, found in the northern regions of North America, Europe, and Asia, also play a \ncrucial role in sequestering carbon. Logging and land-use changes in these regions contribute \nto climate change. These forests are vital for regulating the Earth's climate and supporting \nindigenous communities and wildlife. \nAgriculture \nAgriculture contributes to climate change through methane emissions from livestock, rice \npaddies, and the use of synthetic fertilizers. Methane is a potent greenhouse gas with a much \nhigher heat-trapping capability than CO2, albeit in smaller quantities. \nLivestock Emissions\n\nUnderstanding Climate Change \nChapter 1: Introduction to Climate Change \nClimate change refers to significant, long-term changes in the global climate. The term \n\"global climate\" encompasses the planet's overall weather patterns, including temperature, \nprecipitation, and wind patterns, over an extended period. Over the past century, human \nactivities, particularly the burning of fossil fuels and deforestation, have significantly \ncontributed to climate change. \nHistorical Context \nThe Earth's climate has changed throughout history. Over the past 650,000 years, there have \nbeen seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n11,700 years ago marking the beginning of the modern climate era and human civilization. \nMost of these climate changes are attributed to very small variations in Earth's orbit that \nchange the amount of solar energy our planet receives. During the Holocene epoch, which\n\n\n\nUpdate from node generate_answer\n================================== Ai Message ==================================\n\nThe main reasons for climate change include the increase in greenhouse gases, such as carbon dioxide and methane, primarily due to human activities like burning fossil fuels and deforestation. Additionally, agriculture contributes to climate change through methane emissions from livestock and rice paddies. These factors lead to significant long-term changes in global weather patterns and increased frequency of extreme weather events.\n\n\n\n</pre> In\u00a0[24]: Copied! <pre>inputs = [HumanMessage(content=query)]\nresult = graph.invoke({\"messages\": inputs})\ndisplay(Markdown(result['messages'][-1].content))\n</pre> inputs = [HumanMessage(content=query)] result = graph.invoke({\"messages\": inputs}) display(Markdown(result['messages'][-1].content)) <p>The main reasons for climate change include the increase in greenhouse gases, such as carbon dioxide and methane, primarily due to human activities like burning fossil fuels and deforestation. Additionally, agriculture contributes to climate change through methane emissions from livestock and rice paddies. These factors lead to significant, long-term changes in global weather patterns and extreme weather events.</p> In\u00a0[26]: Copied! <pre>result\n</pre> result Out[26]: <pre>{'messages': [HumanMessage(content='what are the main reasons for climate change?', additional_kwargs={}, response_metadata={}, id='d2be4aa6-54fd-4294-b2d2-6ffd76fc6527'),\n  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kQZwACZot0QewUm2wngtbOZo', 'function': {'arguments': '{\"query\":\"main reasons for climate change\"}', 'name': 'retrieve_doc_info'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 64, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-C9HWDkoN2Q2y5HmxZ1vw38RrcLhmv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c396972d-895a-468d-ba12-12482405318d-0', tool_calls=[{'name': 'retrieve_doc_info', 'args': {'query': 'main reasons for climate change'}, 'id': 'call_kQZwACZot0QewUm2wngtbOZo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 64, 'output_tokens': 19, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n  ToolMessage(content='Climate change is linked to an increase in the frequency and severity of extreme weather \\nevents, such as hurricanes, heatwaves, droughts, and heavy rainfall. These events can have \\ndevastating impacts on communities, economies, and ecosystems. \\nHurricanes and Typhoons \\nWarmer ocean temperatures can intensify hurricanes and typhoons, leading to more \\ndestructive storms. Coastal regions are at heightened risk of storm surge and flooding. Early \\nwarning systems and resilient infrastructure are critical for mitigating these risks. \\nDroughts \\nIncreased temperatures and changing precipitation patterns are contributing to more frequent \\nand severe droughts. This affects agriculture, water supply, and ecosystems, particularly in \\narid and semi-arid regions. Droughts can lead to food and water shortages and exacerbate \\nconflicts. \\nFlooding \\nHeavy rainfall events are becoming more common, leading to increased flooding. Urban\\n\\nchange the amount of solar energy our planet receives. During the Holocene epoch, which \\nbegan at the end of the last ice age, human societies flourished, but the industrial era has seen \\nunprecedented changes. \\nModern Observations \\nModern scientific observations indicate a rapid increase in global temperatures, sea levels, \\nand extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \\ndocumented these changes extensively. Ice core samples, tree rings, and ocean sediments \\nprovide a historical record that scientists use to understand past climate conditions and \\npredict future trends. The evidence overwhelmingly shows that recent changes are primarily \\ndriven by human activities, particularly the emission of greenhouse gases. \\nChapter 2: Causes of Climate Change \\nGreenhouse Gases \\nThe primary cause of recent climate change is the increase in greenhouse gases in the \\natmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous\\n\\nto habitat loss and species extinction. \\nBoreal Forests \\nBoreal forests, found in the northern regions of North America, Europe, and Asia, also play a \\ncrucial role in sequestering carbon. Logging and land-use changes in these regions contribute \\nto climate change. These forests are vital for regulating the Earth\\'s climate and supporting \\nindigenous communities and wildlife. \\nAgriculture \\nAgriculture contributes to climate change through methane emissions from livestock, rice \\npaddies, and the use of synthetic fertilizers. Methane is a potent greenhouse gas with a much \\nhigher heat-trapping capability than CO2, albeit in smaller quantities. \\nLivestock Emissions\\n\\nUnderstanding Climate Change \\nChapter 1: Introduction to Climate Change \\nClimate change refers to significant, long-term changes in the global climate. The term \\n\"global climate\" encompasses the planet\\'s overall weather patterns, including temperature, \\nprecipitation, and wind patterns, over an extended period. Over the past century, human \\nactivities, particularly the burning of fossil fuels and deforestation, have significantly \\ncontributed to climate change. \\nHistorical Context \\nThe Earth\\'s climate has changed throughout history. Over the past 650,000 years, there have \\nbeen seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \\n11,700 years ago marking the beginning of the modern climate era and human civilization. \\nMost of these climate changes are attributed to very small variations in Earth\\'s orbit that \\nchange the amount of solar energy our planet receives. During the Holocene epoch, which', name='retrieve_doc_info', id='d4261e0b-f79d-4fe8-91c6-f5055e87bf4e', tool_call_id='call_kQZwACZot0QewUm2wngtbOZo'),\n  AIMessage(content='The main reasons for climate change include the increase in greenhouse gases, such as carbon dioxide and methane, primarily due to human activities like burning fossil fuels and deforestation. Additionally, agriculture contributes to climate change through methane emissions from livestock and rice paddies. These factors lead to significant, long-term changes in global weather patterns and extreme weather events.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 770, 'total_tokens': 838, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C9HWF4ecgFLtPGYlwxPqELCXDirxR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c0628341-24e1-48f9-a518-653ff1b77a27-0', usage_metadata={'input_tokens': 770, 'output_tokens': 68, 'total_tokens': 838, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}</pre> <p></p>"},{"location":"dia2/notebooks/agentic-rag/#agentic-rag","title":"Agentic RAG\u00b6","text":""},{"location":"dia2/notebooks/agentic-rag/#ingest-the-documents-and-convert-to-chunks","title":"Ingest the document(s) and convert to chunks\u00b6","text":""},{"location":"dia2/notebooks/agentic-rag/#1-query-generation","title":"1. Query Generation\u00b6","text":""},{"location":"dia2/notebooks/agentic-rag/#2-document-grading","title":"2. Document Grading\u00b6","text":""},{"location":"dia2/notebooks/agentic-rag/#3-question-rewriting","title":"3. Question Rewriting\u00b6","text":""},{"location":"dia2/notebooks/agentic-rag/#4-answer-generation","title":"4. Answer Generation\u00b6","text":""},{"location":"dia2/notebooks/agentic-rag/#assembling-the-graph-workflow","title":"Assembling the Graph (workflow)\u00b6","text":""},{"location":"dia2/notebooks/agentic-rag/#follow-up","title":"Follow-up\u00b6","text":"<p>Can we add additional steps to the RAG workflow?</p> <ul> <li>Check if the generation has hallucinations</li> <li>Use Web search as an additional source of information</li> </ul> <p>Adaptive RAG: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/</p>"},{"location":"dia2/notebooks/langchain-tool-call/","title":"LangChain Tool Calling","text":"In\u00a0[13]: Copied! <pre>#\u00a0%pip install pydantic python-dotenv langchain langchain-openai langchain-community\n</pre> #\u00a0%pip install pydantic python-dotenv langchain langchain-openai langchain-community In\u00a0[14]: Copied! <pre>from dotenv import load_dotenv\n\nload_dotenv()\n</pre> from dotenv import load_dotenv  load_dotenv() Out[14]: <pre>True</pre> In\u00a0[15]: Copied! <pre>from langchain_core.tools import tool\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"\n    Generate a weather message for a given location.\n\n    Args:\n        location (str): The location for which to generate the weather report.\n\n    Returns:\n        str: A weather message for the specified location.\n    \"\"\"\n    return f\"The weather in {location} is sunny with a chance of clouds.\"\n</pre> from langchain_core.tools import tool  @tool def get_weather(location: str) -&gt; str:     \"\"\"     Generate a weather message for a given location.      Args:         location (str): The location for which to generate the weather report.      Returns:         str: A weather message for the specified location.     \"\"\"     return f\"The weather in {location} is sunny with a chance of clouds.\" In\u00a0[16]: Copied! <pre># Tools conform to the Runnable interface, which means you can run a tool using the invoke method\n\nget_weather.invoke(\"Tandil\")\n</pre> # Tools conform to the Runnable interface, which means you can run a tool using the invoke method  get_weather.invoke(\"Tandil\") Out[16]: <pre>'The weather in Tandil is sunny with a chance of clouds.'</pre> In\u00a0[17]: Copied! <pre>tool_call = {\n    \"type\": \"tool_call\",\n    \"id\": \"1\",\n    \"args\": { \"location\": \"Mar del Plata\" }\n}\n\nget_weather.invoke(tool_call) # returns a ToolMessage object\n</pre> tool_call = {     \"type\": \"tool_call\",     \"id\": \"1\",     \"args\": { \"location\": \"Mar del Plata\" } }  get_weather.invoke(tool_call) # returns a ToolMessage object Out[17]: <pre>ToolMessage(content='The weather in Mar del Plata is sunny with a chance of clouds.', name='get_weather', tool_call_id='1')</pre> In\u00a0[18]: Copied! <pre>from langchain.chat_models import init_chat_model\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\nSYSTEM_PROMPT = \"\"\"\nYou report the weather of a given location using emojis. \n\nUse the following format for your answers:\n1. Tandil: \u2600\ufe0f - \ud83c\udf21\n2. London: \ufe0f\ud83c\udf27\ufe0f - \u2744\ufe0f\n\"\"\"\n\nmodel = init_chat_model(model=\"gpt-4.1-nano\", temperature=0)\n\nchat_template = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(content=SYSTEM_PROMPT),\n        HumanMessage(content=\"What's the weather like in Paris today?\"),\n    ]\n)\n\n# Bind schema to model\nmodel_with_tools = model.bind_tools([get_weather])\n\n# Invoke the model to produce structured output that matches the schema\nresponse = model_with_tools.invoke(chat_template.format_messages())\n</pre> from langchain.chat_models import init_chat_model from langchain_core.prompts import ChatPromptTemplate from langchain_core.messages import SystemMessage, HumanMessage  SYSTEM_PROMPT = \"\"\" You report the weather of a given location using emojis.   Use the following format for your answers: 1. Tandil: \u2600\ufe0f - \ud83c\udf21 2. London: \ufe0f\ud83c\udf27\ufe0f - \u2744\ufe0f \"\"\"  model = init_chat_model(model=\"gpt-4.1-nano\", temperature=0)  chat_template = ChatPromptTemplate.from_messages(     [         SystemMessage(content=SYSTEM_PROMPT),         HumanMessage(content=\"What's the weather like in Paris today?\"),     ] )  # Bind schema to model model_with_tools = model.bind_tools([get_weather])  # Invoke the model to produce structured output that matches the schema response = model_with_tools.invoke(chat_template.format_messages())  In\u00a0[19]: Copied! <pre>print(response.tool_calls)\n</pre> print(response.tool_calls) <pre>[{'name': 'get_weather', 'args': {'location': 'Paris'}, 'id': 'call_01AXm3pVsMd5DOjFFYP9PtJB', 'type': 'tool_call'}]\n</pre> In\u00a0[20]: Copied! <pre>tool_call = response.tool_calls[0]\n\ntool_return = get_weather.invoke(tool_call)\n</pre> tool_call = response.tool_calls[0]  tool_return = get_weather.invoke(tool_call) In\u00a0[21]: Copied! <pre>tool_return\n</pre> tool_return Out[21]: <pre>ToolMessage(content='The weather in Paris is sunny with a chance of clouds.', name='get_weather', tool_call_id='call_01AXm3pVsMd5DOjFFYP9PtJB')</pre> In\u00a0[22]: Copied! <pre>msgs = chat_template.format_messages() + [response] + [tool_return]\n\nmsgs\n</pre> msgs = chat_template.format_messages() + [response] + [tool_return]  msgs Out[22]: <pre>[SystemMessage(content='\\nYou report the weather of a given location using emojis. \\n\\nUse the following format for your answers:\\n1. Tandil: \u2600\ufe0f - \ud83c\udf21\\n2. London: \ufe0f\ud83c\udf27\ufe0f - \u2744\ufe0f\\n', additional_kwargs={}, response_metadata={}),\n HumanMessage(content=\"What's the weather like in Paris today?\", additional_kwargs={}, response_metadata={}),\n AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_01AXm3pVsMd5DOjFFYP9PtJB', 'function': {'arguments': '{\"location\":\"Paris\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 129, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_950f36939b', 'id': 'chatcmpl-CVT1Q7RJQULYpGtAVj6tu0a46bPOe', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d73c2480-79bb-410a-b4d0-2f3a29a9172e-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Paris'}, 'id': 'call_01AXm3pVsMd5DOjFFYP9PtJB', 'type': 'tool_call'}], usage_metadata={'input_tokens': 129, 'output_tokens': 14, 'total_tokens': 143, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n ToolMessage(content='The weather in Paris is sunny with a chance of clouds.', name='get_weather', tool_call_id='call_01AXm3pVsMd5DOjFFYP9PtJB')]</pre> In\u00a0[23]: Copied! <pre>model_with_tools.invoke(msgs).content\n</pre> model_with_tools.invoke(msgs).content Out[23]: <pre>'Paris: \u2600\ufe0f - \u2601\ufe0f'</pre>"},{"location":"dia2/notebooks/langchain-tool-call/#langchain-tool-calling","title":"LangChain Tool Calling\u00b6","text":"<p>For more information: https://langchain-ai.github.io/langgraph/how-tos/tool-calling/</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/","title":"Langchain multi agent collaboration","text":"In\u00a0[1]: Copied! <pre>#\u00a0!pip install --quiet langchain langchain-openai langchain-community deep-translator\n</pre> #\u00a0!pip install --quiet langchain langchain-openai langchain-community deep-translator In\u00a0[16]: Copied! <pre>import os\n#\u00a0from google.colab import userdata\nimport time\nfrom IPython.display import display, Markdown\nfrom typing import List, Dict\nfrom deep_translator import GoogleTranslator\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\nfrom langchain.memory import ChatMessageHistory\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n</pre> import os #\u00a0from google.colab import userdata import time from IPython.display import display, Markdown from typing import List, Dict from deep_translator import GoogleTranslator  from langchain_openai import ChatOpenAI from langchain_core.messages import AIMessage, HumanMessage, SystemMessage from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_openai import ChatOpenAI from langchain.memory import ChatMessageHistory from langchain_core.runnables.history import RunnableWithMessageHistory from langchain_core.prompts import PromptTemplate from langchain_core.runnables import RunnablePassthrough In\u00a0[17]: Copied! <pre>#\u00a0os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\nos.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\n# Test LLM\nresponse = model.invoke(\"Tell me a joke about data scientists\")\nprint(response.content)\n</pre> #\u00a0os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY') os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')  model = ChatOpenAI(model=\"gpt-4o-mini\")  # Test LLM response = model.invoke(\"Tell me a joke about data scientists\") print(response.content) <pre>Why did the data scientist break up with the statistician?\n\nBecause he found her mean and she thought he was just being biased!\n</pre> In\u00a0[18]: Copied! <pre>class Agent:\n\n    TEMPLATE = \"\"\"You are {name}, a {role}. Your skills include: {skills}.\n\n    Previous conversation history:\n    {chat_history}\n\n    Human: {task}\n    Chatbot:\"\"\"\n\n    def __init__(self, name: str, role: str, skills: List[str]):\n        self.name = name\n        self.role = role\n        self.skills = skills\n        self.model = ChatOpenAI(model=\"gpt-4o-mini\")\n\n    def process(self, task: str, memory: ChatMessageHistory) -&gt; str:\n\n        prompt = PromptTemplate(\n            input_variables=[\"name\", \"role\", \"skills\", \"chat_history\", \"task\"],\n            template=self.TEMPLATE\n            )\n\n        first_step = {\"name\": lambda x: self.name,\n                      \"role\": lambda x: self.role,\n                      \"skills\": lambda x: self.skills,\n                      \"task\": RunnablePassthrough(),\n                      \"chat_history\": RunnablePassthrough(),\n                      }\n        chain = first_step | prompt | self.model\n        with_memory= RunnableWithMessageHistory(\n            chain,\n           # This is needed because in most real world scenarios, a session id is needed\n           # It isn't really used here because we are using a simple in memory ChatMessageHistory\n            lambda session_id: memory,\n            input_messages_key=\"task\",\n            history_messages_key=\"chat_history\"\n            )\n        response = with_memory.invoke(\n            {\"task\": task},\n            config={\"configurable\": {\"session_id\": \"&lt;foo&gt;\"}})\n\n        return response.content  # Retorna el contenido de la respuesta\n</pre> class Agent:      TEMPLATE = \"\"\"You are {name}, a {role}. Your skills include: {skills}.      Previous conversation history:     {chat_history}      Human: {task}     Chatbot:\"\"\"      def __init__(self, name: str, role: str, skills: List[str]):         self.name = name         self.role = role         self.skills = skills         self.model = ChatOpenAI(model=\"gpt-4o-mini\")      def process(self, task: str, memory: ChatMessageHistory) -&gt; str:          prompt = PromptTemplate(             input_variables=[\"name\", \"role\", \"skills\", \"chat_history\", \"task\"],             template=self.TEMPLATE             )          first_step = {\"name\": lambda x: self.name,                       \"role\": lambda x: self.role,                       \"skills\": lambda x: self.skills,                       \"task\": RunnablePassthrough(),                       \"chat_history\": RunnablePassthrough(),                       }         chain = first_step | prompt | self.model         with_memory= RunnableWithMessageHistory(             chain,            # This is needed because in most real world scenarios, a session id is needed            # It isn't really used here because we are using a simple in memory ChatMessageHistory             lambda session_id: memory,             input_messages_key=\"task\",             history_messages_key=\"chat_history\"             )         response = with_memory.invoke(             {\"task\": task},             config={\"configurable\": {\"session_id\": \"\"}})          return response.content  # Retorna el contenido de la respuesta  In\u00a0[19]: Copied! <pre>class HistoryResearchAgent(Agent):\n    def __init__(self):\n        super().__init__(name=\"Clio\", role=\"History Research Specialist\",\n                         skills=[\"deep knowledge of historical events\", \"understanding of historical contexts\", \"identifying historical trends\"])\n\nclass DataAnalysisAgent(Agent):\n    def __init__(self):\n        super().__init__(name=\"Data\", role=\"Data Analysis Expert\",\n                         skills=[\"interpreting numerical data\", \"statistical analysis\", \"data visualization description\"])\n</pre> class HistoryResearchAgent(Agent):     def __init__(self):         super().__init__(name=\"Clio\", role=\"History Research Specialist\",                          skills=[\"deep knowledge of historical events\", \"understanding of historical contexts\", \"identifying historical trends\"])  class DataAnalysisAgent(Agent):     def __init__(self):         super().__init__(name=\"Data\", role=\"Data Analysis Expert\",                          skills=[\"interpreting numerical data\", \"statistical analysis\", \"data visualization description\"]) In\u00a0[20]: Copied! <pre>def research_historical_context(history_agent, task: str, context: ChatMessageHistory) -&gt; ChatMessageHistory:\n    print(\"\ud83c\udfdb\ufe0f History Agent: Researching historical context...\")\n    history_task = f\"Provide relevant historical context and information for the following task: {task}\"\n\n    history_result = history_agent.process(history_task, context)\n    context.add_ai_message(f\"History Agent: {history_result}\")\n    print(f\"\ud83d\udcdc Historical context provided: {history_result[:100]}...\\n\")\n\n    return context\n</pre> def research_historical_context(history_agent, task: str, context: ChatMessageHistory) -&gt; ChatMessageHistory:     print(\"\ud83c\udfdb\ufe0f History Agent: Researching historical context...\")     history_task = f\"Provide relevant historical context and information for the following task: {task}\"      history_result = history_agent.process(history_task, context)     context.add_ai_message(f\"History Agent: {history_result}\")     print(f\"\ud83d\udcdc Historical context provided: {history_result[:100]}...\\n\")      return context In\u00a0[21]: Copied! <pre>def identify_data_needs(data_agent, task: str, context: ChatMessageHistory) -&gt; ChatMessageHistory:\n    print(\"\ud83d\udcca Data Agent: Identifying data needs based on historical context...\")\n    historical_context = context.messages[-1].content\n    data_need_task = f\"Based on the historical context, what specific data or statistical information would be helpful to answer the original question? Historical context: {historical_context}\"\n\n    data_need_result = data_agent.process(data_need_task, context)\n    context.add_ai_message(f\"Data Agent: {data_need_result}\")\n    print(f\"\ud83d\udd0d Data needs identified: {data_need_result[:100]}...\\n\")\n\n    return context\n</pre> def identify_data_needs(data_agent, task: str, context: ChatMessageHistory) -&gt; ChatMessageHistory:     print(\"\ud83d\udcca Data Agent: Identifying data needs based on historical context...\")     historical_context = context.messages[-1].content     data_need_task = f\"Based on the historical context, what specific data or statistical information would be helpful to answer the original question? Historical context: {historical_context}\"      data_need_result = data_agent.process(data_need_task, context)     context.add_ai_message(f\"Data Agent: {data_need_result}\")     print(f\"\ud83d\udd0d Data needs identified: {data_need_result[:100]}...\\n\")      return context  In\u00a0[22]: Copied! <pre>def provide_historical_data(history_agent, task: str, context: ChatMessageHistory) -&gt; ChatMessageHistory:\n    print(\"\ud83c\udfdb\ufe0f History Agent: Providing relevant historical data...\")\n    data_needs = context.messages[-1].content\n    data_provision_task = f\"Based on the data needs identified, provide relevant historical data or statistics. Data needs: {data_needs}\"\n\n    data_provision_result = history_agent.process(data_provision_task, context)\n    context.add_ai_message(f\"History Agent: {data_provision_result}\")\n    print(f\"\ud83d\udcca Historical data provided: {data_provision_result[:100]}...\\n\")\n\n    return context\n</pre> def provide_historical_data(history_agent, task: str, context: ChatMessageHistory) -&gt; ChatMessageHistory:     print(\"\ud83c\udfdb\ufe0f History Agent: Providing relevant historical data...\")     data_needs = context.messages[-1].content     data_provision_task = f\"Based on the data needs identified, provide relevant historical data or statistics. Data needs: {data_needs}\"      data_provision_result = history_agent.process(data_provision_task, context)     context.add_ai_message(f\"History Agent: {data_provision_result}\")     print(f\"\ud83d\udcca Historical data provided: {data_provision_result[:100]}...\\n\")      return context In\u00a0[23]: Copied! <pre>def analyze_data(data_agent, task: str, context: ChatMessageHistory) -&gt; ChatMessageHistory:\n    print(\"\ud83d\udcc8 Data Agent: Analyzing historical data...\")\n    historical_data = context.messages[-1].content\n    analysis_task = f\"Analyze the historical data provided and describe any trends or insights relevant to the original task. Historical data: {historical_data}\"\n\n    analysis_result = data_agent.process(analysis_task, context)\n    context.add_ai_message(f\"Data Agent: {analysis_result}\")\n    print(f\"\ud83d\udca1 Data analysis results: {analysis_result[:100]}...\\n\")\n\n    return context\n</pre> def analyze_data(data_agent, task: str, context: ChatMessageHistory) -&gt; ChatMessageHistory:     print(\"\ud83d\udcc8 Data Agent: Analyzing historical data...\")     historical_data = context.messages[-1].content     analysis_task = f\"Analyze the historical data provided and describe any trends or insights relevant to the original task. Historical data: {historical_data}\"      analysis_result = data_agent.process(analysis_task, context)     context.add_ai_message(f\"Data Agent: {analysis_result}\")     print(f\"\ud83d\udca1 Data analysis results: {analysis_result[:100]}...\\n\")      return context In\u00a0[10]: Copied! <pre>def synthesize_final_answer(history_agent, task: str, context: ChatMessageHistory) -&gt; str:\n    print(\"\ud83c\udfdb\ufe0f History Agent: Synthesizing final answer...\")\n    synthesis_task = \"Based on all the historical context, data, and analysis, provide a comprehensive answer to the original task.\"\n\n    final_result = history_agent.process(synthesis_task, context)\n    context.add_ai_message(final_result)\n\n    return context\n</pre> def synthesize_final_answer(history_agent, task: str, context: ChatMessageHistory) -&gt; str:     print(\"\ud83c\udfdb\ufe0f History Agent: Synthesizing final answer...\")     synthesis_task = \"Based on all the historical context, data, and analysis, provide a comprehensive answer to the original task.\"      final_result = history_agent.process(synthesis_task, context)     context.add_ai_message(final_result)      return context In\u00a0[11]: Copied! <pre>class HistoryDataCollaborationSystem:\n    def __init__(self):\n      self.history_agent = HistoryResearchAgent()\n      self.data_agent = DataAnalysisAgent()\n\n    def solve(self, task: str, timeout: int = 300) -&gt; str:\n        print(f\"\\n\ud83d\udc65 Starting collaboration to solve: {task}\\n\")\n\n        start_time = time.time()\n        memory = ChatMessageHistory(session_id=\"test-session\") # []\n\n        steps = [ # This is the main execution workflow\n            (research_historical_context, self.history_agent),\n            (identify_data_needs, self.data_agent),\n            (provide_historical_data, self.history_agent),\n            (analyze_data, self.data_agent),\n            (synthesize_final_answer, self.history_agent)\n        ]\n\n        for step_func, agent in steps:\n            if time.time() - start_time &gt; timeout:\n                return \"Operation timed out. The process took too long to complete.\"\n            try:\n                result = step_func(agent, task, memory)\n                memory = result\n            except Exception as e:\n                return f\"Error during collaboration: {str(e)}\"\n\n        print(\"\\n\u2705 Collaboration complete. Final answer synthesized.\\n\")\n        return memory.messages[-1].content #\u00a0if memory else \"No content available.\"\n</pre> class HistoryDataCollaborationSystem:     def __init__(self):       self.history_agent = HistoryResearchAgent()       self.data_agent = DataAnalysisAgent()      def solve(self, task: str, timeout: int = 300) -&gt; str:         print(f\"\\n\ud83d\udc65 Starting collaboration to solve: {task}\\n\")          start_time = time.time()         memory = ChatMessageHistory(session_id=\"test-session\") # []          steps = [ # This is the main execution workflow             (research_historical_context, self.history_agent),             (identify_data_needs, self.data_agent),             (provide_historical_data, self.history_agent),             (analyze_data, self.data_agent),             (synthesize_final_answer, self.history_agent)         ]          for step_func, agent in steps:             if time.time() - start_time &gt; timeout:                 return \"Operation timed out. The process took too long to complete.\"             try:                 result = step_func(agent, task, memory)                 memory = result             except Exception as e:                 return f\"Error during collaboration: {str(e)}\"          print(\"\\n\u2705 Collaboration complete. Final answer synthesized.\\n\")         return memory.messages[-1].content #\u00a0if memory else \"No content available.\"  In\u00a0[25]: Copied! <pre># Create an instance of the collaboration system\ncollaboration_system = HistoryDataCollaborationSystem()\n\n# Define a complex historical question that requires both historical knowledge and data analysis\n#\u00a0question = \"How did urbanization rates in Europe compare to those in North America during the Industrial Revolution, and what were the main factors influencing these trends?\"\n#\u00a0question = \"\u00bfComo fue la evolucion del COVID-19?\"\nquestion = \"\u00bfComo ha sido la evoluci\u00f3n de albumes de Taylor Swift?\"\n\n# Solve the question using the collaboration system\nresult = collaboration_system.solve(question)\n\n# Print the result\nprint()\n#\u00a0print(result)\ndisplay(Markdown(result))\n</pre> # Create an instance of the collaboration system collaboration_system = HistoryDataCollaborationSystem()  # Define a complex historical question that requires both historical knowledge and data analysis #\u00a0question = \"How did urbanization rates in Europe compare to those in North America during the Industrial Revolution, and what were the main factors influencing these trends?\" #\u00a0question = \"\u00bfComo fue la evolucion del COVID-19?\" question = \"\u00bfComo ha sido la evoluci\u00f3n de albumes de Taylor Swift?\"  # Solve the question using the collaboration system result = collaboration_system.solve(question)  # Print the result print() #\u00a0print(result) display(Markdown(result)) <pre>\n\ud83d\udc65 Starting collaboration to solve: \u00bfComo ha sido la evoluci\u00f3n de albumes de Taylor Swift?\n\n\ud83c\udfdb\ufe0f History Agent: Researching historical context...\n\ud83d\udcdc Historical context provided: Taylor Swift's musical evolution is a fascinating reflection of both her artistic development and th...\n\n\ud83d\udcca Data Agent: Identifying data needs based on historical context...\n\ud83d\udd0d Data needs identified: To analyze Taylor Swift's musical evolution effectively, specific data and statistical information c...\n\n\ud83c\udfdb\ufe0f History Agent: Providing relevant historical data...\n\ud83d\udcca Historical data provided: To effectively analyze Taylor Swift's musical evolution, the following specific data and statistics ...\n\n\ud83d\udcc8 Data Agent: Analyzing historical data...\n\ud83d\udca1 Data analysis results: To analyze Taylor Swift's musical evolution effectively, several trends emerge from the data provide...\n\n\ud83c\udfdb\ufe0f History Agent: Synthesizing final answer...\n\n\u2705 Collaboration complete. Final answer synthesized.\n\n\n</pre> <p>Taylor Swift's musical evolution is a captivating narrative that spans nearly two decades, marked by her ability to adapt and thrive within the shifting landscape of the music industry. Here\u2019s a comprehensive analysis based on the historical context, relevant data, and insights identified throughout her career:</p> <p></p> In\u00a0[13]: Copied! <pre># TODO: Add your code here\n</pre> # TODO: Add your code here In\u00a0[14]: Copied! <pre>translated = GoogleTranslator(source='auto', target='es').translate(text=result)\ntranslated\n</pre> translated = GoogleTranslator(source='auto', target='es').translate(text=result) translated Out[14]: <pre>'Durante la Revoluci\u00f3n Industrial, que se extendi\u00f3 desde finales del siglo XVIII hasta mediados del siglo XIX, Europa y Am\u00e9rica del Norte experimentaron una urbanizaci\u00f3n significativa a diferentes ritmos, impulsada por diversos factores contextuales. A continuaci\u00f3n se muestra un an\u00e1lisis exhaustivo basado en datos y conocimientos hist\u00f3ricos:\\n\\n### Tarifas de urbanizaci\u00f3n\\n\\n1. **Europa:**\\n   - **Momento de la Urbanizaci\u00f3n:** La Revoluci\u00f3n Industrial se inici\u00f3 en Gran Breta\u00f1a a finales del siglo XVIII. En 1850, los principales centros urbanos como Manchester ten\u00edan poblaciones urbanas superiores al 70%, mientras que ciudades como Londres ten\u00edan alrededor del 50% de su poblaci\u00f3n viviendo en \u00e1reas urbanas.\\n   - **Difusi\u00f3n por el continente:** Otras naciones europeas como Francia y Alemania experimentaron una urbanizaci\u00f3n inicial m\u00e1s lenta; sin embargo, se aceler\u00f3 a medida que la infraestructura mejor\u00f3 y las industrias se expandieron a lo largo del siglo XIX.\\n\\n2. **Am\u00e9rica del Norte:**\\n   - **Urbanizaci\u00f3n gradual:** La urbanizaci\u00f3n en los Estados Unidos se aceler\u00f3 principalmente despu\u00e9s de 1860, cuando la poblaci\u00f3n urbana rondaba el 19%. Para 1900, esto aument\u00f3 a aproximadamente el 40%, destacando un crecimiento urbano significativo, especialmente en ciudades como Nueva York y Chicago.\\n   - **Llegados tard\u00edos:** En comparaci\u00f3n con Europa, el crecimiento urbano en Am\u00e9rica del Norte se produjo m\u00e1s tarde en el siglo XIX, lo que refleja los efectos combinados de la industrializaci\u00f3n, la inmigraci\u00f3n y la expansi\u00f3n ferroviaria.\\n\\n### Factores clave que influyen en las tendencias de urbanizaci\u00f3n\\n\\n1. **Factores econ\u00f3micos:**\\n   - **Europa:** El auge de las f\u00e1bricas y la producci\u00f3n mecanizada atrajo una gran cantidad de mano de obra de las zonas rurales. Esta transici\u00f3n tambi\u00e9n estuvo influenciada por cambios agr\u00edcolas que desplazaron a muchos trabajadores rurales.\\n   - **Am\u00e9rica del Norte:** Factores como el ferrocarril transcontinental y la disponibilidad de tierras facilitaron el movimiento hacia \u00e1reas urbanas y contribuyeron a la creaci\u00f3n de empleo.\\n\\n2. **Factores sociales:**\\n   - **Contexto europeo:** Con una historia m\u00e1s larga de cultura urbana, las ciudades sirvieron como centros culturales y pol\u00edticos vitales, pero el hacinamiento y las malas condiciones de vida llevaron a movimientos de reforma social que abogaban por mejoras en la salud p\u00fablica.\\n   - **Tendencias de Am\u00e9rica del Norte:** El crecimiento urbano estuvo fuertemente influenciado por las olas de inmigraci\u00f3n, con millones de personas que llegaron en busca de oportunidades a finales del siglo XIX y principios del XX, lo que cambi\u00f3 r\u00e1pidamente la demograf\u00eda urbana.\\n\\n3. **Innovaciones tecnol\u00f3gicas:**\\n   - En Europa, avances como la energ\u00eda de vapor mejoraron la producci\u00f3n fabril, lo que provoc\u00f3 una mayor migraci\u00f3n a las ciudades industriales.\\n   - En Am\u00e9rica del Norte, innovaciones como el tel\u00e9grafo y diversas herramientas mecanizadas impulsaron el crecimiento industrial y atrajeron una mayor migraci\u00f3n.\\n\\n4. **Pol\u00edticas gubernamentales:**\\n   - Los gobiernos europeos comenzaron a abordar los desaf\u00edos urbanos a trav\u00e9s de la planificaci\u00f3n y las regulaciones laborales, mientras que las pol\u00edticas de laissez-faire de los Estados Unidos a menudo resultaron en una expansi\u00f3n urbana desenfrenada, lo que gener\u00f3 importantes desaf\u00edos sociales.\\n\\n### Conclusi\u00f3n\\n\\nEn resumen, la urbanizaci\u00f3n experimentada en Europa durante la Revoluci\u00f3n Industrial fue m\u00e1s pronunciada y anterior en comparaci\u00f3n con Am\u00e9rica del Norte. Las primeras pr\u00e1cticas industriales de Europa y la cultura urbana establecida impulsaron una r\u00e1pida urbanizaci\u00f3n, mientras que el crecimiento urbano de Am\u00e9rica del Norte se caracteriz\u00f3 por una inmigraci\u00f3n sustancial y un desarrollo infraestructural que cobr\u00f3 impulso a finales del siglo XIX. Aclarar estas diferencias proporciona informaci\u00f3n valiosa sobre los contextos hist\u00f3ricos que dieron forma al paisaje urbano de cada regi\u00f3n durante este per\u00edodo transformador. Comprender estos patrones de urbanizaci\u00f3n tambi\u00e9n arroja luz sobre los desaf\u00edos y cambios socioecon\u00f3micos que se produjeron cuando las sociedades pasaron de la vida rural a la urbana.'</pre> In\u00a0[15]: Copied! <pre>display(Markdown(translated))\n</pre> display(Markdown(translated)) <p>Durante la Revoluci\u00f3n Industrial, que se extendi\u00f3 desde finales del siglo XVIII hasta mediados del siglo XIX, Europa y Am\u00e9rica del Norte experimentaron una urbanizaci\u00f3n significativa a diferentes ritmos, impulsada por diversos factores contextuales. A continuaci\u00f3n se muestra un an\u00e1lisis exhaustivo basado en datos y conocimientos hist\u00f3ricos:</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#sistema-colaborativo-multi-agente-para-el-analisis-de-datos-e-historia","title":"Sistema Colaborativo (multi-agente) para el An\u00e1lisis de Datos e Historia\u00b6","text":"<p>Este notebook implementa un sistema de colaboraci\u00f3n de m\u00faltiples agentes que combina la investigaci\u00f3n hist\u00f3rica con el an\u00e1lisis de datos para responder preguntas hist\u00f3ricas complejas. Aprovecha el poder de los modelos de lenguaje grandes para simular que agentes especializados trabajan juntos para brindar respuestas integrales.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#motivacion","title":"Motivaci\u00f3n\u00b6","text":"<p>El an\u00e1lisis hist\u00f3rico a menudo requiere tanto una comprensi\u00f3n contextual profunda como una interpretaci\u00f3n de datos cuantitativos. Al crear un sistema que combina estos dos aspectos, nuestro objetivo es brindar respuestas m\u00e1s s\u00f3lidas y perspicaces a preguntas hist\u00f3ricas complejas. Este enfoque imita la colaboraci\u00f3n del mundo real entre historiadores y analistas de datos, lo que potencialmente conduce a perspectivas hist\u00f3ricas m\u00e1s matizadas y basadas en datos.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#componentes-clave","title":"Componentes clave\u00b6","text":"<ol> <li><p>Clase de agente: una clase base para crear agentes de IA especializados.</p> </li> <li><p>HistoricalResearchAgent: especializado en contexto y tendencias hist\u00f3ricas.</p> </li> <li><p>DataAnalysisAgent: centrado en la interpretaci\u00f3n de datos num\u00e9ricos y estad\u00edsticas.</p> </li> <li><p>HistoricalDataCollaborationSystem: organiza la colaboraci\u00f3n entre agentes.</p> </li> </ol>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#detalles-del-metodo","title":"Detalles del m\u00e9todo\u00b6","text":"<p>El sistema (workflow) sigue estos pasos:</p> <ol> <li><p>Contexto hist\u00f3rico: el agente de historia proporciona antecedentes hist\u00f3ricos relevantes.</p> </li> <li><p>Identificaci\u00f3n de las necesidades de datos: el agente de datos determina qu\u00e9 informaci\u00f3n cuantitativa se necesita.</p> </li> <li><p>Suministro de datos hist\u00f3ricos: el agente de datos hist\u00f3ricos proporciona datos hist\u00f3ricos relevantes.</p> </li> <li><p>An\u00e1lisis de datos: el agente de datos interpreta los datos hist\u00f3ricos proporcionados.</p> </li> <li><p>S\u00edntesis final: el agente de datos hist\u00f3ricos combina todos los conocimientos en una respuesta integral.</p> </li> </ol> <p>Este proceso iterativo permite un intercambio entre el contexto hist\u00f3rico y el an\u00e1lisis de datos, imitando la investigaci\u00f3n colaborativa del mundo real.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#inicializar-el-modelo-de-lenguaje","title":"Inicializar el modelo de lenguaje\u00b6","text":""},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#definir-la-clase-base-del-agente","title":"Definir la clase base del Agente\u00b6","text":"<p>Esta clase Agent permite crear un agente que puede procesar tareas bas\u00e1ndose en su nombre, rol y habilidades. El m\u00e9todo process construye un contexto de conversaci\u00f3n, invoca un modelo de lenguaje con ese contexto y devuelve una respuesta adecuada.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#definir-agentes-especializados-historyresearchagent-y-dataanalysisagent","title":"Definir agentes especializados: HistoryResearchAgent y DataAnalysisAgent\u00b6","text":"<p>En resumen, las clases HistoryResearchAgent y DataAnalysisAgent son implementaciones espec\u00edficas de la clase base Agent.</p> <p>HistoryResearchAgent se configura como un especialista en historia, con habilidades que incluyen un conocimiento profundo de eventos hist\u00f3ricos, comprensi\u00f3n de contextos hist\u00f3ricos e identificaci\u00f3n de tendencias hist\u00f3ricas.</p> <p>DataAnalysisAgent se establece como un experto en an\u00e1lisis de datos, con habilidades en interpretaci\u00f3n de datos num\u00e9ricos, an\u00e1lisis estad\u00edstico y descripci\u00f3n de visualizaciones de datos.</p> <p>Ambos agentes est\u00e1n dise\u00f1ados para realizar tareas espec\u00edficas en sus respectivos campos, aprovechando la funcionalidad general de la clase Agent para interactuar con modelos de lenguaje y responder a solicitudes relacionadas con su \u00e1rea de especializaci\u00f3n.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#research-historical-context","title":"Research Historical Context\u00b6","text":"<p>Esta funci\u00f3n se encarga de investigar y proporcionar contexto hist\u00f3rico relevante para una tarea espec\u00edfica. Utiliza un agente especializado en historia para procesar la solicitud y devuelve el contexto actualizado.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#identify-data-needs","title":"Identify Data Needs\u00b6","text":"<p>Esta funci\u00f3n se encarga de identificar las necesidades espec\u00edficas de datos o informaci\u00f3n estad\u00edstica que pueden ayudar a responder a una pregunta original, bas\u00e1ndose en el contexto hist\u00f3rico proporcionado anteriormente. Utiliza un agente especializado en an\u00e1lisis de datos para procesar esta solicitud y devuelve el contexto actualizado.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#provide-historical-data","title":"Provide Historical Data\u00b6","text":"<p>Esta funci\u00f3n se encarga de proporcionar datos hist\u00f3ricos relevantes en respuesta a las necesidades de datos identificadas previamente. Utiliza un agente especializado en investigaci\u00f3n hist\u00f3rica para procesar esta solicitud y devuelve el contexto actualizado.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#analyze-data","title":"Analyze Data\u00b6","text":"<p>Esta funci\u00f3n se encarga de analizar datos hist\u00f3ricos relevantes y extraer tendencias o informaci\u00f3n importante que puede ayudar a abordar la tarea original. Utiliza un agente especializado en an\u00e1lisis de datos para realizar este an\u00e1lisis y devuelve el contexto actualizado con los resultados.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#synthesize-final-answer","title":"Synthesize Final Answer\u00b6","text":"<p>Esta funci\u00f3n se encarga de integrar toda la informaci\u00f3n recopilada a lo largo del proceso (incluyendo contexto hist\u00f3rico, datos y an\u00e1lisis) y proporcionar una respuesta final completa y coherente a la tarea original. Utiliza un agente especializado en investigaci\u00f3n hist\u00f3rica para llevar a cabo esta s\u00edntesis y devolver el resultado.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#historydatacollaborationsystem-el-sistema-multiagente","title":"HistoryDataCollaborationSystem: El Sistema Multiagente\u00b6","text":"<p>La clase HistoryDataCollaborationSystem gestiona la colaboraci\u00f3n entre dos agentes especializados (uno en historia y otro en an\u00e1lisis de datos) para resolver tareas complejas que requieren informaci\u00f3n hist\u00f3rica y an\u00e1lisis de datos. A trav\u00e9s del m\u00e9todo solve, la clase coordina una serie de pasos definidos, asegurando que se cumplan los tiempos de ejecuci\u00f3n y manejando posibles errores, hasta llegar a una respuesta final que integra toda la informaci\u00f3n recopilada.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#ejemplo-de-uso","title":"Ejemplo de uso\u00b6","text":"<p>Este c\u00f3digo crea un sistema que integra agentes de historia y an\u00e1lisis de datos para abordar preguntas complejas que requieren una comprensi\u00f3n profunda de contextos hist\u00f3ricos y el an\u00e1lisis de datos relevantes. Al final, el resultado se presenta de manera que responde a la pregunta formulada.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#1-transition-from-country-to-pop","title":"1. Transition from Country to Pop\u00b6","text":"<ul> <li>Early Career: Swift began as a country artist with her self-titled debut album in 2006, which showcased autobiographical storytelling. Her sophomore album, Fearless (2008), blended country with pop and catapulted her to mainstream success, largely due to hits like \"Love Story\" and \"You Belong with Me.\"</li> <li>Shift to Pop: With albums like Red (2012) and especially 1989 (2014), Swift officially transitioned into pop music. This shift is evidenced in her lyrical themes, production choices, and the adoption of a more polished public image.</li> </ul>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#2-commercial-success-growth","title":"2. Commercial Success Growth\u00b6","text":"<ul> <li>Album Sales: Each of Swift's albums has seen significant sales milestones. Fearless won a Grammy for Album of the Year and became one of the best-selling albums of the era, while 1989 achieved massive first-week sales and topped charts globally, cementing her status as a pop icon.</li> <li>Streaming Metrics: The rise of digital streaming platforms like Spotify and Apple Music has greatly influenced her reach. Her albums Folklore and Evermore, released during the pandemic in 2020, saw high streaming counts, showcasing her ability to innovate and remain relevant.</li> </ul>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#3-lyrical-themes-and-artistic-growth","title":"3. Lyrical Themes and Artistic Growth\u00b6","text":"<ul> <li>Early to Later Works: Initially focusing on youthful experiences and relationships, Swift's later works show a profound maturity and introspection, as seen in the narrative-driven lyrics of Folklore and Evermore.</li> <li>Theme Evolution: Elements of self-empowerment, societal critique, and complex emotional landscapes emerge prominently, indicating her growth as a songwriter.</li> </ul>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#4-awards-and-recognition","title":"4. Awards and Recognition\u00b6","text":"<ul> <li>Accolades: Swift has received numerous awards throughout her career, including multiple Grammy Awards for pivotal albums such as Fearless, 1989, and Folklore. These accolades reflect her ability to resonate with both audiences and critics, highlighting her adaptability and artistic merit.</li> </ul>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#5-social-media-and-engagement","title":"5. Social Media and Engagement\u00b6","text":"<ul> <li>Digital Presence: Swift has effectively utilized social media to foster a strong connection with her fan base. Engagement metrics show spikes during album releases and major announcements, underscoring the importance of these platforms in modern music marketing.</li> </ul>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#6-touring-success","title":"6. Touring Success\u00b6","text":"<ul> <li>Concert Tours: Tours like the Reputation Stadium Tour demonstrate her drawing power and ability to generate significant revenue. Attendance figures often reflect sold-out shows, indicating her sustained popularity and commercial success as a live performer.</li> </ul>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#7-audience-demographics","title":"7. Audience Demographics\u00b6","text":"<ul> <li>Shifts in Fan Base: Analysis of her audience demographics over time reveals a broadening appeal across age ranges and geographical locations, reflecting her crossover success from country to mainstream pop. This is crucial for understanding her market reach as her music style evolved.</li> </ul>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#conclusion","title":"Conclusion\u00b6","text":"<p>Taylor Swift's career is a textbook example of how an artist can navigate and thrive within a rapidly changing music industry. Her transition from a country star to a pop icon, coupled with her ability to tap into current trends (like the digital streaming boom), highlights her dynamic approach to music and marketing. By consistently engaging with her audience and evolving artistically, Swift has maintained her influence and relevance, making her one of the most significant figures in modern music. This comprehensive understanding of her evolution not only underscores her personal growth but also reflects broader industry trends and the changing landscape of popular music.</p>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#discusion","title":"Discusion\u00b6","text":""},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#extra-como-puedo-hacer-para-que-el-sistema-conteste-en-espanol","title":"Extra: \u00bfC\u00f3mo puedo hacer para que el sistema conteste en Espa\u00f1ol?\u00b6","text":""},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#extra-y-podria-agregarle-un-agente-de-reflexioncritica-sobre-lo-generado","title":"Extra: \u00bfY podr\u00eda agregarle un agente de reflexion/cr\u00edtica sobre lo generado?\u00b6","text":""},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#tarifas-de-urbanizacion","title":"Tarifas de urbanizaci\u00f3n\u00b6","text":"<ol> <li><p>Europa:</p> <ul> <li>Momento de la Urbanizaci\u00f3n: La Revoluci\u00f3n Industrial se inici\u00f3 en Gran Breta\u00f1a a finales del siglo XVIII. En 1850, los principales centros urbanos como Manchester ten\u00edan poblaciones urbanas superiores al 70%, mientras que ciudades como Londres ten\u00edan alrededor del 50% de su poblaci\u00f3n viviendo en \u00e1reas urbanas.</li> <li>Difusi\u00f3n por el continente: Otras naciones europeas como Francia y Alemania experimentaron una urbanizaci\u00f3n inicial m\u00e1s lenta; sin embargo, se aceler\u00f3 a medida que la infraestructura mejor\u00f3 y las industrias se expandieron a lo largo del siglo XIX.</li> </ul> </li> <li><p>Am\u00e9rica del Norte:</p> <ul> <li>Urbanizaci\u00f3n gradual: La urbanizaci\u00f3n en los Estados Unidos se aceler\u00f3 principalmente despu\u00e9s de 1860, cuando la poblaci\u00f3n urbana rondaba el 19%. Para 1900, esto aument\u00f3 a aproximadamente el 40%, destacando un crecimiento urbano significativo, especialmente en ciudades como Nueva York y Chicago.</li> <li>Llegados tard\u00edos: En comparaci\u00f3n con Europa, el crecimiento urbano en Am\u00e9rica del Norte se produjo m\u00e1s tarde en el siglo XIX, lo que refleja los efectos combinados de la industrializaci\u00f3n, la inmigraci\u00f3n y la expansi\u00f3n ferroviaria.</li> </ul> </li> </ol>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#factores-clave-que-influyen-en-las-tendencias-de-urbanizacion","title":"Factores clave que influyen en las tendencias de urbanizaci\u00f3n\u00b6","text":"<ol> <li><p>Factores econ\u00f3micos:</p> <ul> <li>Europa: El auge de las f\u00e1bricas y la producci\u00f3n mecanizada atrajo una gran cantidad de mano de obra de las zonas rurales. Esta transici\u00f3n tambi\u00e9n estuvo influenciada por cambios agr\u00edcolas que desplazaron a muchos trabajadores rurales.</li> <li>Am\u00e9rica del Norte: Factores como el ferrocarril transcontinental y la disponibilidad de tierras facilitaron el movimiento hacia \u00e1reas urbanas y contribuyeron a la creaci\u00f3n de empleo.</li> </ul> </li> <li><p>Factores sociales:</p> <ul> <li>Contexto europeo: Con una historia m\u00e1s larga de cultura urbana, las ciudades sirvieron como centros culturales y pol\u00edticos vitales, pero el hacinamiento y las malas condiciones de vida llevaron a movimientos de reforma social que abogaban por mejoras en la salud p\u00fablica.</li> <li>Tendencias de Am\u00e9rica del Norte: El crecimiento urbano estuvo fuertemente influenciado por las olas de inmigraci\u00f3n, con millones de personas que llegaron en busca de oportunidades a finales del siglo XIX y principios del XX, lo que cambi\u00f3 r\u00e1pidamente la demograf\u00eda urbana.</li> </ul> </li> <li><p>Innovaciones tecnol\u00f3gicas:</p> <ul> <li>En Europa, avances como la energ\u00eda de vapor mejoraron la producci\u00f3n fabril, lo que provoc\u00f3 una mayor migraci\u00f3n a las ciudades industriales.</li> <li>En Am\u00e9rica del Norte, innovaciones como el tel\u00e9grafo y diversas herramientas mecanizadas impulsaron el crecimiento industrial y atrajeron una mayor migraci\u00f3n.</li> </ul> </li> <li><p>Pol\u00edticas gubernamentales:</p> <ul> <li>Los gobiernos europeos comenzaron a abordar los desaf\u00edos urbanos a trav\u00e9s de la planificaci\u00f3n y las regulaciones laborales, mientras que las pol\u00edticas de laissez-faire de los Estados Unidos a menudo resultaron en una expansi\u00f3n urbana desenfrenada, lo que gener\u00f3 importantes desaf\u00edos sociales.</li> </ul> </li> </ol>"},{"location":"dia2/notebooks/langchain_multi_agent_collaboration/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<p>En resumen, la urbanizaci\u00f3n experimentada en Europa durante la Revoluci\u00f3n Industrial fue m\u00e1s pronunciada y anterior en comparaci\u00f3n con Am\u00e9rica del Norte. Las primeras pr\u00e1cticas industriales de Europa y la cultura urbana establecida impulsaron una r\u00e1pida urbanizaci\u00f3n, mientras que el crecimiento urbano de Am\u00e9rica del Norte se caracteriz\u00f3 por una inmigraci\u00f3n sustancial y un desarrollo infraestructural que cobr\u00f3 impulso a finales del siglo XIX. Aclarar estas diferencias proporciona informaci\u00f3n valiosa sobre los contextos hist\u00f3ricos que dieron forma al paisaje urbano de cada regi\u00f3n durante este per\u00edodo transformador. Comprender estos patrones de urbanizaci\u00f3n tambi\u00e9n arroja luz sobre los desaf\u00edos y cambios socioecon\u00f3micos que se produjeron cuando las sociedades pasaron de la vida rural a la urbana.</p>"},{"location":"dia2/notebooks/react-web-search/","title":"LangGraph doing a web search plus sentiment analysis using ReAct","text":"In\u00a0[\u00a0]: Copied! <pre>#\u00a0%%capture --no-stderr\n#\u00a0%pip install langchain langchain-openai langchain_experimental python-dotenv matplotlib numpy pandas langgraph langchain-community langchain_tavily\n</pre> #\u00a0%%capture --no-stderr #\u00a0%pip install langchain langchain-openai langchain_experimental python-dotenv matplotlib numpy pandas langgraph langchain-community langchain_tavily In\u00a0[4]: Copied! <pre>import json\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef pp(data):\n    print(json.dumps(data, indent=2, default=str))\n</pre> import json from dotenv import load_dotenv  load_dotenv()  def pp(data):     print(json.dumps(data, indent=2, default=str))  In\u00a0[5]: Copied! <pre>from langchain_tavily import TavilySearch\n\ntavily_search = TavilySearch(\n    max_results=5,\n    topic=\"general\", # Can be \"general\", \"news\", or \"finance\"\n    # include_answer=False,\n    # include_raw_content=False,\n    include_images=False,\n    include_image_descriptions=False,\n    search_depth=\"basic\",\n    time_range=\"day\",\n    # include_domains=None,\n    # exclude_domains=None\n)\n</pre> from langchain_tavily import TavilySearch  tavily_search = TavilySearch(     max_results=5,     topic=\"general\", # Can be \"general\", \"news\", or \"finance\"     # include_answer=False,     # include_raw_content=False,     include_images=False,     include_image_descriptions=False,     search_depth=\"basic\",     time_range=\"day\",     # include_domains=None,     # exclude_domains=None ) In\u00a0[9]: Copied! <pre>from typing_extensions import TypedDict\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import create_react_agent\n\nllm = init_chat_model(\n    \"openai:gpt-5-nano\"\n)\n\n# Decoramos nuestro llm con ReAct - Reason + Act\nreact_llm = create_react_agent(\n    model=llm,\n    tools=[tavily_search],\n)\n\nclass TypedDictState(TypedDict):\n    user_query: str\n    refined_query: str\n    trending_topics: str\n    sentiment_analysis: str\n\ndef search_trending_topics(state: TypedDictState) -&gt; TypedDictState:\n    refined_query = state['refined_query']\n\n    prompt = f\"\"\"\n    Given the refined user query, your job is to search for trending topics related to it.\n\n    Refined query: {refined_query}\n\n    Trending topics:\"\"\"\n\n    # ReAct usar `messages` para devolver el resultado de las tools que invoca\n    response = react_llm.invoke({ \"messages\": [SystemMessage(content=prompt)] })\n\n    return response\n\nresponse = search_trending_topics({\n    \"user_query\": \"What are the latest trends in technology?\",\n    \"refined_query\": \"Latest trends in technology\",\n    \"trending_topics\": \"\",\n    \"sentiment_analysis\": \"\"\n})\n</pre> from typing_extensions import TypedDict from langchain.chat_models import init_chat_model from langchain_core.messages import SystemMessage from langgraph.prebuilt import create_react_agent  llm = init_chat_model(     \"openai:gpt-5-nano\" )  # Decoramos nuestro llm con ReAct - Reason + Act react_llm = create_react_agent(     model=llm,     tools=[tavily_search], )  class TypedDictState(TypedDict):     user_query: str     refined_query: str     trending_topics: str     sentiment_analysis: str  def search_trending_topics(state: TypedDictState) -&gt; TypedDictState:     refined_query = state['refined_query']      prompt = f\"\"\"     Given the refined user query, your job is to search for trending topics related to it.      Refined query: {refined_query}      Trending topics:\"\"\"      # ReAct usar `messages` para devolver el resultado de las tools que invoca     response = react_llm.invoke({ \"messages\": [SystemMessage(content=prompt)] })      return response  response = search_trending_topics({     \"user_query\": \"What are the latest trends in technology?\",     \"refined_query\": \"Latest trends in technology\",     \"trending_topics\": \"\",     \"sentiment_analysis\": \"\" }) In\u00a0[10]: Copied! <pre>response\n</pre> response Out[10]: <pre>{'messages': [SystemMessage(content='\\n    Given the refined user query, your job is to search for trending topics related to it.\\n\\n    Refined query: Latest trends in technology\\n\\n    Trending topics:', additional_kwargs={}, response_metadata={}, id='e430fda8-411c-4b93-9cda-e49eb86d50d7'),\n  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_gmcfxz5mjw0WKlsrlH5QX9EK', 'function': {'arguments': '{\"query\":\"Latest trends in technology\",\"time_range\":\"month\",\"search_depth\":\"advanced\"}', 'name': 'tavily_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 422, 'prompt_tokens': 1381, 'total_tokens': 1803, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CUMePhq1B0eQ44BuobutGMroRr8vC', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1ddd3620-7a98-41e5-85ab-e04365e4437d-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Latest trends in technology', 'time_range': 'month', 'search_depth': 'advanced'}, 'id': 'call_gmcfxz5mjw0WKlsrlH5QX9EK', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1381, 'output_tokens': 422, 'total_tokens': 1803, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}}),\n  ToolMessage(content='{\"query\": \"Latest trends in technology\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.fastcompany.com/technology\", \"title\": \"Tech News: Latest Trends &amp; Innovations\", \"content\": \"Latest Tech News \u00b7 Can OpenAI\\'s Atlas get people to care about browsers again? \u00b7 Tesla recalls over 63,000 Cybertrucks due to the front lights being too bright.\", \"score\": 0.69746524, \"raw_content\": null}, {\"url\": \"https://fortune.com/section/tech/\", \"title\": \"Tech News, Innovation Trends &amp; Industry Insights\", \"content\": \"Get the latest tech news, innovation trends &amp; industry insights from Fortune, your trusted source for AI, technology &amp; digital trends.\", \"score\": 0.58976424, \"raw_content\": null}, {\"url\": \"https://www.usnews.com/topics/subjects/technology\", \"title\": \"The Latest News on Technology\", \"content\": \"Read the latest articles and commentary on technology at US News.\", \"score\": 0.48484412, \"raw_content\": null}, {\"url\": \"https://www.theguardian.com/uk/technology\", \"title\": \"Technology\", \"content\": \"Latest Technology news, comment and analysis from the Guardian, the world\\'s leading liberal voice.\", \"score\": 0.44883752, \"raw_content\": null}, {\"url\": \"https://www.ft.com/technology\", \"title\": \"Technology\", \"content\": \"Get the latest technology news from around the industry. Keep up-to-date with the most influential and innovative technology brands, including Apple,\", \"score\": 0.43597415, \"raw_content\": null}], \"response_time\": 0.86, \"request_id\": \"ee4db96f-1df6-4441-9429-44d29c805a71\"}', name='tavily_search', id='4354c517-5565-4e41-a6af-6eb3cd4a8013', tool_call_id='call_gmcfxz5mjw0WKlsrlH5QX9EK'),\n  AIMessage(content='Trending topics (latest month, based on major tech outlets)\\n\\n- Generative AI in business and productivity tools\\n  - Rise of enterprise copilots, AI-assisted workflows, and AI-powered decision support across industries.\\n  - Sources: Fortune Tech, Fast Company coverage of AI and tools.\\n\\n- AI governance, safety, and regulation\\n  - Growing policy discussions and frameworks around AI safety, ethics, and accountability in the US and EU.\\n  - Sources: FT Technology, Fortune coverage of policy trends.\\n\\n- AI hardware and semiconductor advances\\n  - Advances in AI accelerators, chips, and the broader chip supply chain to support large-scale AI workloads.\\n  - Sources: FT Technology, Fortune tech coverage.\\n\\n- Electric vehicles and autonomous driving tech\\n  - Ongoing updates in EV demand, battery tech, and autonomous driving capabilities; notable recalls/updates in the sector.\\n  - Sources: Fast Company tech news (e.g., EV/AV topics and recalls).\\n\\n- Cybersecurity and data privacy in the AI era\\n  - Increased focus on securing AI systems, data privacy, and resilience against AI-enabled threats.\\n  - Sources: General tech coverage across outlets (Fortune, Guardian, FT).\\n\\n- Edge computing and on-device AI\\n  - Push for on-device inference to reduce latency and improve privacy, enabling smarter devices and edge networks.\\n  - Sources: Broad tech press coverage (Fortune, FT).\\n\\n- Climate tech and sustainability innovations\\n  - Tech innovations for energy efficiency, storage, grid optimization, and decarbonization tech.\\n  - Sources: FT Technology and broader climate-tech coverage.\\n\\n- AR/VR and mixed reality\\n  - New consumer devices, enterprise AI-assisted workflows, and content/interaction advances in AR/VR.\\n  - Sources: Guardian Technology, FT Technology.\\n\\n- Health tech and biotech digital tools\\n  - AI-enabled drug discovery, digital health platforms, and biotech tooling gaining momentum.\\n  - Sources: US News Technology, Fortune/FT tech coverage.\\n\\n- 5G/6G networks and telecom infrastructure\\n  - Network upgrades and innovations enabling more capable AI, IoT, and edge services.\\n  - Sources: FT Technology, Fortune tech coverage.\\n\\nWould you like me to:\\n- Drill into any of these topics with a deeper summary and recent article highlights, or\\n- Tailor the trends to a specific industry (e.g., finance, healthcare, manufacturing) or region?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2736, 'prompt_tokens': 1825, 'total_tokens': 4561, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 2240, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CUMeWffOxwJgCPQDGWW7leer5ClcM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--49b30dd4-68d7-448f-855d-845cfe12c35c-0', usage_metadata={'input_tokens': 1825, 'output_tokens': 2736, 'total_tokens': 4561, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 2240}})]}</pre> In\u00a0[11]: Copied! <pre>response[\"messages\"][-1].content.strip()\n</pre> response[\"messages\"][-1].content.strip() Out[11]: <pre>'Trending topics (latest month, based on major tech outlets)\\n\\n- Generative AI in business and productivity tools\\n  - Rise of enterprise copilots, AI-assisted workflows, and AI-powered decision support across industries.\\n  - Sources: Fortune Tech, Fast Company coverage of AI and tools.\\n\\n- AI governance, safety, and regulation\\n  - Growing policy discussions and frameworks around AI safety, ethics, and accountability in the US and EU.\\n  - Sources: FT Technology, Fortune coverage of policy trends.\\n\\n- AI hardware and semiconductor advances\\n  - Advances in AI accelerators, chips, and the broader chip supply chain to support large-scale AI workloads.\\n  - Sources: FT Technology, Fortune tech coverage.\\n\\n- Electric vehicles and autonomous driving tech\\n  - Ongoing updates in EV demand, battery tech, and autonomous driving capabilities; notable recalls/updates in the sector.\\n  - Sources: Fast Company tech news (e.g., EV/AV topics and recalls).\\n\\n- Cybersecurity and data privacy in the AI era\\n  - Increased focus on securing AI systems, data privacy, and resilience against AI-enabled threats.\\n  - Sources: General tech coverage across outlets (Fortune, Guardian, FT).\\n\\n- Edge computing and on-device AI\\n  - Push for on-device inference to reduce latency and improve privacy, enabling smarter devices and edge networks.\\n  - Sources: Broad tech press coverage (Fortune, FT).\\n\\n- Climate tech and sustainability innovations\\n  - Tech innovations for energy efficiency, storage, grid optimization, and decarbonization tech.\\n  - Sources: FT Technology and broader climate-tech coverage.\\n\\n- AR/VR and mixed reality\\n  - New consumer devices, enterprise AI-assisted workflows, and content/interaction advances in AR/VR.\\n  - Sources: Guardian Technology, FT Technology.\\n\\n- Health tech and biotech digital tools\\n  - AI-enabled drug discovery, digital health platforms, and biotech tooling gaining momentum.\\n  - Sources: US News Technology, Fortune/FT tech coverage.\\n\\n- 5G/6G networks and telecom infrastructure\\n  - Network upgrades and innovations enabling more capable AI, IoT, and edge services.\\n  - Sources: FT Technology, Fortune tech coverage.\\n\\nWould you like me to:\\n- Drill into any of these topics with a deeper summary and recent article highlights, or\\n- Tailor the trends to a specific industry (e.g., finance, healthcare, manufacturing) or region?'</pre> In\u00a0[13]: Copied! <pre>from langgraph.graph import START, END, StateGraph\nfrom IPython.display import Image, display\nfrom typing_extensions import TypedDict\nfrom langchain.chat_models import init_chat_model\n\nfrom langgraph.prebuilt import create_react_agent\n\nllm = init_chat_model(\n    \"openai:gpt-5-nano\"\n)\n\nreact_llm = create_react_agent(\n    model=llm,\n    tools=[tavily_search],\n)\n\nclass TypedDictState(TypedDict):\n    user_query: str\n    refined_query: str\n    trending_topics: str\n    sentiment_analysis: str\n\ndef refine_user_query(state: TypedDictState) -&gt; TypedDictState:\n    user_query = state['user_query']\n\n    prompt = f\"\"\"\n    Given the user query, your job is to rewrite it to make it more specific and clear.\n\n    User query: {user_query}\n\n    Revised query:\"\"\"\n\n    response = llm.invoke(prompt)\n\n    return { \"refined_query\": response.content.strip() }\n\ndef search_trending_topics(state: TypedDictState) -&gt; TypedDictState:\n    refined_query = state['refined_query']\n\n    prompt = f\"\"\"\n    Given the refined user query, your job is to search for trending topics related to it.\n\n    Refined query: {refined_query}\n\n    Trending topics:\"\"\"\n\n    response = react_llm.invoke({ \"messages\": [SystemMessage(content=prompt)] })\n\n    return { \"trending_topics\": response[\"messages\"][-1].content.strip() }\n\ndef analyze_sentiment(state: TypedDictState) -&gt; TypedDictState:\n\n    trending_topics = state['trending_topics']\n\n    prompt = f\"\"\"\n    Given the trending topics, your job is to analyze their sentiment.\n\n    Trending topics: {trending_topics}\n\n    Sentiment analysis:\"\"\"\n\n    response = llm.invoke(prompt)\n\n    return { \"sentiment_analysis\": response.content.strip() }\n\nbuilder = StateGraph(TypedDictState)\n\nbuilder.add_node(\"refine_user_query\", refine_user_query)\nbuilder.add_node(\"search_trending_topics\", search_trending_topics)\nbuilder.add_node(\"analyze_sentiment\", analyze_sentiment)\n\nbuilder.add_edge(START, \"refine_user_query\")\nbuilder.add_edge(\"refine_user_query\", \"search_trending_topics\")\nbuilder.add_edge(\"search_trending_topics\", \"analyze_sentiment\")\nbuilder.add_edge(\"analyze_sentiment\", END)\n\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n</pre> from langgraph.graph import START, END, StateGraph from IPython.display import Image, display from typing_extensions import TypedDict from langchain.chat_models import init_chat_model  from langgraph.prebuilt import create_react_agent  llm = init_chat_model(     \"openai:gpt-5-nano\" )  react_llm = create_react_agent(     model=llm,     tools=[tavily_search], )  class TypedDictState(TypedDict):     user_query: str     refined_query: str     trending_topics: str     sentiment_analysis: str  def refine_user_query(state: TypedDictState) -&gt; TypedDictState:     user_query = state['user_query']      prompt = f\"\"\"     Given the user query, your job is to rewrite it to make it more specific and clear.      User query: {user_query}      Revised query:\"\"\"      response = llm.invoke(prompt)      return { \"refined_query\": response.content.strip() }  def search_trending_topics(state: TypedDictState) -&gt; TypedDictState:     refined_query = state['refined_query']      prompt = f\"\"\"     Given the refined user query, your job is to search for trending topics related to it.      Refined query: {refined_query}      Trending topics:\"\"\"      response = react_llm.invoke({ \"messages\": [SystemMessage(content=prompt)] })      return { \"trending_topics\": response[\"messages\"][-1].content.strip() }  def analyze_sentiment(state: TypedDictState) -&gt; TypedDictState:      trending_topics = state['trending_topics']      prompt = f\"\"\"     Given the trending topics, your job is to analyze their sentiment.      Trending topics: {trending_topics}      Sentiment analysis:\"\"\"      response = llm.invoke(prompt)      return { \"sentiment_analysis\": response.content.strip() }  builder = StateGraph(TypedDictState)  builder.add_node(\"refine_user_query\", refine_user_query) builder.add_node(\"search_trending_topics\", search_trending_topics) builder.add_node(\"analyze_sentiment\", analyze_sentiment)  builder.add_edge(START, \"refine_user_query\") builder.add_edge(\"refine_user_query\", \"search_trending_topics\") builder.add_edge(\"search_trending_topics\", \"analyze_sentiment\") builder.add_edge(\"analyze_sentiment\", END)  graph = builder.compile()  display(Image(graph.get_graph(xray=True).draw_mermaid_png())) In\u00a0[14]: Copied! <pre>initial_state : TypedDictState = {\n    \"user_query\": \"Is people buying ETH on the last month?\",\n    \"refined_query\": \"\",\n    \"trending_topics\": \"\",\n    \"sentiment_analysis\": \"\"\n}\n\nstate = graph.invoke(input=initial_state)\n</pre> initial_state : TypedDictState = {     \"user_query\": \"Is people buying ETH on the last month?\",     \"refined_query\": \"\",     \"trending_topics\": \"\",     \"sentiment_analysis\": \"\" }  state = graph.invoke(input=initial_state) In\u00a0[15]: Copied! <pre>state\n</pre> state Out[15]: <pre>{'user_query': 'Is people buying ETH on the last month?',\n 'refined_query': 'Revised query: What was the total ETH buying volume in the last 30 days?',\n 'trending_topics': 'Here are current trending topics and angles people are discussing around the query \u201cWhat was the total ETH buying volume in the last 30 days?\u201d\\n\\n- 30-day ETH trading volume context\\n  - Market data often cites ETH daily trading volume in the tens of billions of dollars. For example, reports have mentioned daily volumes around $41B during the recent month, which is the closest public framing you\u2019ll typically see for \u201cbuying volume\u201d over a 30-day window (note: this is exchange-traded volume, not a strict \u201cbuying\u201d metric).\\n\\n- Price action vs. volume\\n  - ETH has seen a noticeable move over the past 30 days (roughly a mid-single-digit to high-single-digit percent change in various briefs). Trends in volume often accompany these moves, with spikes around rallies or pullbacks.\\n\\n- On-chain buy-pressure signals\\n  - Analysts and data providers discuss \u201cbuying pressure\u201d and accumulation indicators (on-chain wallet activity, exchange inflows/outflows, large-address activity) as proxies for total buying activity over a month. These are common complements to raw volume when interpreting \u201cbuying volume.\u201d\\n\\n- Exchange vs. on-chain perspectives\\n  - Some sources differentiate between:\\n    - Exchange/trade volume (what\u2019s visible on order books and aggregators)\\n    - On-chain buy signals (net inflows to exchanges, wallet accumulation)\\n  These together help approximate total buying activity over 30 days.\\n\\n- Data sources you can use for a precise figure\\n  - Exchange/trade volume: Investing.com ETH page, Binance/other exchange dashboards\\n  - On-chain buy pressure and accumulation: IntoTheBlock, Glassnode, CryptoQuant\\n  - News and analysis pieces sometimes summarize the 30-day picture with a figure like \u201cdaily volume near $41B\u201d as a basis for an approximate total\\n\\nIf you\u2019d like, I can pull more precise figures for the last 30 days from specific data sources (e.g., a particular exchange\u2019s aggregate buy-volume, or on-chain buy-sell balance from IntoTheBlock/Glassnode) and compute an exact total in USD. Also, please tell me:\\n- Do you want USD volume, ETH-denominated volume, or both?\\n- Do you want on-chain buy pressure (wallet accumulation) included, or strictly exchange/trade volume?',\n 'sentiment_analysis': 'Sentiment analysis:\\n\\n- Overall sentiment: Neutral to cautiously optimistic.\\n  - The topics present a measured, data-driven view of ETH buying/volume, with a focus on multiple data sources and definitions.\\n\\n- Tone and approach:\\n  - Technical, analytical, and balanced. Emphasizes both exchange-traded volume and on-chain indicators, highlighting the nuances and potential biases of each.\\n\\n- Key sentiment cues by subtopics:\\n  - 30-day volume context: Neutral; presents a widely cited figure (around $41B daily) as a contextual anchor rather than a definitive metric.\\n  - Price action vs. volume: Neutral but acknowledges meaningful activity; ties sentiment to observable moves and volume spikes.\\n  - On-chain buy-pressure signals: Neutral-to-moderate optimism about using on-chain metrics as proxies for buying activity.\\n  - Exchange vs. on-chain perspectives: Informative; acknowledges the need to triangulate between visible trade volume and on-chain activity.\\n  - Data sources and precision: Positive and proactive; invites data-driven follow-through and corroboration from multiple sources.\\n\\n- Implications and potential biases:\\n  - The content recognizes that \u201cbuying volume\u201d is not a single, universal metric and benefits from a multi-metric approach. There is a healthy skepticism about relying on a single figure.\\n\\n- Recommendations implied by the sentiment:\\n  - If interpreting these trends, combine exchange/trade volume with on-chain buy pressure signals for a fuller picture.\\n  - Consider clarifying what type of volume is being measured (USD vs ETH-denominated) and whether on-chain signals are included.\\n\\n- Brief takeaway:\\n  - The sentiment is forward-looking but cautious, signaling that precise, actionable insights require pulling exact numbers from credible sources and clearly defining the measurement approach.\\n\\nIf you\u2019d like, I can fetch exact last-30-day figures from specific sources (e.g., an exchange aggregate, IntoTheBlock, Glassnode) and present a clear breakdown in USD and ETH terms, with or without on-chain signals.'}</pre> In\u00a0[14]: Copied! <pre>from IPython.display import Markdown, display\n\ndisplay(Markdown(f\"**Sentiment Analysis:**\\n\\n{state['sentiment_analysis']}\"))\n</pre> from IPython.display import Markdown, display  display(Markdown(f\"**Sentiment Analysis:**\\n\\n{state['sentiment_analysis']}\"))  <p>Sentiment Analysis:</p> <p>Here\u2019s a sentiment snapshot of the trending ETH-buyers and purchase-volume topics:</p> <p>Overall vibe:</p> <ul> <li>The narrative is broadly cautiously optimistic: demand signals (on-chain buyers, purchase volume, and network activity) are favorable, but progress is tempered by data-definition debates and mixed institution flows.</li> </ul> <p>Topic-by-topic sentiment:</p> <ul> <li>ETH on-chain buyer activity trends (unique buyers in last 30 days): Positive to cautiously optimistic. Rising buying activity suggests growing demand, but sustainability and the exact definition of \u201cbuyers\u201d are key caveats.</li> <li>Total ETH purchase volume over the last 30 days (on-chain buy-volume): Positive/optimistic when framed by volume growth, though context matters. Traders want clear buy-intensity signals, and data quality or methodology nuances can mute enthusiasm.</li> <li>Institutional demand signals for ETH (ETF/institutional flows, custody, large-trader activity): Mixed. Pro-inflow signals (e.g., ETF inflows) would be positive, while outflows or ambiguous institutional activity keep sentiment uncertain.</li> <li>ETH network activity vs price dynamics (daily transactions, gas usage, and price correlation): Positive. Higher network activity aligned with price moves is viewed as a supportive sign for demand and potential price momentum.</li> <li>Methodology and definitions debate (how \u201cbuyers\u201d and \u201cpurchase volume\u201d are defined on-chain): Neutral/uncertain. This topic dampens conviction because differing definitions can lead to apples-vs-oranges comparisons.</li> </ul> <p>What to watch next (implications):</p> <ul> <li>If unique buyers and purchase-volume both rise with consistent definitions, sentiment tilts more bullish.</li> <li>If institutional flows pivot (e.g., sustained inflows) or outflows dominate, sentiment could flip toward caution or bearish risk-off sentiment.</li> <li>Ongoing debates on methodology will keep short-term interpretation ranges wide; standardized metrics will improve clarity and confidence.</li> </ul> <p>If you want, I can pull current figures from specific sources (e.g., Glassnode, Santiment, Nansen) and provide:</p> <ul> <li>number of unique ETH buying addresses in the last 30 days</li> <li>total ETH purchased (in ETH and USD if available)</li> <li>a quick comparison to the prior 30 days I can also tailor the sentiment to your preferred definitions (e.g., only buyers that initiate purchases vs. all addresses receiving ETH via purchases) and by data source.</li> </ul>"},{"location":"dia2/notebooks/react-web-search/#langgraph-doing-a-web-search-plus-sentiment-analysis-using-react","title":"LangGraph doing a web search plus sentiment analysis using ReAct\u00b6","text":"<ul> <li>Sign up to: https://app.tavily.com/home</li> <li>Tavily search tool: https://python.langchain.com/docs/integrations/tools/tavily_search</li> </ul>"},{"location":"dia2/notebooks/web-search/","title":"LangGraph doing a web search plus sentiment analysis","text":"In\u00a0[\u00a0]: Copied! <pre># %%capture --no-stderr\n# %pip install langchain langchain-openai langchain_experimental python-dotenv matplotlib numpy pandas langgraph langchain-community langchain_tavily\n</pre> # %%capture --no-stderr # %pip install langchain langchain-openai langchain_experimental python-dotenv matplotlib numpy pandas langgraph langchain-community langchain_tavily In\u00a0[2]: Copied! <pre>import json\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef pp(data):\n    print(json.dumps(data, indent=2, default=str))\n</pre> import json from dotenv import load_dotenv  load_dotenv()  def pp(data):     print(json.dumps(data, indent=2, default=str))  In\u00a0[3]: Copied! <pre>from langchain_tavily import TavilySearch\n\ntavily_search = TavilySearch(\n    max_results=5,\n    topic=\"general\", # Can be \"general\", \"news\", or \"finance\"\n    # include_answer=False,\n    # include_raw_content=False,\n    include_images=False,\n    include_image_descriptions=False,\n    search_depth=\"basic\",\n    time_range=\"day\",\n    # include_domains=None,\n    # exclude_domains=None\n)\n\nresponse = tavily_search.invoke(\"Is people buying Bitcoin on the last month?\")\n</pre> from langchain_tavily import TavilySearch  tavily_search = TavilySearch(     max_results=5,     topic=\"general\", # Can be \"general\", \"news\", or \"finance\"     # include_answer=False,     # include_raw_content=False,     include_images=False,     include_image_descriptions=False,     search_depth=\"basic\",     time_range=\"day\",     # include_domains=None,     # exclude_domains=None )  response = tavily_search.invoke(\"Is people buying Bitcoin on the last month?\")  In\u00a0[4]: Copied! <pre>response['results']\n</pre> response['results'] Out[4]: <pre>[{'url': 'https://www.mitrade.com/au/insights/news/live-news/article-3-1226260-20251028',\n  'title': 'Trump-Backed American Bitcoin Jumps After $163M $BTC Buy',\n  'content': 'Bitcoin is up by around 4.7% in the past week, and sits just under $115K, near a two-week high. ABTC forms part of the growing push for crypto treasuries, and',\n  'score': 0.3936886,\n  'raw_content': None},\n {'url': 'https://www.betashares.com.au/insights/bitcoin-retraces-from-october-losses/',\n  'title': 'Bitcoin retraces from October losses | Betashares',\n  'content': 'According to data from Glassnode, as of 25 October 2025 the percentage supply of Bitcoin held on exchanges has consistently declined. This is seen as positive',\n  'score': 0.27167162,\n  'raw_content': None},\n {'url': 'https://finance.yahoo.com/news/bitcoin-uptober-cliffhanger-btc-close-194911059.html',\n  'title': 'Bitcoin Uptober Cliffhanger: Will BTC Close the Month Green?',\n  'content': 'With just days left in Uptober, Bitcoin is in the green for the month\u2014but just barely. Can it hold on and keep the seasonal trend alive?',\n  'score': 0.2428968,\n  'raw_content': None},\n {'url': 'https://m.economictimes.com/news/international/us/end-of-the-bull-run-bitcoin-could-tank-to-70000-says-top-market-analyst/articleshow/124874365.cms',\n  'title': 'End of the bull run? Bitcoin could tank to $70000, says top market ...',\n  'content': 'Bitcoin may face a big fall after its strong bull run. Market expert Jon Glover says the price could drop to around $70000 in the coming months.',\n  'score': 0.23506488,\n  'raw_content': None},\n {'url': 'https://www.coinbase.com/price/bitcoin',\n  'title': 'Bitcoin Price, BTC Price, Live Charts, and Marketcap - Coinbase',\n  'content': \"Price of BTC is $114,216.14 right now. Looking at Bitcoin's historical prices, it's 3% up from the previous week's price of $111,128.03, and observed a 1%\",\n  'score': 0.18929654,\n  'raw_content': None}]</pre> In\u00a0[5]: Copied! <pre>from langgraph.graph import START, END, StateGraph\nfrom IPython.display import Image, display\nfrom typing_extensions import TypedDict\nfrom langchain.chat_models import init_chat_model\n\nllm = init_chat_model(\"openai:gpt-5-nano\")\n\nclass TypedDictState(TypedDict):\n    user_query: str\n    refined_query: str\n    trending_topics: str\n    sentiment_analysis: str\n\ndef refine_user_query(state: TypedDictState) -&gt; TypedDictState:\n    user_query = state['user_query']\n\n    prompt = f\"\"\"\n    Given the user query, your job is to rewrite it to make it more specific and clear.\n\n    User query: {user_query}\n\n    Revised query:\"\"\"\n\n    response = llm.invoke(prompt)\n\n    return { \"refined_query\": response.content.strip() }\n\ndef search_trending_topics(state: TypedDictState) -&gt; TypedDictState:\n    refined_query = state['refined_query']\n\n    prompt = f\"\"\"\n    Given the refined user query, your job is to search for trending topics related to it.\n\n    Refined query: {refined_query}\n\n    Trending topics:\"\"\"\n\n    response = llm.bind_tools([tavily_search]).invoke(prompt)\n\n    return { \"trending_topics\": response.content.strip() }\n\ndef analyze_sentiment(state: TypedDictState) -&gt; TypedDictState:\n\n    trending_topics = state['trending_topics']\n\n    prompt = f\"\"\"\n    Given the trending topics, your job is to analyze their sentiment.\n\n    Trending topics: {trending_topics}\n\n    Sentiment analysis:\"\"\"\n\n    response = llm.invoke(prompt)\n\n    return { \"sentiment_analysis\": response.content.strip() }\n\nbuilder = StateGraph(TypedDictState)\n\nbuilder.add_node(\"refine_user_query\", refine_user_query)\nbuilder.add_node(\"search_trending_topics\", search_trending_topics)\nbuilder.add_node(\"analyze_sentiment\", analyze_sentiment)\n\nbuilder.add_edge(START, \"refine_user_query\")\nbuilder.add_edge(\"refine_user_query\", \"search_trending_topics\")\nbuilder.add_edge(\"search_trending_topics\", \"analyze_sentiment\")\nbuilder.add_edge(\"analyze_sentiment\", END)\n\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n</pre> from langgraph.graph import START, END, StateGraph from IPython.display import Image, display from typing_extensions import TypedDict from langchain.chat_models import init_chat_model  llm = init_chat_model(\"openai:gpt-5-nano\")  class TypedDictState(TypedDict):     user_query: str     refined_query: str     trending_topics: str     sentiment_analysis: str  def refine_user_query(state: TypedDictState) -&gt; TypedDictState:     user_query = state['user_query']      prompt = f\"\"\"     Given the user query, your job is to rewrite it to make it more specific and clear.      User query: {user_query}      Revised query:\"\"\"      response = llm.invoke(prompt)      return { \"refined_query\": response.content.strip() }  def search_trending_topics(state: TypedDictState) -&gt; TypedDictState:     refined_query = state['refined_query']      prompt = f\"\"\"     Given the refined user query, your job is to search for trending topics related to it.      Refined query: {refined_query}      Trending topics:\"\"\"      response = llm.bind_tools([tavily_search]).invoke(prompt)      return { \"trending_topics\": response.content.strip() }  def analyze_sentiment(state: TypedDictState) -&gt; TypedDictState:      trending_topics = state['trending_topics']      prompt = f\"\"\"     Given the trending topics, your job is to analyze their sentiment.      Trending topics: {trending_topics}      Sentiment analysis:\"\"\"      response = llm.invoke(prompt)      return { \"sentiment_analysis\": response.content.strip() }  builder = StateGraph(TypedDictState)  builder.add_node(\"refine_user_query\", refine_user_query) builder.add_node(\"search_trending_topics\", search_trending_topics) builder.add_node(\"analyze_sentiment\", analyze_sentiment)  builder.add_edge(START, \"refine_user_query\") builder.add_edge(\"refine_user_query\", \"search_trending_topics\") builder.add_edge(\"search_trending_topics\", \"analyze_sentiment\") builder.add_edge(\"analyze_sentiment\", END)  graph = builder.compile()  display(Image(graph.get_graph(xray=True).draw_mermaid_png())) In\u00a0[6]: Copied! <pre>initial_state : TypedDictState = {\n    \"user_query\": \"Is people buying ETH on the last month?\",\n    \"refined_query\": \"\",\n    \"trending_topics\": \"\",\n    \"sentiment_analysis\": \"\"\n}\n\nstate = graph.invoke(input=initial_state)\n</pre> initial_state : TypedDictState = {     \"user_query\": \"Is people buying ETH on the last month?\",     \"refined_query\": \"\",     \"trending_topics\": \"\",     \"sentiment_analysis\": \"\" }  state = graph.invoke(input=initial_state) In\u00a0[7]: Copied! <pre>state\n</pre> state Out[7]: <pre>{'user_query': 'Is people buying ETH on the last month?',\n 'refined_query': 'Revised query: What has been Ethereum (ETH) buying activity in the last 30 days (the past month), specifically the total buy volume and net buy/sell amount on major exchanges?',\n 'trending_topics': '',\n 'sentiment_analysis': 'I\u2019m missing the actual list of trending topics. Please paste them (and optionally specify language and timeframe), and I\u2019ll return a sentiment analysis like:\\n\\n- Topic: &lt;topic name&gt;\\n  - Sentiment: Positive / Negative / Neutral\\n  - Score: [-1.0 to 1.0]\\n  - Key drivers: [terms that push sentiment]\\n  - Summary: short interpretation\\n\\nIf you\u2019d like, I can also output in JSON for easy parsing, or provide an overall sentiment across all topics.\\n\\nExample format (for one topic):\\n- Topic: \u201cTopic A\u201d\\n  - Sentiment: Positive\\n  - Score: 0.68\\n  - Key drivers: [\"positivity\", \"approval\", \"success\"]\\n  - Summary: Topic A is generally viewed positively, driven by favorable outcomes and endorsements.\\n\\nPlease share the trending topics and I\u2019ll analyze them.'}</pre> In\u00a0[8]: Copied! <pre>from IPython.display import Markdown, display\n\ndisplay(Markdown(f\"**Sentiment Analysis:**\\n\\n{state['sentiment_analysis']}\"))\n</pre> from IPython.display import Markdown, display  display(Markdown(f\"**Sentiment Analysis:**\\n\\n{state['sentiment_analysis']}\"))  <p>Sentiment Analysis:</p> <p>I\u2019m missing the actual list of trending topics. Please paste them (and optionally specify language and timeframe), and I\u2019ll return a sentiment analysis like:</p> <ul> <li>Topic: <ul> <li>Sentiment: Positive / Negative / Neutral</li> <li>Score: [-1.0 to 1.0]</li> <li>Key drivers: [terms that push sentiment]</li> <li>Summary: short interpretation</li> </ul> </li> </ul> <p>If you\u2019d like, I can also output in JSON for easy parsing, or provide an overall sentiment across all topics.</p> <p>Example format (for one topic):</p> <ul> <li>Topic: \u201cTopic A\u201d<ul> <li>Sentiment: Positive</li> <li>Score: 0.68</li> <li>Key drivers: [\"positivity\", \"approval\", \"success\"]</li> <li>Summary: Topic A is generally viewed positively, driven by favorable outcomes and endorsements.</li> </ul> </li> </ul> <p>Please share the trending topics and I\u2019ll analyze them.</p> In\u00a0[9]: Copied! <pre>from langchain_core.runnables import RunnableConfig\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ninitial_state : TypedDictState = {\n    \"user_query\": \"Is people buying ETH on the last month?\",\n    \"refined_query\": \"\",\n    \"trending_topics\": \"\",\n    \"sentiment_analysis\": \"\"\n}\n\n# Configuraci\u00f3n para poder disponer del estado de la ejecuci\u00f3n\nconfig : RunnableConfig = {\n    \"configurable\": {\n        # Esto funciona como una key para la corriga que voy a monitorear\n        \"thread_id\": \"web_search_run_1234\",\n    }\n}\n\n# Funciona como un logger en memoria\nmemory = InMemorySaver()\n\n# Conecto el checkpointer al grafo\ngraph = builder.compile(checkpointer=memory)\n\n# stream_mode disponibles =&gt; https://langchain-ai.github.io/langgraph/how-tos/streaming/\niter = graph.stream(initial_state, config=config, debug=True, stream_mode=\"updates\")\n\nfor state in iter:\n    print(state)\n</pre> from langchain_core.runnables import RunnableConfig from langgraph.checkpoint.memory import InMemorySaver  initial_state : TypedDictState = {     \"user_query\": \"Is people buying ETH on the last month?\",     \"refined_query\": \"\",     \"trending_topics\": \"\",     \"sentiment_analysis\": \"\" }  # Configuraci\u00f3n para poder disponer del estado de la ejecuci\u00f3n config : RunnableConfig = {     \"configurable\": {         # Esto funciona como una key para la corriga que voy a monitorear         \"thread_id\": \"web_search_run_1234\",     } }  # Funciona como un logger en memoria memory = InMemorySaver()  # Conecto el checkpointer al grafo graph = builder.compile(checkpointer=memory)  # stream_mode disponibles =&gt; https://langchain-ai.github.io/langgraph/how-tos/streaming/ iter = graph.stream(initial_state, config=config, debug=True, stream_mode=\"updates\")  for state in iter:     print(state)  <pre>[values] {'user_query': 'Is people buying ETH on the last month?', 'refined_query': '', 'trending_topics': '', 'sentiment_analysis': ''}\n[updates] {'refine_user_query': {'refined_query': 'Revised query: Have people been buying Ethereum (ETH) over the past 30 days, and what are the trends in ETH buying activity during that period?'}}\n{'refine_user_query': {'refined_query': 'Revised query: Have people been buying Ethereum (ETH) over the past 30 days, and what are the trends in ETH buying activity during that period?'}}\n[values] {'user_query': 'Is people buying ETH on the last month?', 'refined_query': 'Revised query: Have people been buying Ethereum (ETH) over the past 30 days, and what are the trends in ETH buying activity during that period?', 'trending_topics': '', 'sentiment_analysis': ''}\n[updates] {'search_trending_topics': {'trending_topics': ''}}\n{'search_trending_topics': {'trending_topics': ''}}\n[values] {'user_query': 'Is people buying ETH on the last month?', 'refined_query': 'Revised query: Have people been buying Ethereum (ETH) over the past 30 days, and what are the trends in ETH buying activity during that period?', 'trending_topics': '', 'sentiment_analysis': ''}\n[updates] {'analyze_sentiment': {'sentiment_analysis': 'I don\u2019t see the trending topics yet. Please provide the list of topics you want analyzed (one per line or as a comma/semicolon-separated list). If you can include the language and time window, that helps too.\\n\\nWhat I\u2019ll deliver:\\n- Per-topic sentiment: Positive, Neutral, or Negative (with a brief justification if helpful)\\n- Optional: a numeric score (e.g., -1 to -5 for negative to +5 for positive)\\n- Overall sentiment summary across all topics\\n- Quick notes on any notable clusters or common themes\\n\\nSample output format:\\nTopic: \"Topic text\"\\nSentiment: Positive\\nScore: +3\\nRationale: Uses upbeat language, mentions success or celebration\\n\\nTopic: \"Topic text 2\"\\nSentiment: Neutral\\nScore: 0\\nRationale: Descriptive, no clear sentiment\\n\\nWould you like me to analyze a sample now? If you paste a few topics, I can show exactly how the results will look.'}}\n{'analyze_sentiment': {'sentiment_analysis': 'I don\u2019t see the trending topics yet. Please provide the list of topics you want analyzed (one per line or as a comma/semicolon-separated list). If you can include the language and time window, that helps too.\\n\\nWhat I\u2019ll deliver:\\n- Per-topic sentiment: Positive, Neutral, or Negative (with a brief justification if helpful)\\n- Optional: a numeric score (e.g., -1 to -5 for negative to +5 for positive)\\n- Overall sentiment summary across all topics\\n- Quick notes on any notable clusters or common themes\\n\\nSample output format:\\nTopic: \"Topic text\"\\nSentiment: Positive\\nScore: +3\\nRationale: Uses upbeat language, mentions success or celebration\\n\\nTopic: \"Topic text 2\"\\nSentiment: Neutral\\nScore: 0\\nRationale: Descriptive, no clear sentiment\\n\\nWould you like me to analyze a sample now? If you paste a few topics, I can show exactly how the results will look.'}}\n[values] {'user_query': 'Is people buying ETH on the last month?', 'refined_query': 'Revised query: Have people been buying Ethereum (ETH) over the past 30 days, and what are the trends in ETH buying activity during that period?', 'trending_topics': '', 'sentiment_analysis': 'I don\u2019t see the trending topics yet. Please provide the list of topics you want analyzed (one per line or as a comma/semicolon-separated list). If you can include the language and time window, that helps too.\\n\\nWhat I\u2019ll deliver:\\n- Per-topic sentiment: Positive, Neutral, or Negative (with a brief justification if helpful)\\n- Optional: a numeric score (e.g., -1 to -5 for negative to +5 for positive)\\n- Overall sentiment summary across all topics\\n- Quick notes on any notable clusters or common themes\\n\\nSample output format:\\nTopic: \"Topic text\"\\nSentiment: Positive\\nScore: +3\\nRationale: Uses upbeat language, mentions success or celebration\\n\\nTopic: \"Topic text 2\"\\nSentiment: Neutral\\nScore: 0\\nRationale: Descriptive, no clear sentiment\\n\\nWould you like me to analyze a sample now? If you paste a few topics, I can show exactly how the results will look.'}\n</pre> In\u00a0[10]: Copied! <pre>initial_state : TypedDictState = {\n    \"user_query\": \"Is people buying ETH on the last month?\",\n    \"refined_query\": \"\",\n    \"trending_topics\": \"\",\n    \"sentiment_analysis\": \"\"\n}\n\niter = graph.stream(initial_state, interrupt_before=\"search_trending_topics\", debug=True, config=config, stream_mode=\"updates\")\n\nfor state in iter:\n    print(state)\n</pre> initial_state : TypedDictState = {     \"user_query\": \"Is people buying ETH on the last month?\",     \"refined_query\": \"\",     \"trending_topics\": \"\",     \"sentiment_analysis\": \"\" }  iter = graph.stream(initial_state, interrupt_before=\"search_trending_topics\", debug=True, config=config, stream_mode=\"updates\")  for state in iter:     print(state) <pre>[values] {'user_query': 'Is people buying ETH on the last month?', 'refined_query': '', 'trending_topics': '', 'sentiment_analysis': ''}\n[updates] {'refine_user_query': {'refined_query': 'Revised query: What has been the level of Ethereum (ETH) buying activity over the past month?'}}\n{'refine_user_query': {'refined_query': 'Revised query: What has been the level of Ethereum (ETH) buying activity over the past month?'}}\n[values] {'user_query': 'Is people buying ETH on the last month?', 'refined_query': 'Revised query: What has been the level of Ethereum (ETH) buying activity over the past month?', 'trending_topics': '', 'sentiment_analysis': ''}\n[updates] {'__interrupt__': ()}\n{'__interrupt__': ()}\n</pre> In\u00a0[11]: Copied! <pre>snapshot = graph.get_state(config=config)\n</pre> snapshot = graph.get_state(config=config) In\u00a0[12]: Copied! <pre>pp(snapshot.values)\n</pre> pp(snapshot.values) <pre>{\n  \"user_query\": \"Is people buying ETH on the last month?\",\n  \"refined_query\": \"Revised query: What has been the level of Ethereum (ETH) buying activity over the past month?\",\n  \"trending_topics\": \"\",\n  \"sentiment_analysis\": \"\"\n}\n</pre> In\u00a0[13]: Copied! <pre>graph.nodes\n</pre> graph.nodes Out[13]: <pre>{'__start__': &lt;langgraph.pregel._read.PregelNode at 0x117fb1fd0&gt;,\n 'refine_user_query': &lt;langgraph.pregel._read.PregelNode at 0x117fb3c10&gt;,\n 'search_trending_topics': &lt;langgraph.pregel._read.PregelNode at 0x117fb3910&gt;,\n 'analyze_sentiment': &lt;langgraph.pregel._read.PregelNode at 0x117fba250&gt;}</pre> In\u00a0[14]: Copied! <pre>next_state = graph.nodes['search_trending_topics'].invoke(snapshot.values)\n</pre> next_state = graph.nodes['search_trending_topics'].invoke(snapshot.values) In\u00a0[15]: Copied! <pre>next_state\n</pre> next_state Out[15]: <pre>{'trending_topics': ''}</pre> In\u00a0[16]: Copied! <pre>def search_trending_topics(state: TypedDictState) -&gt; TypedDictState:\n    refined_query = state['refined_query']\n\n    prompt = f\"\"\"\n    Given the refined user query, your job is to search for trending topics related to it.\n\n    Refined query: {refined_query}\n\n    Trending topics:\"\"\"\n\n    response = llm.bind_tools([tavily_search]).invoke(prompt)\n\n    pp(response.tool_calls)\n\n    return { \"trending_topics\": response.content.strip() }\n\nsearch_trending_topics(snapshot.values)\n</pre> def search_trending_topics(state: TypedDictState) -&gt; TypedDictState:     refined_query = state['refined_query']      prompt = f\"\"\"     Given the refined user query, your job is to search for trending topics related to it.      Refined query: {refined_query}      Trending topics:\"\"\"      response = llm.bind_tools([tavily_search]).invoke(prompt)      pp(response.tool_calls)      return { \"trending_topics\": response.content.strip() }  search_trending_topics(snapshot.values) <pre>[\n  {\n    \"name\": \"tavily_search\",\n    \"args\": {\n      \"query\": \"Ethereum buying activity past month on-chain metrics\",\n      \"time_range\": \"month\",\n      \"search_depth\": \"advanced\",\n      \"include_images\": false,\n      \"start_date\": null,\n      \"end_date\": null,\n      \"include_domains\": null,\n      \"exclude_domains\": null,\n      \"topic\": \"general\",\n      \"include_favicon\": false\n    },\n    \"id\": \"call_D2QvbYbNmPIaABSDM547FSO9\",\n    \"type\": \"tool_call\"\n  },\n  {\n    \"name\": \"tavily_search\",\n    \"args\": {\n      \"query\": \"ETH whale activity last month\",\n      \"time_range\": \"month\",\n      \"search_depth\": \"advanced\",\n      \"include_images\": false,\n      \"start_date\": null,\n      \"end_date\": null,\n      \"include_domains\": null,\n      \"exclude_domains\": null,\n      \"topic\": \"finance\",\n      \"include_favicon\": false\n    },\n    \"id\": \"call_kCJsMdjc3XUKTmdYmiPkqwf6\",\n    \"type\": \"tool_call\"\n  },\n  {\n    \"name\": \"tavily_search\",\n    \"args\": {\n      \"query\": \"ETH exchange inflows outflows past 30 days\",\n      \"time_range\": \"month\",\n      \"search_depth\": \"advanced\",\n      \"include_images\": false,\n      \"start_date\": null,\n      \"end_date\": null,\n      \"include_domains\": null,\n      \"exclude_domains\": null,\n      \"topic\": \"finance\",\n      \"include_favicon\": false\n    },\n    \"id\": \"call_vS2fV0Is8b4SbrOz8uNLPBHm\",\n    \"type\": \"tool_call\"\n  }\n]\n</pre> Out[16]: <pre>{'trending_topics': ''}</pre>"},{"location":"dia2/notebooks/web-search/#langgraph-doing-a-web-search-plus-sentiment-analysis","title":"LangGraph doing a web search plus sentiment analysis\u00b6","text":"<ul> <li>Sign up to: https://app.tavily.com/home</li> <li>Tavily search tool: https://python.langchain.com/docs/integrations/tools/tavily_search</li> <li>LangGraph persistence: https://langchain-ai.github.io/langgraph/concepts/persistence/</li> </ul>"},{"location":"dia2/notebooks/web-search/#tavily-search-tool","title":"Tavily Search Tool\u00b6","text":""},{"location":"dia2/notebooks/web-search/#langgraph-basics","title":"LangGraph Basics\u00b6","text":""},{"location":"dia2/notebooks/web-search/#ejecucion-secuencial-utilizando-iteradores-aka-stream","title":"Ejecuci\u00f3n secuencial utilizando iteradores (aka stream)\u00b6","text":""},{"location":"dia2/notebooks/web-search/#quien-ejecuta-la-tool-call","title":"Quien ejecuta la Tool Call?\u00b6","text":""},{"location":"dia3/clases/","title":"Protocolos y desaf\u00edos avanzados","text":""},{"location":"dia3/notebooks/docrag_client/","title":"Docrag client","text":"In\u00a0[\u00a0]: Copied! <pre>import asyncio\nfrom fastmcp import Client\nimport argparse\n</pre> import asyncio from fastmcp import Client import argparse In\u00a0[\u00a0]: Copied! <pre>parser = argparse.ArgumentParser(description=\"...\")\nparser.add_argument('-q', '--question', type=str, required=True)\nparser.add_argument(\"-r\", \"--remote\", action=\"store_true\", help=\"User HTTP protocol (default: False)\")\n</pre> parser = argparse.ArgumentParser(description=\"...\") parser.add_argument('-q', '--question', type=str, required=True) parser.add_argument(\"-r\", \"--remote\", action=\"store_true\", help=\"User HTTP protocol (default: False)\") In\u00a0[\u00a0]: Copied! <pre>async def call_tool(name: str):\n    \n    async with client:\n        #\u00a0Basic server interaction\n        await client.ping()\n        print(\"Server is reachable\")\n\n        #\u00a0List available operations\n        tools = await client.list_tools()\n        resources = await client.list_resources()\n        prompts = await client.list_prompts()\n\n        print(tools)\n        print(resources)\n        print(prompts)\n\n        result = await client.call_tool(\"search_docs\", {\"query\": name})\n        print(result.content[-1].text)\n</pre> async def call_tool(name: str):          async with client:         #\u00a0Basic server interaction         await client.ping()         print(\"Server is reachable\")          #\u00a0List available operations         tools = await client.list_tools()         resources = await client.list_resources()         prompts = await client.list_prompts()          print(tools)         print(resources)         print(prompts)          result = await client.call_tool(\"search_docs\", {\"query\": name})         print(result.content[-1].text) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    args = parser.parse_args()\n    print(\"Running Python script ...\") #, args)\n\n    if args.question:\n        if args.remote:\n            print(\"Using remote client...\")\n            client = Client(\"http://localhost:8000/mcp\")\n        else:\n            # Local client\n            print(\"Using local client...\")\n            client = Client(\"docrag_server.py\")\n\n        asyncio.run(call_tool(args.question))\n        \n    print(\"--- Done ---\")\n</pre> if __name__ == \"__main__\":     args = parser.parse_args()     print(\"Running Python script ...\") #, args)      if args.question:         if args.remote:             print(\"Using remote client...\")             client = Client(\"http://localhost:8000/mcp\")         else:             # Local client             print(\"Using local client...\")             client = Client(\"docrag_server.py\")          asyncio.run(call_tool(args.question))              print(\"--- Done ---\")"},{"location":"dia3/notebooks/docrag_server/","title":"Docrag server","text":"In\u00a0[\u00a0]: Copied! <pre>from fastmcp import FastMCP\n</pre> from fastmcp import FastMCP In\u00a0[\u00a0]: Copied! <pre>from dotenv import load_dotenv\nimport os \n</pre> from dotenv import load_dotenv import os  In\u00a0[\u00a0]: Copied! <pre>from langchain_chroma import Chroma # This is a Chroma wrapper from Langchain\nfrom langchain_openai import ChatOpenAI # Import OpenAI LLM\nfrom langchain import hub\nfrom langchain_huggingface import HuggingFaceEmbeddings\n</pre> from langchain_chroma import Chroma # This is a Chroma wrapper from Langchain from langchain_openai import ChatOpenAI # Import OpenAI LLM from langchain import hub from langchain_huggingface import HuggingFaceEmbeddings In\u00a0[\u00a0]: Copied! <pre>from langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\n</pre> from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser In\u00a0[\u00a0]: Copied! <pre>import argparse\n</pre> import argparse In\u00a0[\u00a0]: Copied! <pre>from langchain_core.prompts import PromptTemplate\nfrom langchain_core.prompts import ChatPromptTemplate\n</pre> from langchain_core.prompts import PromptTemplate from langchain_core.prompts import ChatPromptTemplate In\u00a0[\u00a0]: Copied! <pre>TEMPLATE = \"\"\"You are a helpful AI assistant for question-answering tasks. \\\n    Use the following pieces of retrieved context to answer the question. \\\n    If you don't know the answer, just say that you don't know, don't try to make up an answer\n\n    Question: {question}\n   \n    Context: {context}\n\n    Answer (answer in Spanish, only if question is in Spanish):\n\"\"\"\n</pre> TEMPLATE = \"\"\"You are a helpful AI assistant for question-answering tasks. \\     Use the following pieces of retrieved context to answer the question. \\     If you don't know the answer, just say that you don't know, don't try to make up an answer      Question: {question}         Context: {context}      Answer (answer in Spanish, only if question is in Spanish): \"\"\" In\u00a0[\u00a0]: Copied! <pre>load_dotenv()  # Load environment variables from .env file\n</pre> load_dotenv()  # Load environment variables from .env file In\u00a0[\u00a0]: Copied! <pre>os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n</pre> os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>mcp = FastMCP(\"docrag-server\")\n</pre> mcp = FastMCP(\"docrag-server\") In\u00a0[\u00a0]: Copied! <pre># initialize once\nrag_prompt = hub.pull(\"rlm/rag-prompt\", include_model=True)\n#\u00a0rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)\n</pre> # initialize once rag_prompt = hub.pull(\"rlm/rag-prompt\", include_model=True) #\u00a0rag_prompt = ChatPromptTemplate.from_template(TEMPLATE) In\u00a0[\u00a0]: Copied! <pre>embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Local model\n</pre> embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Local model In\u00a0[\u00a0]: Copied! <pre>llm_model = os.environ[\"OPENAI_MODEL\"]\nllm = ChatOpenAI(model=llm_model, temperature=0.1)\n</pre> llm_model = os.environ[\"OPENAI_MODEL\"] llm = ChatOpenAI(model=llm_model, temperature=0.1) In\u00a0[\u00a0]: Copied! <pre>vectorstore_chroma = Chroma(\n        collection_name=\"project_collection\",\n        embedding_function=embeddings,\n        persist_directory=\"./project_chroma_db\" # Optional: specify a directory to persist your data\n    )\n</pre> vectorstore_chroma = Chroma(         collection_name=\"project_collection\",         embedding_function=embeddings,         persist_directory=\"./project_chroma_db\" # Optional: specify a directory to persist your data     ) In\u00a0[\u00a0]: Copied! <pre>retriever = vectorstore_chroma.as_retriever(search_kwargs={\"k\": 7})\n</pre> retriever = vectorstore_chroma.as_retriever(search_kwargs={\"k\": 7}) In\u00a0[\u00a0]: Copied! <pre>rag_chain = {\"context\": retriever,  \"question\": RunnablePassthrough()} | rag_prompt | llm | StrOutputParser()\n</pre> rag_chain = {\"context\": retriever,  \"question\": RunnablePassthrough()} | rag_prompt | llm | StrOutputParser() In\u00a0[\u00a0]: Copied! <pre>@mcp.tool(description=\"Search architecture Surrogates scientific documentation and summarize relevant info\")\ndef search_docs(query: str) -&gt; str:\n    # \n    result = rag_chain.invoke(query)\n    #\n    return result.strip()\n</pre> @mcp.tool(description=\"Search architecture Surrogates scientific documentation and summarize relevant info\") def search_docs(query: str) -&gt; str:     #      result = rag_chain.invoke(query)     #     return result.strip() In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    mcp.run(transport=\"http\", port=8000)\n    # mcp.run()\n</pre> if __name__ == \"__main__\":     mcp.run(transport=\"http\", port=8000)     # mcp.run() <p>parser = argparse.ArgumentParser(description=\"...\") parser.add_argument('-q', '--question', type=str, required=True)</p> <p>if name == \"main\": args = parser.parse_args() print(\"Running Python script ...\") #, args)</p> <pre><code>if args.question:\n    print(search_docs(args.question))</code></pre> <pre><code>print(\"--- Done ---\")</code></pre>"},{"location":"dia3/notebooks/ingest/","title":"Ingest","text":"In\u00a0[\u00a0]: Copied! <pre>from dotenv import load_dotenv\nimport argparse\n</pre> from dotenv import load_dotenv import argparse In\u00a0[\u00a0]: Copied! <pre>from langchain.document_loaders.pdf import PyPDFDirectoryLoader \nfrom langchain.document_loaders import PyPDFLoader\n</pre> from langchain.document_loaders.pdf import PyPDFDirectoryLoader  from langchain.document_loaders import PyPDFLoader In\u00a0[\u00a0]: Copied! <pre>from langchain.text_splitter import RecursiveCharacterTextSplitter \nfrom langchain_chroma import Chroma # This is a Chroma wrapper from Langchain\nfrom langchain_huggingface import HuggingFaceEmbeddings\n</pre> from langchain.text_splitter import RecursiveCharacterTextSplitter  from langchain_chroma import Chroma # This is a Chroma wrapper from Langchain from langchain_huggingface import HuggingFaceEmbeddings In\u00a0[\u00a0]: Copied! <pre>load_dotenv()  # Load environment variables from a .env file if present\n</pre> load_dotenv()  # Load environment variables from a .env file if present In\u00a0[\u00a0]: Copied! <pre>embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Local model\n</pre> embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Local model In\u00a0[\u00a0]: Copied! <pre>def ingest(pdf_path: str):\n\n    loader = PyPDFLoader(pdf_path) # Tool to load and process a PDF file\n    pdf_documents = loader.load() # Each document corresponds actually to a page\n    print(len(pdf_documents), \"loaded\")\n\n    # Split documents into chunks\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000, chunk_overlap=100)\n\n    texts = text_splitter.split_documents(pdf_documents)\n    print(len(texts), \"chunks\")\n\n    # We use a simple vector store for the chunks\n    vectorstore_chroma = Chroma(\n            collection_name=\"project_collection\",\n            embedding_function=embeddings,\n            persist_directory=\"./project_chroma_db\" # Optional: specify a directory to persist your data\n        )\n    vectorstore_chroma.add_documents(texts)\n</pre> def ingest(pdf_path: str):      loader = PyPDFLoader(pdf_path) # Tool to load and process a PDF file     pdf_documents = loader.load() # Each document corresponds actually to a page     print(len(pdf_documents), \"loaded\")      # Split documents into chunks     text_splitter = RecursiveCharacterTextSplitter(         chunk_size=1000, chunk_overlap=100)      texts = text_splitter.split_documents(pdf_documents)     print(len(texts), \"chunks\")      # We use a simple vector store for the chunks     vectorstore_chroma = Chroma(             collection_name=\"project_collection\",             embedding_function=embeddings,             persist_directory=\"./project_chroma_db\" # Optional: specify a directory to persist your data         )     vectorstore_chroma.add_documents(texts) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>parser = argparse.ArgumentParser(description=\"...\")\nparser.add_argument('-d', '--pdf_path', type=str, required=True)\n</pre> parser = argparse.ArgumentParser(description=\"...\") parser.add_argument('-d', '--pdf_path', type=str, required=True) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    args = parser.parse_args()\n    print(\"Ingesting PDF document...\", args.pdf_path)\n    ingest(args.pdf_path)\n</pre> if __name__ == \"__main__\":     args = parser.parse_args()     print(\"Ingesting PDF document...\", args.pdf_path)     ingest(args.pdf_path)"},{"location":"dia3/notebooks/langchain_agent/","title":"Langchain agent","text":"In\u00a0[12]: Copied! <pre>from langchain_mcp_adapters.client import MultiServerMCPClient  \nfrom langgraph.prebuilt import create_react_agent\n</pre> from langchain_mcp_adapters.client import MultiServerMCPClient   from langgraph.prebuilt import create_react_agent   In\u00a0[13]: Copied! <pre>from dotenv import load_dotenv\nimport os \n\nload_dotenv()  # Carga las variables de entorno desde el archivo .env\n</pre> from dotenv import load_dotenv import os   load_dotenv()  # Carga las variables de entorno desde el archivo .env Out[13]: <pre>True</pre> In\u00a0[14]: Copied! <pre>client = MultiServerMCPClient(  \n    {\n        \"github\": {\n            \"transport\": \"streamable_http\",  # HTTP-based remote server\n            \"url\": \"https://api.githubcopilot.com/mcp/\",\n            \"headers\": {\n                \"Authorization\": os.environ[\"GITHUB_TOKEN\"]\n            },\n            \"timeout\": 5000\n        },\n        # \"math\": {\n        #     \"transport\": \"stdio\",  # Local subprocess communication\n        #     \"command\": \"python\",\n        #     # Absolute path to your math_server.py file\n        #     \"args\": [\"/path/to/math_server.py\"],\n        # },\n        \"docrag-server\": {\n            \"transport\": \"streamable_http\",  # HTTP-based remote server\n            # Ensure you start your weather server on port 8000\n            \"url\": \"http://127.0.0.1:8000/mcp\",\n        }\n    }\n)\n</pre> client = MultiServerMCPClient(       {         \"github\": {             \"transport\": \"streamable_http\",  # HTTP-based remote server             \"url\": \"https://api.githubcopilot.com/mcp/\",             \"headers\": {                 \"Authorization\": os.environ[\"GITHUB_TOKEN\"]             },             \"timeout\": 5000         },         # \"math\": {         #     \"transport\": \"stdio\",  # Local subprocess communication         #     \"command\": \"python\",         #     # Absolute path to your math_server.py file         #     \"args\": [\"/path/to/math_server.py\"],         # },         \"docrag-server\": {             \"transport\": \"streamable_http\",  # HTTP-based remote server             # Ensure you start your weather server on port 8000             \"url\": \"http://127.0.0.1:8000/mcp\",         }     } ) In\u00a0[15]: Copied! <pre>tools = await client.get_tools()  \n</pre> tools = await client.get_tools()    In\u00a0[\u00a0]: Copied! <pre>agent = create_react_agent(\n    \"openai:gpt-4.1\",\n    tools,  \n)\n</pre> agent = create_react_agent(     \"openai:gpt-4.1\",     tools,   ) In\u00a0[\u00a0]: Copied! <pre># test_query = \"I want to understand the main AWS services related to compute and storage, and how they integrate with each other.\"\n#\u00a0test_query = \"What are the contributors to my architecture surrogates project (user: andresdp)?\"\nmy_project = \"architecture-surrogates\"\nmy_owner = \"andresdp\"\n\ntest_query = \"\"\"Identify open documentation-related issues in Github project {project} (owner: \n  {owner}). For each one, search the most relevant scientific documentation sections that could be expanded or linked to. \n  Suggest improvements for the issues.\n\"\"\"\n# Check if there are any open GitHub issues in my projects (user: andredp) related to AWS services, so that the issue could be solvable with AWS.\n# \"\"\"\n\nagent_response = await agent.ainvoke({\"messages\": test_query.format(project=my_project, owner=my_owner)})\nprint(agent_response['messages'][-1].text())\n</pre>  # test_query = \"I want to understand the main AWS services related to compute and storage, and how they integrate with each other.\" #\u00a0test_query = \"What are the contributors to my architecture surrogates project (user: andresdp)?\" my_project = \"architecture-surrogates\" my_owner = \"andresdp\"  test_query = \"\"\"Identify open documentation-related issues in Github project {project} (owner:    {owner}). For each one, search the most relevant scientific documentation sections that could be expanded or linked to.    Suggest improvements for the issues. \"\"\" # Check if there are any open GitHub issues in my projects (user: andredp) related to AWS services, so that the issue could be solvable with AWS. # \"\"\"  agent_response = await agent.ainvoke({\"messages\": test_query.format(project=my_project, owner=my_owner)}) print(agent_response['messages'][-1].text()) <pre>Here are the open documentation-related issues in the \"architecture-surrogates\" repository by andresdp, along with improvement suggestions and relevant scientific documentation insights:\n\n---\n\n1. Issue: Clarify how architectural features are extracted from JSON files\n   - Issue Summary: The extraction process in extract_features.py is unclear; the relationship between features in architecture JSONs and surrogate inputs needs explanation in docs/architecture-surrogates-overview.md.\n   - Relevant Scientific Documentation Insight:\n     - Architectural feature extraction typically involves representing architecture characteristics as informative features (e.g., one-hot encoding for tactics, graph embeddings for structure).\n     - These encoded features allow for downstream machine learning and optimization tasks.\n   - Improvement Suggestions:\n     - Add a clear explanation in overview.md describing how JSON fields are mapped/encoded into model features (including any one-hot or embedding scheme used).\n     - Illustrate with an example: show a snippet of an input JSON and how it becomes a feature vector for the surrogate.\n     - Consider linking to background materials on graph embeddings and feature engineering for architectures.\n\n---\n\n2. Issue: Missing explanation of surrogate model training process\n   - Issue Summary: The documentation does not detail how the surrogate model's training data is generated or normalized; should be covered in docs/model_training.md.\n   - Relevant Scientific Documentation Insight:\n     - Surrogate model training requires a dataset of architecture configurations and quality attribute values, generated by sampling or heuristics.\n     - Data is often normalized to ensure effective model convergence.\n     - The model is retrained as more data becomes available.\n   - Improvement Suggestions:\n     - Add a section explaining how architectural design points are sampled/generated for training.\n     - Describe any normalization or preprocessing applied to both features and target values.\n     - Optionally, provide a visual pipeline or workflow chart showing data generation, normalization, training, and re-training steps.\n     - Link to resources on surrogate modeling best practices and rationale for data normalization.\n\nWould you like step-by-step assistance drafting text for these documentation improvements?\n</pre>"},{"location":"dia3/notebooks/langgraph-interrupt/","title":"Human in the Loop","text":"<p>Langgraph supports different ways of interrumpting the control flow of a graph and allow for human intervention. The interruption of the states</p> In\u00a0[7]: Copied! <pre>from dotenv import load_dotenv\nimport pandas as pd\nfrom pathlib import Path\nimport json\nfrom dotenv import load_dotenv\nimport os \nimport pprint\nfrom IPython.display import Image, display\nfrom typing import TypedDict, Annotated, Literal, Dict, Any\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n</pre> from dotenv import load_dotenv import pandas as pd from pathlib import Path import json from dotenv import load_dotenv import os  import pprint from IPython.display import Image, display from typing import TypedDict, Annotated, Literal, Dict, Any from rich.console import Console from rich.markdown import Markdown <p>Import the necessary classes from Langchain and Langgraph</p> In\u00a0[2]: Copied! <pre>from langgraph.graph import StateGraph, END\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.checkpoint.memory import MemorySaver\n</pre> from langgraph.graph import StateGraph, END from langchain.chat_models import init_chat_model from langgraph.checkpoint.memory import MemorySaver  In\u00a0[61]: Copied! <pre># %pip install rich\n</pre> # %pip install rich In\u00a0[3]: Copied! <pre># Load environment variables from .env file\nload_dotenv()\n</pre> # Load environment variables from .env file load_dotenv() Out[3]: <pre>True</pre> <p>We define an LLM model to use in a zero-shot mode to answer user questions</p> In\u00a0[4]: Copied! <pre>llm_model = \"openai:\"+os.getenv(\"OPENAI_MODEL\")\nprint(llm_model)\nllm = init_chat_model(llm_model, temperature=0)\n</pre> llm_model = \"openai:\"+os.getenv(\"OPENAI_MODEL\") print(llm_model) llm = init_chat_model(llm_model, temperature=0) <pre>openai:gpt-4o-mini\n</pre> In\u00a0[5]: Copied! <pre># Define the state schema\nclass HumanInTheLoopState(TypedDict):\n    question: str  # User's original question\n    ai_draft: str  # AI's initial response draft\n    human_feedback: str  # Human reviewer's feedback\n    final_response: str  # Final response after incorporating feedback\n</pre> # Define the state schema class HumanInTheLoopState(TypedDict):     question: str  # User's original question     ai_draft: str  # AI's initial response draft     human_feedback: str  # Human reviewer's feedback     final_response: str  # Final response after incorporating feedback <p>Definition of the nodes for the graph</p> In\u00a0[8]: Copied! <pre>def draft_response(state: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"AI drafts an initial response to the user's question\"\"\"\n    question = state[\"question\"]\n    \n    # In a real application, this would use an LLM to generate a response\n    #\u00a0draft = f\"DRAFT: Here's my initial answer to: '{question}'\"\n    draft = llm.invoke(\"Answer the following question: \"+question)\n    \n    print(f\"\\n\ud83e\udd16 AI has drafted an initial response\")\n    #\u00a0pprint.pprint(f\"{draft.content}\\n\")\n    return {\"ai_draft\": draft.content}\n\ndef get_human_feedback(state: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Collect feedback from a human reviewer\"\"\"\n    # In a real application, this would be implemented with a UI\n    # or messaging platform to collect human input\n    print(\"\\n\ud83d\udc4b HUMAN REVIEW REQUIRED!\\n\")\n    print(f\"Original question: {state['question']}\")\n    #\u00a0print(f\"AI draft: {state['ai_draft']}\")\n    renderable_markup = Markdown(f\"**AI draft:** {state['ai_draft']}\")\n    console.print(renderable_markup)\n\n    # Simulating human input via console\n    feedback = input(\"\\nPlease provide feedback or type 'approve' to accept: \")\n    \n    print(f\"\\n\ud83d\udc64 Human provided feedback: {feedback}\\n\")\n    return {\"human_feedback\": feedback}\n\ndef decide_next_step(state: Dict[str, Any]) -&gt; Literal[\"revise\", \"finalize\"]:\n    \"\"\"Decide whether to revise the response or finalize it\"\"\"\n    if state[\"human_feedback\"].lower() == \"approve\":\n        return \"finalize\"\n    else:\n        return \"revise\"\n\ndef revise_response(state: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Revise the response based on human feedback\"\"\"\n    # In a real application, this would use an LLM to incorporate feedback\n    old_draft = \"For the following question: \"+ state['question']+ \", you previously provided this answer: \"+ state['ai_draft']\n    revised_response = llm.invoke(old_draft + \". The user provided feedback: \"+ state['human_feedback']+ \". Please revisit it and provide a new answer.\")\n    #\u00a0revised_response = f\"REVISED: I've updated my answer based on feedback: '{state['human_feedback']}'\"\n    \n    print(f\"\\n\ud83e\udd16 AI has revised the response.\")\n    #\u00a0pprint.pprint(f\"{revised_response.content}\\n\")\n    return {\"ai_draft\": revised_response.content}\n\ndef finalize_response(state: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Finalize the response and return to the user\"\"\"\n    \n    print(f\"\\n\u2705 Final response ready:\")\n    renderable_markup = Markdown(f\"{state['ai_draft']}\\n\")\n    console.print(renderable_markup)\n\n    return {\"final_response\": state['ai_draft']}\n</pre> def draft_response(state: Dict[str, Any]) -&gt; Dict[str, Any]:     \"\"\"AI drafts an initial response to the user's question\"\"\"     question = state[\"question\"]          # In a real application, this would use an LLM to generate a response     #\u00a0draft = f\"DRAFT: Here's my initial answer to: '{question}'\"     draft = llm.invoke(\"Answer the following question: \"+question)          print(f\"\\n\ud83e\udd16 AI has drafted an initial response\")     #\u00a0pprint.pprint(f\"{draft.content}\\n\")     return {\"ai_draft\": draft.content}  def get_human_feedback(state: Dict[str, Any]) -&gt; Dict[str, Any]:     \"\"\"Collect feedback from a human reviewer\"\"\"     # In a real application, this would be implemented with a UI     # or messaging platform to collect human input     print(\"\\n\ud83d\udc4b HUMAN REVIEW REQUIRED!\\n\")     print(f\"Original question: {state['question']}\")     #\u00a0print(f\"AI draft: {state['ai_draft']}\")     renderable_markup = Markdown(f\"**AI draft:** {state['ai_draft']}\")     console.print(renderable_markup)      # Simulating human input via console     feedback = input(\"\\nPlease provide feedback or type 'approve' to accept: \")          print(f\"\\n\ud83d\udc64 Human provided feedback: {feedback}\\n\")     return {\"human_feedback\": feedback}  def decide_next_step(state: Dict[str, Any]) -&gt; Literal[\"revise\", \"finalize\"]:     \"\"\"Decide whether to revise the response or finalize it\"\"\"     if state[\"human_feedback\"].lower() == \"approve\":         return \"finalize\"     else:         return \"revise\"  def revise_response(state: Dict[str, Any]) -&gt; Dict[str, Any]:     \"\"\"Revise the response based on human feedback\"\"\"     # In a real application, this would use an LLM to incorporate feedback     old_draft = \"For the following question: \"+ state['question']+ \", you previously provided this answer: \"+ state['ai_draft']     revised_response = llm.invoke(old_draft + \". The user provided feedback: \"+ state['human_feedback']+ \". Please revisit it and provide a new answer.\")     #\u00a0revised_response = f\"REVISED: I've updated my answer based on feedback: '{state['human_feedback']}'\"          print(f\"\\n\ud83e\udd16 AI has revised the response.\")     #\u00a0pprint.pprint(f\"{revised_response.content}\\n\")     return {\"ai_draft\": revised_response.content}  def finalize_response(state: Dict[str, Any]) -&gt; Dict[str, Any]:     \"\"\"Finalize the response and return to the user\"\"\"          print(f\"\\n\u2705 Final response ready:\")     renderable_markup = Markdown(f\"{state['ai_draft']}\\n\")     console.print(renderable_markup)      return {\"final_response\": state['ai_draft']} <p>Assembling the graph</p> In\u00a0[9]: Copied! <pre># Initialize the graph\ngraph = StateGraph(HumanInTheLoopState)\n\n# Add nodes\ngraph.add_node(\"draft\", draft_response)\ngraph.add_node(\"human_review\", get_human_feedback)  # Renamed from \"human_feedback\"\ngraph.add_node(\"revise\", revise_response)\ngraph.add_node(\"finalize\", finalize_response)\n\n# Define the flow\ngraph.add_edge(\"draft\", \"human_review\")\ngraph.add_conditional_edges(\n    \"human_review\",\n    decide_next_step,\n    {\n        \"revise\": \"revise\",\n        \"finalize\": \"finalize\"\n    }\n)\ngraph.add_edge(\"revise\", \"human_review\")\ngraph.add_edge(\"finalize\", END)\n\n# Set the entry point\ngraph.set_entry_point(\"draft\")\n\ncheckpointer = MemorySaver()\nworkflow = graph.compile(checkpointer=checkpointer)\n\ndisplay(Image(workflow.get_graph().draw_mermaid_png()))\n</pre> # Initialize the graph graph = StateGraph(HumanInTheLoopState)  # Add nodes graph.add_node(\"draft\", draft_response) graph.add_node(\"human_review\", get_human_feedback)  # Renamed from \"human_feedback\" graph.add_node(\"revise\", revise_response) graph.add_node(\"finalize\", finalize_response)  # Define the flow graph.add_edge(\"draft\", \"human_review\") graph.add_conditional_edges(     \"human_review\",     decide_next_step,     {         \"revise\": \"revise\",         \"finalize\": \"finalize\"     } ) graph.add_edge(\"revise\", \"human_review\") graph.add_edge(\"finalize\", END)  # Set the entry point graph.set_entry_point(\"draft\")  checkpointer = MemorySaver() workflow = graph.compile(checkpointer=checkpointer)  display(Image(workflow.get_graph().draw_mermaid_png())) In\u00a0[10]: Copied! <pre>from rich.console import Console\nfrom rich.markdown import Markdown\n\n# This console is only for rendering Markup content nicely\nconsole = Console()\n</pre> from rich.console import Console from rich.markdown import Markdown  # This console is only for rendering Markup content nicely console = Console() <p>Testing a simple \"approve\" interruption</p> In\u00a0[11]: Copied! <pre># Initial state with a user question\nquery = \"What's the best way to implement HITL systems?\"\n\ninitial_state = {\n    \"question\": query,\n    \"ai_draft\": \"\",\n    \"human_feedback\": \"\",\n    \"final_response\": \"\",\n    \"thread_id\": 1\n}\n\n# Run the workflow\nconfig = {\"configurable\": {\"thread_id\": 1}}\nfor output in workflow.stream(initial_state, config):\n    # In a production application, you would handle these events appropriately\n    pass\n    \n# Final state contains the complete conversation\nprint(\"\\n=== WORKFLOW COMPLETED ===\")\n</pre> # Initial state with a user question query = \"What's the best way to implement HITL systems?\"  initial_state = {     \"question\": query,     \"ai_draft\": \"\",     \"human_feedback\": \"\",     \"final_response\": \"\",     \"thread_id\": 1 }  # Run the workflow config = {\"configurable\": {\"thread_id\": 1}} for output in workflow.stream(initial_state, config):     # In a production application, you would handle these events appropriately     pass      # Final state contains the complete conversation print(\"\\n=== WORKFLOW COMPLETED ===\") <pre>\n\ud83e\udd16 AI has drafted an initial response\n\n\ud83d\udc4b HUMAN REVIEW REQUIRED!\n\nOriginal question: What's the best way to implement HITL systems?\n</pre> <pre>AI draft: Implementing Human-in-the-Loop (HITL) systems effectively involves several key steps and considerations. \nHere\u2019s a structured approach to ensure successful implementation:                                                  \n\n  1 Define Objectives and Use Cases:                                                                               \n     \u2022 Clearly outline the goals of the HITL system. Identify specific tasks where human input is necessary, such  \n       as data labeling, decision-making, or quality assurance.                                                    \n     \u2022 Determine the scenarios in which human intervention will enhance the system's performance.                  \n  2 Select Appropriate Technology:                                                                                 \n     \u2022 Choose the right tools and platforms that support HITL processes. This may include machine learning         \n       frameworks, data annotation tools, and user interfaces for human operators.                                 \n     \u2022 Ensure that the technology can seamlessly integrate with existing systems.                                  \n  3 Design the Workflow:                                                                                           \n     \u2022 Create a workflow that defines how humans will interact with the system. This includes specifying when and  \n       how human input is solicited, as well as how feedback is incorporated into the system.                      \n     \u2022 Consider using iterative processes where human feedback can continuously improve the model.                 \n  4 User Interface and Experience:                                                                                 \n     \u2022 Develop an intuitive user interface that allows human operators to easily provide input and feedback. The   \n       design should minimize cognitive load and facilitate quick decision-making.                                 \n     \u2022 Provide clear instructions and context for tasks to ensure that users understand their role.                \n  5 Training and Onboarding:                                                                                       \n     \u2022 Train human operators on the system, including how to use the interface and the importance of their         \n       contributions.                                                                                              \n     \u2022 Offer ongoing support and resources to help users adapt to the system and improve their performance.        \n  6 Feedback Mechanisms:                                                                                           \n     \u2022 Implement mechanisms for collecting feedback from human operators about the system\u2019s performance and        \n       usability.                                                                                                  \n     \u2022 Use this feedback to make iterative improvements to both the technology and the workflow.                   \n  7 Monitoring and Evaluation:                                                                                     \n     \u2022 Continuously monitor the performance of the HITL system. Analyze the impact of human input on the overall   \n       outcomes.                                                                                                   \n     \u2022 Evaluate the effectiveness of the system regularly and make adjustments as needed.                          \n  8 Ethical Considerations:                                                                                        \n     \u2022 Address ethical concerns related to data privacy, bias, and the role of human judgment in decision-making.  \n     \u2022 Ensure transparency in how human input is used and maintain accountability for decisions made by the system.\n  9 Scalability and Adaptability:                                                                                  \n     \u2022 Design the HITL system to be scalable, allowing for the integration of more human operators or the expansion\n       of use cases as needed.                                                                                     \n     \u2022 Ensure that the system can adapt to changes in technology, user needs, and operational requirements.        \n 10 Documentation and Knowledge Sharing:                                                                           \n     \u2022 Maintain thorough documentation of the system\u2019s design, workflows, and user feedback.                       \n     \u2022 Encourage knowledge sharing among team members to foster a culture of continuous improvement.               \n\nBy following these steps, organizations can effectively implement HITL systems that leverage human expertise to    \nenhance machine learning models and decision-making processes.                                                     \n</pre> <pre>\n\ud83d\udc64 Human provided feedback: I need a Langraph example\n\n\n\ud83e\udd16 AI has revised the response.\n\n\ud83d\udc4b HUMAN REVIEW REQUIRED!\n\nOriginal question: What's the best way to implement HITL systems?\n</pre> <pre>AI draft: Implementing Human-in-the-Loop (HITL) systems can be greatly enhanced by using a Langraph example, which \nillustrates how to structure the interaction between humans and machines in a specific context. Below is a revised \nanswer that incorporates a Langraph example to clarify the implementation process:                                 \n\n                      Implementing HITL Systems: A Structured Approach with Langraph Example                       \n\n  1 Define Objectives and Use Cases:                                                                               \n     \u2022 Example: In a medical imaging application, the objective is to improve the accuracy of disease detection in \n       X-ray images. The use case involves radiologists reviewing and annotating images flagged by an AI model.    \n  2 Select Appropriate Technology:                                                                                 \n     \u2022 Choose tools that support image processing and annotation. For instance, use a machine learning framework   \n       like TensorFlow for model training and a user-friendly annotation tool like Labelbox for human input.       \n  3 Design the Workflow:                                                                                           \n     \u2022 Langraph Example: Create a flowchart that outlines the process:                                             \n        \u2022 Step 1: AI model analyzes X-ray images and flags potential anomalies.                                    \n        \u2022 Step 2: Images are sent to a dashboard for radiologists to review.                                       \n        \u2022 Step 3: Radiologists confirm or correct the AI's findings.                                               \n        \u2022 Step 4: Feedback is sent back to the AI model for retraining.                                            \n  4 User Interface and Experience:                                                                                 \n     \u2022 Develop an intuitive dashboard where radiologists can easily view flagged images, provide feedback, and     \n       access relevant patient information. Ensure the interface highlights key areas of concern identified by the \n       AI.                                                                                                         \n  5 Training and Onboarding:                                                                                       \n     \u2022 Conduct training sessions for radiologists on how to use the dashboard, interpret AI suggestions, and       \n       understand the importance of their feedback in improving model accuracy.                                    \n  6 Feedback Mechanisms:                                                                                           \n     \u2022 Implement a feedback loop where radiologists can rate the AI's suggestions and provide comments. This       \n       feedback can be collected through the dashboard and analyzed to identify areas for model improvement.       \n  7 Monitoring and Evaluation:                                                                                     \n     \u2022 Continuously monitor the accuracy of the AI model based on radiologist feedback. Use metrics such as        \n       precision, recall, and F1 score to evaluate the model's performance over time.                              \n  8 Ethical Considerations:                                                                                        \n     \u2022 Address concerns about data privacy by ensuring that patient data is anonymized. Discuss the implications of\n       human judgment in critical medical decisions and maintain transparency about how AI suggestions are used.   \n  9 Scalability and Adaptability:                                                                                  \n     \u2022 Design the system to accommodate more radiologists as the workload increases. Ensure that the model can be  \n       retrained with new data and feedback to adapt to evolving medical standards.                                \n 10 Documentation and Knowledge Sharing:                                                                           \n     \u2022 Maintain comprehensive documentation of the workflow, user feedback, and model performance. Create a shared \n       knowledge base where radiologists can access best practices and lessons learned from the HITL process.      \n\n                                             Langraph Example Summary                                              \n\nIn this example, the Langraph illustrates the interaction between the AI model and human radiologists in a medical \nimaging context. The workflow clearly defines how human input is integrated into the system, emphasizing the       \nimportance of collaboration between AI and human expertise to enhance diagnostic accuracy.                         \n\nBy following this structured approach and utilizing a Langraph example, organizations can effectively implement    \nHITL systems that leverage human expertise to improve machine learning models and decision-making processes.       \n</pre> <pre>\n\ud83d\udc64 Human provided feedback: approve\n\n\n\u2705 Final response ready:\n</pre> <pre>Implementing Human-in-the-Loop (HITL) systems can be greatly enhanced by using a Langraph example, which           \nillustrates how to structure the interaction between humans and machines in a specific context. Below is a revised \nanswer that incorporates a Langraph example to clarify the implementation process:                                 \n\n                      Implementing HITL Systems: A Structured Approach with Langraph Example                       \n\n  1 Define Objectives and Use Cases:                                                                               \n     \u2022 Example: In a medical imaging application, the objective is to improve the accuracy of disease detection in \n       X-ray images. The use case involves radiologists reviewing and annotating images flagged by an AI model.    \n  2 Select Appropriate Technology:                                                                                 \n     \u2022 Choose tools that support image processing and annotation. For instance, use a machine learning framework   \n       like TensorFlow for model training and a user-friendly annotation tool like Labelbox for human input.       \n  3 Design the Workflow:                                                                                           \n     \u2022 Langraph Example: Create a flowchart that outlines the process:                                             \n        \u2022 Step 1: AI model analyzes X-ray images and flags potential anomalies.                                    \n        \u2022 Step 2: Images are sent to a dashboard for radiologists to review.                                       \n        \u2022 Step 3: Radiologists confirm or correct the AI's findings.                                               \n        \u2022 Step 4: Feedback is sent back to the AI model for retraining.                                            \n  4 User Interface and Experience:                                                                                 \n     \u2022 Develop an intuitive dashboard where radiologists can easily view flagged images, provide feedback, and     \n       access relevant patient information. Ensure the interface highlights key areas of concern identified by the \n       AI.                                                                                                         \n  5 Training and Onboarding:                                                                                       \n     \u2022 Conduct training sessions for radiologists on how to use the dashboard, interpret AI suggestions, and       \n       understand the importance of their feedback in improving model accuracy.                                    \n  6 Feedback Mechanisms:                                                                                           \n     \u2022 Implement a feedback loop where radiologists can rate the AI's suggestions and provide comments. This       \n       feedback can be collected through the dashboard and analyzed to identify areas for model improvement.       \n  7 Monitoring and Evaluation:                                                                                     \n     \u2022 Continuously monitor the accuracy of the AI model based on radiologist feedback. Use metrics such as        \n       precision, recall, and F1 score to evaluate the model's performance over time.                              \n  8 Ethical Considerations:                                                                                        \n     \u2022 Address concerns about data privacy by ensuring that patient data is anonymized. Discuss the implications of\n       human judgment in critical medical decisions and maintain transparency about how AI suggestions are used.   \n  9 Scalability and Adaptability:                                                                                  \n     \u2022 Design the system to accommodate more radiologists as the workload increases. Ensure that the model can be  \n       retrained with new data and feedback to adapt to evolving medical standards.                                \n 10 Documentation and Knowledge Sharing:                                                                           \n     \u2022 Maintain comprehensive documentation of the workflow, user feedback, and model performance. Create a shared \n       knowledge base where radiologists can access best practices and lessons learned from the HITL process.      \n\n                                             Langraph Example Summary                                              \n\nIn this example, the Langraph illustrates the interaction between the AI model and human radiologists in a medical \nimaging context. The workflow clearly defines how human input is integrated into the system, emphasizing the       \nimportance of collaboration between AI and human expertise to enhance diagnostic accuracy.                         \n\nBy following this structured approach and utilizing a Langraph example, organizations can effectively implement    \nHITL systems that leverage human expertise to improve machine learning models and decision-making processes.       \n</pre> <pre>\n=== WORKFLOW COMPLETED ===\n</pre> <p>In general, when a graph is interrupted, its state is persisted until the graph is resumed (from the interrupted node).</p> In\u00a0[79]: Copied! <pre># If the graph is interrumpted, the status per thread can be checked with:\n\n#\u00a0workflow.get_state(thread_config).tasks[0].interrupts[0].value\n</pre> # If the graph is interrumpted, the status per thread can be checked with:  #\u00a0workflow.get_state(thread_config).tasks[0].interrupts[0].value Out[79]: <pre>()</pre>"},{"location":"dia3/notebooks/langgraph-interrupt/#human-in-the-loop","title":"Human in the Loop\u00b6","text":""},{"location":"dia3/notebooks/mcp_rag/","title":"Retrieval Augmented Generation (RAG)","text":"In\u00a0[21]: Copied! <pre>from dotenv import load_dotenv\nimport pandas as pd\nimport json\nfrom dotenv import load_dotenv\nimport os \nfrom IPython.display import display, Markdown\nimport pprint\n</pre> from dotenv import load_dotenv import pandas as pd import json from dotenv import load_dotenv import os  from IPython.display import display, Markdown import pprint In\u00a0[22]: Copied! <pre>from langchain.document_loaders.pdf import PyPDFDirectoryLoader \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \nfrom langchain_openai import OpenAIEmbeddings \nfrom langchain.schema import Document \nfrom langchain_chroma import Chroma # This is a Chroma wrapper from Langchain\nfrom langchain_openai import ChatOpenAI # Import OpenAI LLM\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain import hub\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.document_loaders import PyPDFLoader\n\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\nfrom langchain_huggingface import HuggingFaceEmbeddings\n</pre> from langchain.document_loaders.pdf import PyPDFDirectoryLoader  from langchain.text_splitter import RecursiveCharacterTextSplitter  from langchain_openai import OpenAIEmbeddings  from langchain.schema import Document  from langchain_chroma import Chroma # This is a Chroma wrapper from Langchain from langchain_openai import ChatOpenAI # Import OpenAI LLM from langchain_core.vectorstores import InMemoryVectorStore from langchain import hub from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser from langchain.document_loaders import PyPDFLoader  from langchain.retrievers import ContextualCompressionRetriever from langchain.retrievers.document_compressors import CrossEncoderReranker from langchain_community.cross_encoders import HuggingFaceCrossEncoder from langchain_huggingface import HuggingFaceEmbeddings  In\u00a0[3]: Copied! <pre># %pip install pypdf langchain-huggingface sentence-transformers\n</pre> # %pip install pypdf langchain-huggingface sentence-transformers In\u00a0[23]: Copied! <pre># Carga de variables de ambiente desde el archivo .env\nload_dotenv()\n</pre> # Carga de variables de ambiente desde el archivo .env load_dotenv() Out[23]: <pre>True</pre> In\u00a0[24]: Copied! <pre>embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Local model\n</pre> embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Local model In\u00a0[25]: Copied! <pre>llm_model = os.environ[\"OPENAI_MODEL\"]\nprint(llm_model)\nllm = ChatOpenAI(model=llm_model, temperature=0.1)\n</pre> llm_model = os.environ[\"OPENAI_MODEL\"] print(llm_model) llm = ChatOpenAI(model=llm_model, temperature=0.1) <pre>gpt-4o-mini\n</pre> In\u00a0[26]: Copied! <pre>llm.invoke(\"Hello, world!\")  # Test LLM\n</pre> llm.invoke(\"Hello, world!\")  # Test LLM Out[26]: <pre>AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CTxMQ5zsoZjCoyBmnpgmEW5yFn4v5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--13c610c5-fb4a-450a-bc26-47bda60da95e-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})</pre> <p>El patron de prompt t\u00edpico de RAG considera una pregunta y un context</p> In\u00a0[27]: Copied! <pre># Example for a public prompt (https://smith.langchain.com/hub/rlm/rag-prompt)\nrag_prompt = hub.pull(\"rlm/rag-prompt\", include_model=True)\nrag_prompt_template = rag_prompt.messages[0].prompt\nrag_prompt_template.model_dump() # Pydantic object in JSON format\n</pre> # Example for a public prompt (https://smith.langchain.com/hub/rlm/rag-prompt) rag_prompt = hub.pull(\"rlm/rag-prompt\", include_model=True) rag_prompt_template = rag_prompt.messages[0].prompt rag_prompt_template.model_dump() # Pydantic object in JSON format <pre>/Users/adiazpace/opt/anaconda3/envs/ia_desde_cero/lib/python3.11/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n  warnings.warn(\n</pre> Out[27]: <pre>{'name': None,\n 'input_variables': ['context', 'question'],\n 'optional_variables': [],\n 'output_parser': None,\n 'partial_variables': {},\n 'metadata': None,\n 'tags': None,\n 'template': \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\",\n 'template_format': 'f-string',\n 'validate_template': False}</pre> In\u00a0[28]: Copied! <pre>pprint.pprint(rag_prompt_template.template) # El prompt en s\u00ed del RAG\n</pre> pprint.pprint(rag_prompt_template.template) # El prompt en s\u00ed del RAG <pre>('You are an assistant for question-answering tasks. Use the following pieces '\n \"of retrieved context to answer the question. If you don't know the answer, \"\n \"just say that you don't know. Use three sentences maximum and keep the \"\n 'answer concise.\\n'\n 'Question: {question} \\n'\n 'Context: {context} \\n'\n 'Answer:')\n</pre> <p>Si se quisiera, se puede hacer un prompt m\u00e1s custom, siempre respetando la estructura b\u00e1sica y la variable context</p> In\u00a0[9]: Copied! <pre>from langchain_core.prompts import PromptTemplate\nfrom langchain_core.prompts import ChatPromptTemplate\n\nTEMPLATE = \"\"\"You are a helpful AI assistant for question-answering tasks. \\\n    Use the following pieces of retrieved context to answer the question. \\\n    If you don't know the answer, just say that you don't know, don't try to make up an answer\n\n    Question: {question}\n   \n    Context: {context}\n    \n    Answer (in Spanish, if question is formulated in Spanish):\n\"\"\"\n\n# my_rag_prompt = PromptTemplate(\n#     input_variables=[\"context\", \"question\"],\n#     template=TEMPLATE,\n# )\n\nmy_rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)\n</pre> from langchain_core.prompts import PromptTemplate from langchain_core.prompts import ChatPromptTemplate  TEMPLATE = \"\"\"You are a helpful AI assistant for question-answering tasks. \\     Use the following pieces of retrieved context to answer the question. \\     If you don't know the answer, just say that you don't know, don't try to make up an answer      Question: {question}         Context: {context}          Answer (in Spanish, if question is formulated in Spanish): \"\"\"  # my_rag_prompt = PromptTemplate( #     input_variables=[\"context\", \"question\"], #     template=TEMPLATE, # )  my_rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)   In\u00a0[29]: Copied! <pre># We consider a large PDF file\npdf_path = \"./docs/aws-general.pdf\"\n\nloader = PyPDFLoader(pdf_path) # Tool to load and process a PDF file\npdf_documents = loader.load() # Each document corresponds actually to a page\nprint(len(pdf_documents), \"loaded\")\n</pre> # We consider a large PDF file pdf_path = \"./docs/aws-general.pdf\"  loader = PyPDFLoader(pdf_path) # Tool to load and process a PDF file pdf_documents = loader.load() # Each document corresponds actually to a page print(len(pdf_documents), \"loaded\") <pre>3123 loaded\n</pre> In\u00a0[30]: Copied! <pre># Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=100)\n\ntexts = text_splitter.split_documents(pdf_documents)\nprint(len(texts), \"chunks\")\n</pre> # Split documents into chunks text_splitter = RecursiveCharacterTextSplitter(     chunk_size=1000, chunk_overlap=100)  texts = text_splitter.split_documents(pdf_documents) print(len(texts), \"chunks\") <pre>4902 chunks\n</pre> In\u00a0[\u00a0]: Copied! <pre># We use a simple vector store for the chunks\nvectorstore_chroma = Chroma(\n        collection_name=\"aws_collection\",\n        embedding_function=embeddings,\n        persist_directory=\"./aws_chroma_db\" # Optional: specify a directory to persist your data\n    )\nvectorstore_chroma.add_documents(texts)\n\nretriever = vectorstore_chroma.as_retriever(search_kwargs={\"k\": 7})\n</pre> # We use a simple vector store for the chunks vectorstore_chroma = Chroma(         collection_name=\"aws_collection\",         embedding_function=embeddings,         persist_directory=\"./aws_chroma_db\" # Optional: specify a directory to persist your data     ) vectorstore_chroma.add_documents(texts)  retriever = vectorstore_chroma.as_retriever(search_kwargs={\"k\": 7}) In\u00a0[31]: Copied! <pre># Helper function for printing docs\ndef pretty_print_docs(docs):\n    print(\n        f\"\\n{'-' * 100}\\n\".join(\n            [f\"{i + 1}. Document {d.id}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n        )\n    )\n\ntest_query = \"I want to understand the main AWS services related to compute and storage, and how they integrate with each other.\"\n\n# Solamente se est\u00e1 haciendo recuperaci\u00f3n\ncontext_docs =  retriever.invoke(input=test_query)\npretty_print_docs(context_docs)\n</pre> # Helper function for printing docs def pretty_print_docs(docs):     print(         f\"\\n{'-' * 100}\\n\".join(             [f\"{i + 1}. Document {d.id}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]         )     )  test_query = \"I want to understand the main AWS services related to compute and storage, and how they integrate with each other.\"  # Solamente se est\u00e1 haciendo recuperaci\u00f3n context_docs =  retriever.invoke(input=test_query) pretty_print_docs(context_docs) <pre>1. Document c4996ad7-8d6e-4676-8057-ad501418e603:\n\nAWS General Reference Reference guide\nService endpoints\nThe following sections describe the service endpoints for AWS AppCon\ufb01g. AWS AppCon\ufb01g uses\ncontrol plane APIs for setting up and con\ufb01guring AWS AppCon\ufb01g applications, environments, \ncon\ufb01guration pro\ufb01les, and deployment strategies. AWS AppCon\ufb01g uses the AWS AppCon\ufb01g Data \nservice to call data plane APIs for retrieving stored con\ufb01gurations.\nTopics\n\u2022 Control plane endpoints\n\u2022 Data plane endpoints\nControl plane endpoints\nThe following table contains AWS Region-speci\ufb01c endpoints that AWS AppCon\ufb01g supports for \ncontrol plane operations. Control plane operations are used for creating, updating, and managing \ncon\ufb01guration data. For more information, see AWS AppCon\ufb01g operations in the AWS AppCon\ufb01g API \nReference.\nRegion \nName\nRegion Endpoint Protocol\nUS East \n(Ohio)\nus-east-2 appcon\ufb01g.us-east-2.amazonaws.com\nappcon\ufb01g-\ufb01ps.us-east-2.api.aws\nappcon\ufb01g-\ufb01ps.us-east-2.amazonaws.com\nappcon\ufb01g.us-east-2.api.aws\nHTTPS\nHTTPS\nHTTPS\nHTTPS\nUS \nEast (N.\n----------------------------------------------------------------------------------------------------\n2. Document c6fa0469-5c02-4635-a99b-5f42de4ebd6c:\n\nAWS General Reference Reference guide\nAPIs AWS Regions Transactions per second\nEurope (Milan) Region\nEurope (Paris) Region\nEurope (Spain)\nEurope (Stockholm) Region\nIsrael (Tel Aviv) Region\nMiddle East (Bahrain) Region\nSouth America (S\u00e3o Paulo) \nRegion\nPutDataProtectionPolicy All Commercial Regions 1\nMessage Archiving and Replay\nPolicy AWS Regions Standard topics FIFO topics\nAll Commercial \nRegions\nN/A YesArchivePolicy\nAWS GovCloud (US) \nRegions\nN/A Yes\nAll Commercial \nRegions\nN/A YesReplayPolicy\nAWS GovCloud (US) \nRegions\nN/A Yes\nAmazon Simple Queue Service endpoints and quotas\nTo connect programmatically to an AWS service, you use an endpoint. AWS services o\ufb00er the \nfollowing endpoint types in some or all of the AWS Regions that the service supports: IPv4 \nAmazon SQS Version 1.0 2844\n----------------------------------------------------------------------------------------------------\n3. Document d42544db-6646-4cc3-85be-98ab6a975f0f:\n\nAWS General Reference Reference guide\nName Default Adjustabl \ne\nDescription\nSubnets per subnet group Each supported \nRegion: 60\nYes The maximum number \nof subnets allowed per \nsubnet group.\nTask count Each supported \nRegion: 600\nYes The maximum number \nof tasks allowed in this \naccount in the current \nRegion.\nThe amount of collected data in DMS \nFleet Advisor\nEach supported \nRegion: 10 \nGigabytes\nNo The amount of data \nthat can be collected by \nall DMS Fleet Advisor \ncollectors\nTotal storage Each supported \nRegion: 30,000 \nGigabytes\nYes The maximum total \nstorage (in GB) for all \nreplication instances \nadded together.\nAWS Deadline Cloud endpoints and quotas\nTo connect programmatically to an AWS service, you use an endpoint. AWS services o\ufb00er the \nfollowing endpoint types in some or all of the AWS Regions that the service supports: IPv4 \nendpoints, dual-stack endpoints, and FIPS endpoints. Some services provide global endpoints. For \nmore information, see AWS service endpoints.\n----------------------------------------------------------------------------------------------------\n4. Document 911c7f5f-49ad-468c-9a90-8c13c883013a:\n\nAWS General Reference Reference guide\nAWS service endpoints\nTo connect programmatically to an AWS service, you use an endpoint. An endpoint is the URL of \nthe entry point for an AWS web service. The AWS SDKs and the AWS Command Line Interface (AWS \nCLI) automatically use the default endpoint for each service in an AWS Region. But you can specify \nan alternate endpoint for your API requests.\nIf a service supports Regions, the resources in each Region are independent of similar resources in \nother Regions. For example, you can create an Amazon EC2 instance or an Amazon SQS queue in \none Region. When you do, the instance or queue is independent of instances or queues in all other \nRegions.\nContents\n\u2022 Regional endpoints\n\u2022 Global endpoints\n\u2022 View the service endpoints\n\u2022 FIPS endpoints\n\u2022 Dual stack endpoints\n\u2022 Learn more\nRegional endpoints\nMost Amazon Web Services o\ufb00er a Regional endpoint that you can use to make your requests. In\n----------------------------------------------------------------------------------------------------\n5. Document 7906d49c-fb51-4abe-8e29-18792aa89b58:\n\nAWS General Reference Reference guide\nAmazon Simple Storage Service endpoints and quotas\nTo connect programmatically to an AWS service, you use an endpoint. AWS services o\ufb00er the \nfollowing endpoint types in some or all of the AWS Regions that the service supports: IPv4 \nendpoints, dual-stack endpoints, and FIPS endpoints. Some services provide global endpoints. For \nmore information, see AWS service endpoints.\nService quotas, also referred to as limits, are the maximum number of service resources or \noperations for your AWS account. For more information, see AWS service quotas.\nThe following are the service endpoints and service quotas for this service.\nService endpoints\nAmazon S3 endpoints\nWhen you use the REST API to send requests to the endpoints shown in the following table, you \ncan use the virtual-hosted style and path-style methods. For more information, see Virtual hosting \nof buckets.\nNote\nSome Regions support legacy endpoints. For information about Amazon S3 backward\n----------------------------------------------------------------------------------------------------\n6. Document 0e2d1bd9-5e83-4f0c-a19f-81c3b8a4719a:\n\nAWS General Reference Reference guide\nAWS Marketplace endpoints and quotas\nAWS Marketplace is a curated digital catalog that makes it easy for customers to \ufb01nd, buy, deploy, \nand manage third-party software and services that customers need to build solutions and run their \nbusinesses.\nTo connect programmatically to an AWS service, you use an endpoint. AWS services o\ufb00er the \nfollowing endpoint types in some or all of the AWS Regions that the service supports: IPv4 \nendpoints, dual-stack endpoints, and FIPS endpoints. Some services provide global endpoints. For \nmore information, see AWS service endpoints.\nService quotas, also referred to as limits, are the maximum number of service resources or \noperations for your AWS account. For more information, see AWS service quotas.\nThe following are the service endpoints and service quotas for this service.\nService endpoints\nThe AWS Marketplace website is available globally. The AWS Marketplace console is available in the\n----------------------------------------------------------------------------------------------------\n7. Document 0e52cc51-6869-49fe-8f55-ea29636f1729:\n\nAWS General Reference Reference guide\nName Default Adjustabl \ne\nDescription\nTotal concurrent machine learning task \nruns for transforms per account\nEach supported \nRegion: 30\nYes The total number of \nconcurrent machine \nlearning transform \ntask runs for machine \nlearning transforms for \nthis account.\nFor more information, see AWS Glue in the AWS GovCloud (US) User Guide.\nAWS Glue DataBrew endpoints and quotas\nTo connect programmatically to an AWS service, you use an endpoint. AWS services o\ufb00er the \nfollowing endpoint types in some or all of the AWS Regions that the service supports: IPv4 \nendpoints, dual-stack endpoints, and FIPS endpoints. Some services provide global endpoints. For \nmore information, see AWS service endpoints.\nService quotas, also referred to as limits, are the maximum number of service resources or \noperations for your AWS account. For more information, see AWS service quotas.\nThe following are the service endpoints and service quotas for this service.\n</pre> In\u00a0[32]: Copied! <pre># Conexi\u00f3n entre el retriever y el LLM usando el prompt de RAG\nrag_chain = {\"context\": retriever,  \"question\": RunnablePassthrough()} | rag_prompt | llm | StrOutputParser()\n\n#\u00a0query = \"I want to understand the main AWS services related to compute and storage, and how they integrate with each other.\"\n#\u00a0query = \"I want to know more about the endpoints of CloudWatch and how to use them.\"\nquery = \"Quiero saber sobre los endpoints de CloudWatch y c\u00f3mo utilizarlos.\"\nresult = rag_chain.invoke(input=test_query)\n#\u00a0pprint.pprint(result)\ndisplay(Markdown(result))\n</pre> # Conexi\u00f3n entre el retriever y el LLM usando el prompt de RAG rag_chain = {\"context\": retriever,  \"question\": RunnablePassthrough()} | rag_prompt | llm | StrOutputParser()  #\u00a0query = \"I want to understand the main AWS services related to compute and storage, and how they integrate with each other.\" #\u00a0query = \"I want to know more about the endpoints of CloudWatch and how to use them.\" query = \"Quiero saber sobre los endpoints de CloudWatch y c\u00f3mo utilizarlos.\" result = rag_chain.invoke(input=test_query) #\u00a0pprint.pprint(result) display(Markdown(result)) <p>The main AWS services related to compute include Amazon EC2 (Elastic Compute Cloud) for scalable virtual servers and AWS Lambda for serverless computing. For storage, Amazon S3 (Simple Storage Service) provides scalable object storage, while Amazon EBS (Elastic Block Store) offers block storage for EC2 instances. These services integrate seamlessly, allowing EC2 instances to use EBS for persistent storage and S3 for data storage and retrieval.</p> <p>Es un paso adicional para intentar maximizar la chance de recuperar chunks relevantes a la pregunta, que pueden haber quedado m\u00e1s abajo en el ranking original del retriever. En este ejemplo se usa una estrategia de cross-encoding (de HuggingFace), pero existen otras estrategias e incluso mediante un prompt al LLM.</p> In\u00a0[33]: Copied! <pre># Initialize the cross encoder\nmodel = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n\n# Create a reranker compressor\ncompressor = CrossEncoderReranker(model=model, top_n=3)\n\n# Wrap your base retriever with the compression retriever\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=retriever\n)\n</pre> # Initialize the cross encoder model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  # Create a reranker compressor compressor = CrossEncoderReranker(model=model, top_n=3)  # Wrap your base retriever with the compression retriever compression_retriever = ContextualCompressionRetriever(     base_compressor=compressor,     base_retriever=retriever ) In\u00a0[34]: Copied! <pre># Use the compression retriever\ncompressed_docs = compression_retriever.invoke(test_query)\npretty_print_docs(compressed_docs)\n</pre> # Use the compression retriever compressed_docs = compression_retriever.invoke(test_query) pretty_print_docs(compressed_docs) <pre>1. Document 7906d49c-fb51-4abe-8e29-18792aa89b58:\n\nAWS General Reference Reference guide\nAmazon Simple Storage Service endpoints and quotas\nTo connect programmatically to an AWS service, you use an endpoint. AWS services o\ufb00er the \nfollowing endpoint types in some or all of the AWS Regions that the service supports: IPv4 \nendpoints, dual-stack endpoints, and FIPS endpoints. Some services provide global endpoints. For \nmore information, see AWS service endpoints.\nService quotas, also referred to as limits, are the maximum number of service resources or \noperations for your AWS account. For more information, see AWS service quotas.\nThe following are the service endpoints and service quotas for this service.\nService endpoints\nAmazon S3 endpoints\nWhen you use the REST API to send requests to the endpoints shown in the following table, you \ncan use the virtual-hosted style and path-style methods. For more information, see Virtual hosting \nof buckets.\nNote\nSome Regions support legacy endpoints. For information about Amazon S3 backward\n----------------------------------------------------------------------------------------------------\n2. Document d42544db-6646-4cc3-85be-98ab6a975f0f:\n\nAWS General Reference Reference guide\nName Default Adjustabl \ne\nDescription\nSubnets per subnet group Each supported \nRegion: 60\nYes The maximum number \nof subnets allowed per \nsubnet group.\nTask count Each supported \nRegion: 600\nYes The maximum number \nof tasks allowed in this \naccount in the current \nRegion.\nThe amount of collected data in DMS \nFleet Advisor\nEach supported \nRegion: 10 \nGigabytes\nNo The amount of data \nthat can be collected by \nall DMS Fleet Advisor \ncollectors\nTotal storage Each supported \nRegion: 30,000 \nGigabytes\nYes The maximum total \nstorage (in GB) for all \nreplication instances \nadded together.\nAWS Deadline Cloud endpoints and quotas\nTo connect programmatically to an AWS service, you use an endpoint. AWS services o\ufb00er the \nfollowing endpoint types in some or all of the AWS Regions that the service supports: IPv4 \nendpoints, dual-stack endpoints, and FIPS endpoints. Some services provide global endpoints. For \nmore information, see AWS service endpoints.\n----------------------------------------------------------------------------------------------------\n3. Document 911c7f5f-49ad-468c-9a90-8c13c883013a:\n\nAWS General Reference Reference guide\nAWS service endpoints\nTo connect programmatically to an AWS service, you use an endpoint. An endpoint is the URL of \nthe entry point for an AWS web service. The AWS SDKs and the AWS Command Line Interface (AWS \nCLI) automatically use the default endpoint for each service in an AWS Region. But you can specify \nan alternate endpoint for your API requests.\nIf a service supports Regions, the resources in each Region are independent of similar resources in \nother Regions. For example, you can create an Amazon EC2 instance or an Amazon SQS queue in \none Region. When you do, the instance or queue is independent of instances or queues in all other \nRegions.\nContents\n\u2022 Regional endpoints\n\u2022 Global endpoints\n\u2022 View the service endpoints\n\u2022 FIPS endpoints\n\u2022 Dual stack endpoints\n\u2022 Learn more\nRegional endpoints\nMost Amazon Web Services o\ufb00er a Regional endpoint that you can use to make your requests. In\n</pre> In\u00a0[35]: Copied! <pre>rag_chain1 = (\n    {\"context\": compression_retriever,  \"question\": RunnablePassthrough()} \n    | rag_prompt \n    | llm\n    | StrOutputParser()\n)\n\nresult = rag_chain.invoke(test_query)\n#\u00a0pprint.pprint(result)\ndisplay(Markdown(result))\n</pre> rag_chain1 = (     {\"context\": compression_retriever,  \"question\": RunnablePassthrough()}      | rag_prompt      | llm     | StrOutputParser() )  result = rag_chain.invoke(test_query) #\u00a0pprint.pprint(result) display(Markdown(result)) <p>The main AWS services related to compute include Amazon EC2 (Elastic Compute Cloud) for scalable computing capacity and AWS Lambda for serverless computing. For storage, Amazon S3 (Simple Storage Service) provides scalable object storage, while Amazon EBS (Elastic Block Store) offers block storage for EC2 instances. These services integrate seamlessly, allowing EC2 instances to access data stored in S3 or EBS, facilitating efficient data processing and storage management.</p>"},{"location":"dia3/notebooks/mcp_rag/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)\u00b6","text":""},{"location":"dia3/notebooks/mcp_rag/#ingestion","title":"Ingestion\u00b6","text":""},{"location":"dia3/notebooks/mcp_rag/#re-ranking","title":"Re-Ranking\u00b6","text":""},{"location":"dia3/notebooks/reflection/","title":"Reflection (Revise)","text":"In\u00a0[\u00a0]: Copied! <pre>from dotenv import load_dotenv\nimport pandas as pd\nfrom pathlib import Path\nimport json\nfrom dotenv import load_dotenv\nimport os \nimport pprint\nfrom IPython.display import Image, display\nfrom typing import TypedDict, Annotated, Literal, Dict, Any\n</pre> from dotenv import load_dotenv import pandas as pd from pathlib import Path import json from dotenv import load_dotenv import os  import pprint from IPython.display import Image, display from typing import TypedDict, Annotated, Literal, Dict, Any <p>Import the necessary classes from Langchain and Langgraph</p> In\u00a0[15]: Copied! <pre>from langchain.chat_models import init_chat_model\nfrom langchain_core.messages import AIMessage, BaseMessage, HumanMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom typing import Annotated, List, Sequence\nfrom langgraph.graph import END, StateGraph, START\nfrom langgraph.graph.message import add_messages\nfrom langgraph.checkpoint.memory import InMemorySaver\n</pre> from langchain.chat_models import init_chat_model from langchain_core.messages import AIMessage, BaseMessage, HumanMessage from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from typing import Annotated, List, Sequence from langgraph.graph import END, StateGraph, START from langgraph.graph.message import add_messages from langgraph.checkpoint.memory import InMemorySaver In\u00a0[2]: Copied! <pre># Load environment variables from .env file\nload_dotenv()\n</pre> # Load environment variables from .env file load_dotenv() Out[2]: <pre>True</pre> <p>We define an LLM model to use in a zero-shot mode to answer user questions</p> In\u00a0[5]: Copied! <pre>llm_model = \"openai:\"+os.getenv(\"OPENAI_MODEL\")\nprint(llm_model)\nllm = init_chat_model(llm_model, temperature=0)\n</pre> llm_model = \"openai:\"+os.getenv(\"OPENAI_MODEL\") print(llm_model) llm = init_chat_model(llm_model, temperature=0) <pre>openai:gpt-4o-mini\n</pre> In\u00a0[\u00a0]: Copied! <pre>prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are an essay assistant tasked with writing excellent 5-paragraph essays.\"\n            \" Generate the best essay possible for the user's request.\"\n            \" If the user provides critique, respond with a revised version of your previous attempts.\",\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\ngenerate = prompt | llm # Chain for generating an essay version\n</pre> prompt = ChatPromptTemplate.from_messages(     [         (             \"system\",             \"You are an essay assistant tasked with writing excellent 5-paragraph essays.\"             \" Generate the best essay possible for the user's request.\"             \" If the user provides critique, respond with a revised version of your previous attempts.\",         ),         MessagesPlaceholder(variable_name=\"messages\"),     ] )  generate = prompt | llm # Chain for generating an essay version In\u00a0[9]: Copied! <pre>from rich.console import Console\nfrom rich.markdown import Markdown\n\n# This console is only for rendering Markup content nicely\nconsole = Console()\n</pre> from rich.console import Console from rich.markdown import Markdown  # This console is only for rendering Markup content nicely console = Console() In\u00a0[11]: Copied! <pre>hint = \"Write an essay on why the little prince is relevant in modern childhood\"\n\nessay = \"\"\nrequest = HumanMessage(\n    content=hint\n)\nfor chunk in generate.stream({\"messages\": [request]}):\n    print(chunk.content, end=\"\")\n    essay += chunk.content\n\nconsole.print(Markdown(essay))\n</pre> hint = \"Write an essay on why the little prince is relevant in modern childhood\"  essay = \"\" request = HumanMessage(     content=hint ) for chunk in generate.stream({\"messages\": [request]}):     print(chunk.content, end=\"\")     essay += chunk.content  console.print(Markdown(essay)) <pre>**The Relevance of \"The Little Prince\" in Modern Childhood**\n\nAntoine de Saint-Exup\u00e9ry's \"The Little Prince,\" first published in 1943, has transcended generations, captivating readers with its profound themes and whimsical storytelling. In an age dominated by technology and rapid change, the lessons embedded in this timeless tale remain strikingly relevant to modern childhood. The story's exploration of imagination, the importance of relationships, and the critique of adult values resonate deeply with children today, making it a vital part of their literary landscape.\n\nOne of the most significant aspects of \"The Little Prince\" is its celebration of imagination and creativity. In a world increasingly focused on standardized education and digital distractions, the book encourages children to embrace their imaginative capabilities. The Little Prince himself embodies the spirit of curiosity and wonder, exploring various planets and meeting unique characters that challenge conventional thinking. This narrative invites children to dream, question, and explore their own worlds, fostering a sense of creativity that is essential for personal development. In a time when children are often pressured to conform to societal expectations, the story serves as a reminder of the importance of nurturing one's imagination.\n\nMoreover, \"The Little Prince\" emphasizes the value of relationships and emotional connections. The bond between the Little Prince and his rose illustrates the significance of love, care, and responsibility. In an era where social media often replaces face-to-face interactions, the book's message about the importance of genuine relationships is more pertinent than ever. Children today face challenges in forming meaningful connections, often feeling isolated despite being constantly connected online. The Little Prince teaches that true relationships require effort and understanding, encouraging children to cultivate empathy and compassion in their interactions with others.\n\nAdditionally, the story critiques adult values, highlighting the absurdities of the adult world through the eyes of a child. The Little Prince encounters various characters, such as the businessman and the geographer, who embody the often misguided priorities of adulthood\u2014materialism, power, and superficiality. This critique resonates with modern children, who are increasingly aware of societal pressures and the complexities of adult life. By presenting these themes through a child's perspective, Saint-Exup\u00e9ry invites young readers to question the values they observe in the world around them, fostering critical thinking and self-reflection.\n\nIn conclusion, \"The Little Prince\" remains a relevant and essential read for modern childhood. Its celebration of imagination, emphasis on meaningful relationships, and critique of adult values provide valuable lessons for children navigating today's complex world. As they encounter the whimsical yet profound journey of the Little Prince, young readers are encouraged to embrace their creativity, cultivate empathy, and question societal norms. In a rapidly changing landscape, the timeless wisdom of \"The Little Prince\" continues to inspire and guide children, making it a cherished classic for generations to come.</pre> <pre>The Relevance of \"The Little Prince\" in Modern Childhood                                                           \n\nAntoine de Saint-Exup\u00e9ry's \"The Little Prince,\" first published in 1943, has transcended generations, captivating  \nreaders with its profound themes and whimsical storytelling. In an age dominated by technology and rapid change,   \nthe lessons embedded in this timeless tale remain strikingly relevant to modern childhood. The story's exploration \nof imagination, the importance of relationships, and the critique of adult values resonate deeply with children    \ntoday, making it a vital part of their literary landscape.                                                         \n\nOne of the most significant aspects of \"The Little Prince\" is its celebration of imagination and creativity. In a  \nworld increasingly focused on standardized education and digital distractions, the book encourages children to     \nembrace their imaginative capabilities. The Little Prince himself embodies the spirit of curiosity and wonder,     \nexploring various planets and meeting unique characters that challenge conventional thinking. This narrative       \ninvites children to dream, question, and explore their own worlds, fostering a sense of creativity that is         \nessential for personal development. In a time when children are often pressured to conform to societal             \nexpectations, the story serves as a reminder of the importance of nurturing one's imagination.                     \n\nMoreover, \"The Little Prince\" emphasizes the value of relationships and emotional connections. The bond between the\nLittle Prince and his rose illustrates the significance of love, care, and responsibility. In an era where social  \nmedia often replaces face-to-face interactions, the book's message about the importance of genuine relationships is\nmore pertinent than ever. Children today face challenges in forming meaningful connections, often feeling isolated \ndespite being constantly connected online. The Little Prince teaches that true relationships require effort and    \nunderstanding, encouraging children to cultivate empathy and compassion in their interactions with others.         \n\nAdditionally, the story critiques adult values, highlighting the absurdities of the adult world through the eyes of\na child. The Little Prince encounters various characters, such as the businessman and the geographer, who embody   \nthe often misguided priorities of adulthood\u2014materialism, power, and superficiality. This critique resonates with   \nmodern children, who are increasingly aware of societal pressures and the complexities of adult life. By presenting\nthese themes through a child's perspective, Saint-Exup\u00e9ry invites young readers to question the values they observe\nin the world around them, fostering critical thinking and self-reflection.                                         \n\nIn conclusion, \"The Little Prince\" remains a relevant and essential read for modern childhood. Its celebration of  \nimagination, emphasis on meaningful relationships, and critique of adult values provide valuable lessons for       \nchildren navigating today's complex world. As they encounter the whimsical yet profound journey of the Little      \nPrince, young readers are encouraged to embrace their creativity, cultivate empathy, and question societal norms.  \nIn a rapidly changing landscape, the timeless wisdom of \"The Little Prince\" continues to inspire and guide         \nchildren, making it a cherished classic for generations to come.                                                   \n</pre> In\u00a0[\u00a0]: Copied! <pre>reflection_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a teacher grading an essay submission. Generate critique and recommendations for the user's submission.\"\n            \" Provide detailed recommendations, including requests for length, depth, style, etc.\",\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\nreflect = reflection_prompt | llm # Chain for reflecting on the essay\n</pre> reflection_prompt = ChatPromptTemplate.from_messages(     [         (             \"system\",             \"You are a teacher grading an essay submission. Generate critique and recommendations for the user's submission.\"             \" Provide detailed recommendations, including requests for length, depth, style, etc.\",         ),         MessagesPlaceholder(variable_name=\"messages\"),     ] ) reflect = reflection_prompt | llm # Chain for reflecting on the essay In\u00a0[13]: Copied! <pre>reflection = \"\"\nfor chunk in reflect.stream({\"messages\": [request, HumanMessage(content=essay)]}):\n    print(chunk.content, end=\"\")\n    reflection += chunk.content\n\nconsole.print(Markdown(reflection))\n</pre> reflection = \"\" for chunk in reflect.stream({\"messages\": [request, HumanMessage(content=essay)]}):     print(chunk.content, end=\"\")     reflection += chunk.content  console.print(Markdown(reflection)) <pre>**Critique and Recommendations for Your Essay on \"The Little Prince\"**\n\nYour essay on the relevance of \"The Little Prince\" in modern childhood presents a thoughtful analysis of the themes within the book and their significance today. You have effectively highlighted key aspects such as imagination, relationships, and the critique of adult values. However, there are several areas where you could enhance your essay to make it more compelling and comprehensive. Here are my detailed recommendations:\n\n### Structure and Length\n1. **Introduction**: Your introduction sets the stage well, but consider expanding it to include a brief overview of the plot or main characters. This will provide context for readers who may not be familiar with the book. Aim for 4-5 sentences in the introduction.\n\n2. **Body Paragraphs**: Each of your main points is well-articulated, but they could benefit from more depth. For each theme you discuss, consider adding:\n   - **Examples**: Include specific examples or quotes from the text to support your claims. This will strengthen your arguments and provide concrete evidence of the themes you are discussing.\n   - **Personal Reflection**: Share how these themes might resonate with children today. Perhaps include anecdotes or observations about modern childhood experiences that align with the lessons from the book.\n\n3. **Conclusion**: Your conclusion effectively summarizes your main points, but it could be more impactful. Consider ending with a call to action or a thought-provoking question that encourages readers to reflect on the importance of these themes in their own lives. Aim for 3-4 sentences.\n\n### Depth and Analysis\n- **Expand on Themes**: While you touch on imagination, relationships, and adult values, consider delving deeper into how these themes manifest in the lives of children today. For instance, how does the pressure of social media impact their ability to form genuine relationships? How does the emphasis on standardized testing stifle creativity?\n  \n- **Cultural Relevance**: Discuss how \"The Little Prince\" has been adapted or referenced in modern media (films, art, etc.) and how these adaptations continue to resonate with children. This could provide a contemporary lens through which to view the book's relevance.\n\n### Style and Tone\n- **Engaging Language**: While your writing is clear, consider incorporating more vivid language and imagery to capture the whimsical nature of the story. This will help to engage readers and evoke the charm of the original text.\n\n- **Variety in Sentence Structure**: To enhance the flow of your essay, vary your sentence structure. Mix shorter, impactful sentences with longer, more complex ones to create a more dynamic reading experience.\n\n### Additional Considerations\n- **Length**: Aim for a total word count of around 800-1000 words. This will allow you to explore your points in greater detail without overwhelming the reader.\n\n- **Proofreading**: Ensure that you proofread your essay for grammatical errors and clarity. A polished piece will enhance your credibility as a writer.\n\n### Final Thoughts\nOverall, your essay presents a solid foundation for discussing the relevance of \"The Little Prince\" in modern childhood. By expanding on your points, incorporating specific examples, and enhancing your writing style, you can create a more engaging and insightful analysis. I look forward to seeing how you develop these ideas further!</pre> <pre>Critique and Recommendations for Your Essay on \"The Little Prince\"                                                 \n\nYour essay on the relevance of \"The Little Prince\" in modern childhood presents a thoughtful analysis of the themes\nwithin the book and their significance today. You have effectively highlighted key aspects such as imagination,    \nrelationships, and the critique of adult values. However, there are several areas where you could enhance your     \nessay to make it more compelling and comprehensive. Here are my detailed recommendations:                          \n\n                                               Structure and Length                                                \n\n 1 Introduction: Your introduction sets the stage well, but consider expanding it to include a brief overview of   \n   the plot or main characters. This will provide context for readers who may not be familiar with the book. Aim   \n   for 4-5 sentences in the introduction.                                                                          \n 2 Body Paragraphs: Each of your main points is well-articulated, but they could benefit from more depth. For each \n   theme you discuss, consider adding:                                                                             \n    \u2022 Examples: Include specific examples or quotes from the text to support your claims. This will strengthen your\n      arguments and provide concrete evidence of the themes you are discussing.                                    \n    \u2022 Personal Reflection: Share how these themes might resonate with children today. Perhaps include anecdotes or \n      observations about modern childhood experiences that align with the lessons from the book.                   \n 3 Conclusion: Your conclusion effectively summarizes your main points, but it could be more impactful. Consider   \n   ending with a call to action or a thought-provoking question that encourages readers to reflect on the          \n   importance of these themes in their own lives. Aim for 3-4 sentences.                                           \n\n                                                Depth and Analysis                                                 \n\n \u2022 Expand on Themes: While you touch on imagination, relationships, and adult values, consider delving deeper into \n   how these themes manifest in the lives of children today. For instance, how does the pressure of social media   \n   impact their ability to form genuine relationships? How does the emphasis on standardized testing stifle        \n   creativity?                                                                                                     \n \u2022 Cultural Relevance: Discuss how \"The Little Prince\" has been adapted or referenced in modern media (films, art, \n   etc.) and how these adaptations continue to resonate with children. This could provide a contemporary lens      \n   through which to view the book's relevance.                                                                     \n\n                                                  Style and Tone                                                   \n\n \u2022 Engaging Language: While your writing is clear, consider incorporating more vivid language and imagery to       \n   capture the whimsical nature of the story. This will help to engage readers and evoke the charm of the original \n   text.                                                                                                           \n \u2022 Variety in Sentence Structure: To enhance the flow of your essay, vary your sentence structure. Mix shorter,    \n   impactful sentences with longer, more complex ones to create a more dynamic reading experience.                 \n\n                                             Additional Considerations                                             \n\n \u2022 Length: Aim for a total word count of around 800-1000 words. This will allow you to explore your points in      \n   greater detail without overwhelming the reader.                                                                 \n \u2022 Proofreading: Ensure that you proofread your essay for grammatical errors and clarity. A polished piece will    \n   enhance your credibility as a writer.                                                                           \n\n                                                  Final Thoughts                                                   \n\nOverall, your essay presents a solid foundation for discussing the relevance of \"The Little Prince\" in modern      \nchildhood. By expanding on your points, incorporating specific examples, and enhancing your writing style, you can \ncreate a more engaging and insightful analysis. I look forward to seeing how you develop these ideas further!      \n</pre> In\u00a0[14]: Copied! <pre>new_essay = \"\"\nfor chunk in generate.stream(\n    {\"messages\": [request, AIMessage(content=essay), HumanMessage(content=reflection)]}\n):\n    print(chunk.content, end=\"\")\n    new_essay += chunk.content\n\nconsole.print(Markdown(new_essay))\n</pre> new_essay = \"\" for chunk in generate.stream(     {\"messages\": [request, AIMessage(content=essay), HumanMessage(content=reflection)]} ):     print(chunk.content, end=\"\")     new_essay += chunk.content  console.print(Markdown(new_essay)) <pre>**The Relevance of \"The Little Prince\" in Modern Childhood**\n\nAntoine de Saint-Exup\u00e9ry's \"The Little Prince,\" first published in 1943, has captivated readers for generations with its profound themes and whimsical storytelling. The narrative follows a young prince who travels from planet to planet, encountering various inhabitants and learning valuable life lessons along the way. In an age dominated by technology and rapid change, the lessons embedded in this timeless tale remain strikingly relevant to modern childhood. The story's exploration of imagination, the importance of relationships, and the critique of adult values resonate deeply with children today, making it a vital part of their literary landscape.\n\nOne of the most significant aspects of \"The Little Prince\" is its celebration of imagination and creativity. In a world increasingly focused on standardized education and digital distractions, the book encourages children to embrace their imaginative capabilities. The Little Prince himself embodies the spirit of curiosity and wonder, exploring various planets and meeting unique characters that challenge conventional thinking. For instance, his encounter with the fox teaches him that \"what is essential is invisible to the eye,\" emphasizing the importance of seeing beyond the surface. This narrative invites children to dream, question, and explore their own worlds, fostering a sense of creativity that is essential for personal development. In a time when children are often pressured to conform to societal expectations, the story serves as a reminder of the importance of nurturing one's imagination, which is crucial for problem-solving and innovation in their future endeavors.\n\nMoreover, \"The Little Prince\" emphasizes the value of relationships and emotional connections. The bond between the Little Prince and his rose illustrates the significance of love, care, and responsibility. The Little Prince learns that his rose is unique and special because of the time and effort he has invested in her. In an era where social media often replaces face-to-face interactions, the book's message about the importance of genuine relationships is more pertinent than ever. Children today face challenges in forming meaningful connections, often feeling isolated despite being constantly connected online. The Little Prince teaches that true relationships require effort and understanding, encouraging children to cultivate empathy and compassion in their interactions with others. For example, the Little Prince's friendship with the fox highlights the beauty of forming bonds that enrich our lives, a lesson that resonates with children navigating the complexities of modern friendships.\n\nAdditionally, the story critiques adult values, highlighting the absurdities of the adult world through the eyes of a child. The Little Prince encounters various characters, such as the businessman and the geographer, who embody the often misguided priorities of adulthood\u2014materialism, power, and superficiality. This critique resonates with modern children, who are increasingly aware of societal pressures and the complexities of adult life. The businessman, obsessed with counting stars for profit, serves as a cautionary tale about losing sight of what truly matters. By presenting these themes through a child's perspective, Saint-Exup\u00e9ry invites young readers to question the values they observe in the world around them, fostering critical thinking and self-reflection. In a society that often prioritizes success and wealth, the Little Prince's journey encourages children to seek deeper meanings and values in their own lives.\n\nIn conclusion, \"The Little Prince\" remains a relevant and essential read for modern childhood. Its celebration of imagination, emphasis on meaningful relationships, and critique of adult values provide valuable lessons for children navigating today's complex world. As they encounter the whimsical yet profound journey of the Little Prince, young readers are encouraged to embrace their creativity, cultivate empathy, and question societal norms. The story's enduring appeal is evident in its adaptations across various media, from animated films to theatrical productions, which continue to introduce its timeless wisdom to new generations. In a rapidly changing landscape, the lessons of \"The Little Prince\" inspire and guide children, making it a cherished classic that will resonate for years to come. As we reflect on the importance of these themes, we must ask ourselves: how can we, as adults, foster the same sense of wonder and connection in the lives of the children around us?</pre> <pre>The Relevance of \"The Little Prince\" in Modern Childhood                                                           \n\nAntoine de Saint-Exup\u00e9ry's \"The Little Prince,\" first published in 1943, has captivated readers for generations    \nwith its profound themes and whimsical storytelling. The narrative follows a young prince who travels from planet  \nto planet, encountering various inhabitants and learning valuable life lessons along the way. In an age dominated  \nby technology and rapid change, the lessons embedded in this timeless tale remain strikingly relevant to modern    \nchildhood. The story's exploration of imagination, the importance of relationships, and the critique of adult      \nvalues resonate deeply with children today, making it a vital part of their literary landscape.                    \n\nOne of the most significant aspects of \"The Little Prince\" is its celebration of imagination and creativity. In a  \nworld increasingly focused on standardized education and digital distractions, the book encourages children to     \nembrace their imaginative capabilities. The Little Prince himself embodies the spirit of curiosity and wonder,     \nexploring various planets and meeting unique characters that challenge conventional thinking. For instance, his    \nencounter with the fox teaches him that \"what is essential is invisible to the eye,\" emphasizing the importance of \nseeing beyond the surface. This narrative invites children to dream, question, and explore their own worlds,       \nfostering a sense of creativity that is essential for personal development. In a time when children are often      \npressured to conform to societal expectations, the story serves as a reminder of the importance of nurturing one's \nimagination, which is crucial for problem-solving and innovation in their future endeavors.                        \n\nMoreover, \"The Little Prince\" emphasizes the value of relationships and emotional connections. The bond between the\nLittle Prince and his rose illustrates the significance of love, care, and responsibility. The Little Prince learns\nthat his rose is unique and special because of the time and effort he has invested in her. In an era where social  \nmedia often replaces face-to-face interactions, the book's message about the importance of genuine relationships is\nmore pertinent than ever. Children today face challenges in forming meaningful connections, often feeling isolated \ndespite being constantly connected online. The Little Prince teaches that true relationships require effort and    \nunderstanding, encouraging children to cultivate empathy and compassion in their interactions with others. For     \nexample, the Little Prince's friendship with the fox highlights the beauty of forming bonds that enrich our lives, \na lesson that resonates with children navigating the complexities of modern friendships.                           \n\nAdditionally, the story critiques adult values, highlighting the absurdities of the adult world through the eyes of\na child. The Little Prince encounters various characters, such as the businessman and the geographer, who embody   \nthe often misguided priorities of adulthood\u2014materialism, power, and superficiality. This critique resonates with   \nmodern children, who are increasingly aware of societal pressures and the complexities of adult life. The          \nbusinessman, obsessed with counting stars for profit, serves as a cautionary tale about losing sight of what truly \nmatters. By presenting these themes through a child's perspective, Saint-Exup\u00e9ry invites young readers to question \nthe values they observe in the world around them, fostering critical thinking and self-reflection. In a society    \nthat often prioritizes success and wealth, the Little Prince's journey encourages children to seek deeper meanings \nand values in their own lives.                                                                                     \n\nIn conclusion, \"The Little Prince\" remains a relevant and essential read for modern childhood. Its celebration of  \nimagination, emphasis on meaningful relationships, and critique of adult values provide valuable lessons for       \nchildren navigating today's complex world. As they encounter the whimsical yet profound journey of the Little      \nPrince, young readers are encouraged to embrace their creativity, cultivate empathy, and question societal norms.  \nThe story's enduring appeal is evident in its adaptations across various media, from animated films to theatrical  \nproductions, which continue to introduce its timeless wisdom to new generations. In a rapidly changing landscape,  \nthe lessons of \"The Little Prince\" inspire and guide children, making it a cherished classic that will resonate for\nyears to come. As we reflect on the importance of these themes, we must ask ourselves: how can we, as adults,      \nfoster the same sense of wonder and connection in the lives of the children around us?                             \n</pre> In\u00a0[28]: Copied! <pre>class State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\ndef generation_node(state: State) -&gt; State:\n    return {\"messages\": [generate.invoke(state[\"messages\"])]}\n\n\ndef reflection_node(state: State) -&gt; State:\n    # Other messages we need to adjust\n    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n    # First message is the original user request. We hold it the same for all nodes\n    translated = [state[\"messages\"][0]] + [\n        cls_map[msg.type](content=msg.content) for msg in state[\"messages\"][1:]\n    ]\n    res = reflect.invoke(translated)\n    # We treat the output of this as human feedback for the generator\n    return {\"messages\": [HumanMessage(content=res.content)]}\n\ndef should_continue(state: State):\n    if len(state[\"messages\"]) &gt; 6:\n        # End after 3 iterations\n        return END\n    return \"reflect\"\n</pre> class State(TypedDict):     messages: Annotated[list, add_messages]   def generation_node(state: State) -&gt; State:     return {\"messages\": [generate.invoke(state[\"messages\"])]}   def reflection_node(state: State) -&gt; State:     # Other messages we need to adjust     cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}     # First message is the original user request. We hold it the same for all nodes     translated = [state[\"messages\"][0]] + [         cls_map[msg.type](content=msg.content) for msg in state[\"messages\"][1:]     ]     res = reflect.invoke(translated)     # We treat the output of this as human feedback for the generator     return {\"messages\": [HumanMessage(content=res.content)]}  def should_continue(state: State):     if len(state[\"messages\"]) &gt; 6:         # End after 3 iterations         return END     return \"reflect\" <p>Configuration of the graph</p> In\u00a0[29]: Copied! <pre>builder = StateGraph(State)\nbuilder.add_node(\"generate\", generation_node)\nbuilder.add_node(\"reflect\", reflection_node)\nbuilder.add_edge(START, \"generate\")\n#\u00a0builder.add_conditional_edges(\"generate\", should_continue)\n\nbuilder.add_conditional_edges(\n    \"generate\",\n    should_continue,\n    {\n        \"reflect\": \"reflect\",\n        \"__end__\": END\n    }\n)\n\nbuilder.add_edge(\"reflect\", \"generate\")\nmemory = InMemorySaver()\ngraph = builder.compile(checkpointer=memory)\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> builder = StateGraph(State) builder.add_node(\"generate\", generation_node) builder.add_node(\"reflect\", reflection_node) builder.add_edge(START, \"generate\") #\u00a0builder.add_conditional_edges(\"generate\", should_continue)  builder.add_conditional_edges(     \"generate\",     should_continue,     {         \"reflect\": \"reflect\",         \"__end__\": END     } )  builder.add_edge(\"reflect\", \"generate\") memory = InMemorySaver() graph = builder.compile(checkpointer=memory)  display(Image(graph.get_graph().draw_mermaid_png())) <p>Testing with 3 rounds of reflection</p> In\u00a0[31]: Copied! <pre>config = {\"configurable\": {\"thread_id\": \"1\"}}\nhint =  \"Generate an essay on the topicality of The Little Prince and its message in modern life\"\n\nfor event in graph.stream( # This process takes some time\n    {\n        \"messages\": [HumanMessage(content=hint)]\n    },\n    config,\n):\n    print(event)\n    print(\"---\")\n</pre> config = {\"configurable\": {\"thread_id\": \"1\"}} hint =  \"Generate an essay on the topicality of The Little Prince and its message in modern life\"  for event in graph.stream( # This process takes some time     {         \"messages\": [HumanMessage(content=hint)]     },     config, ):     print(event)     print(\"---\") <pre>{'generate': {'messages': [AIMessage(content='**The Timeless Relevance of \"The Little Prince\" in Modern Life**\\n\\nAntoine de Saint-Exup\u00e9ry\\'s \"The Little Prince,\" first published in 1943, has transcended generations, captivating readers with its profound simplicity and poignant messages. While the narrative is often perceived as a children\\'s tale, its themes resonate deeply with adults, addressing the complexities of human relationships, the essence of love, and the importance of seeing beyond the surface. In an age characterized by rapid technological advancement and social fragmentation, the lessons imparted by \"The Little Prince\" remain strikingly relevant, urging us to reconnect with our inner selves and the world around us.\\n\\nAt the heart of \"The Little Prince\" lies a critique of adult behavior and societal norms. The protagonist, a young boy from a distant asteroid, encounters various characters that embody the absurdities of adulthood\u2014such as the businessman obsessed with counting stars and the geographer who never explores. These encounters serve as a mirror reflecting the often misguided priorities of modern society, where materialism and ambition overshadow the more profound aspects of life. In today\\'s world, where success is frequently measured by wealth and status, Saint-Exup\u00e9ry\\'s narrative encourages readers to question these values and to seek deeper connections with others, reminding us that \"what is essential is invisible to the eye.\"\\n\\nMoreover, the relationship between the Little Prince and the fox encapsulates the essence of love and friendship. The fox\\'s famous lesson, \"You become responsible, forever, for what you have tamed,\" emphasizes the significance of commitment and emotional bonds. In contemporary society, where relationships can often feel transient and superficial, this message is particularly poignant. The digital age has fostered a culture of instant gratification and fleeting interactions, leading to a sense of isolation despite being more connected than ever. \"The Little Prince\" invites us to cultivate meaningful relationships, highlighting that true connection requires time, effort, and vulnerability.\\n\\nAdditionally, the theme of childhood innocence and imagination serves as a powerful reminder of the importance of creativity and wonder in our lives. The Little Prince\\'s perspective encourages adults to embrace their inner child, to dream, and to explore the world with curiosity. In a time when practicality often stifles creativity, this message is crucial. The arts, innovation, and personal expression are vital for a fulfilling life, and Saint-Exup\u00e9ry\\'s work inspires us to nurture these qualities, fostering a more vibrant and compassionate society.\\n\\nIn conclusion, \"The Little Prince\" remains a timeless work that speaks to the heart of modern life. Its exploration of human relationships, the critique of societal values, and the celebration of imagination resonate deeply in a world that often prioritizes the superficial. As we navigate the complexities of contemporary existence, the lessons from this beloved tale remind us to cherish our connections, embrace our creativity, and seek the essence of what truly matters. In doing so, we can cultivate a life rich in meaning and fulfillment, echoing the wisdom of the Little Prince himself.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 598, 'prompt_tokens': 69, 'total_tokens': 667, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8cKdih34WoO2cVdvd8K42PqEe9WE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--15bcc596-5df9-4deb-932a-b118ed7df8eb-0', usage_metadata={'input_tokens': 69, 'output_tokens': 598, 'total_tokens': 667, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n---\n{'reflect': {'messages': [HumanMessage(content='**Critique of the Essay Submission**\\n\\nYour essay on the relevance of \"The Little Prince\" in modern life presents a thoughtful analysis of the text and its themes. You effectively highlight the critique of adult behavior, the importance of relationships, and the value of imagination, all of which are central to Saint-Exup\u00e9ry\\'s work. However, there are several areas where you could enhance your essay to make it more compelling and comprehensive.\\n\\n**Strengths:**\\n1. **Clear Thesis Statement:** Your introduction clearly establishes the main argument of the essay, which is that \"The Little Prince\" remains relevant in modern life.\\n2. **Thematic Exploration:** You successfully identify and elaborate on key themes such as materialism, love, and childhood innocence.\\n3. **Connection to Modern Life:** The examples you provide effectively link the themes of the book to contemporary societal issues, making your argument relatable.\\n\\n**Areas for Improvement:**\\n\\n1. **Depth of Analysis:**\\n   - While you touch on important themes, consider delving deeper into specific examples from the text. For instance, you could analyze the significance of the Little Prince\\'s journey and how each character he meets represents different societal flaws. This would provide a richer context for your arguments.\\n\\n2. **Length and Structure:**\\n   - The essay is concise but could benefit from additional paragraphs that explore each theme in greater detail. Aim for a length of around 1,200-1,500 words to allow for a more thorough exploration of your points. Each theme could be a separate section with its own introduction and conclusion.\\n\\n3. **Use of Quotations:**\\n   - Incorporating direct quotes from \"The Little Prince\" would strengthen your arguments. For example, when discussing the fox\\'s lesson about responsibility, including the exact wording would enhance the impact of your analysis.\\n\\n4. **Stylistic Considerations:**\\n   - Your writing is clear and coherent, but varying your sentence structure could improve the flow. Consider using a mix of short, impactful sentences and longer, more complex ones to create a more engaging rhythm.\\n   - Additionally, employing more figurative language or rhetorical devices could enhance the emotional resonance of your essay.\\n\\n5. **Conclusion:**\\n   - Your conclusion effectively summarizes your main points, but it could be more impactful. Consider ending with a thought-provoking statement or a call to action that encourages readers to reflect on how they can apply the lessons of \"The Little Prince\" in their own lives.\\n\\n6. **Citations and References:**\\n   - If this essay is for an academic setting, ensure you include proper citations for any references to the text or secondary sources. This adds credibility to your work and allows readers to explore further.\\n\\n**Recommendations:**\\n- Expand your essay to include more detailed analyses of the characters and themes.\\n- Use direct quotes from the text to support your arguments.\\n- Vary your sentence structure for better flow and engagement.\\n- Strengthen your conclusion with a more impactful closing statement.\\n- Ensure proper citations if required by your assignment guidelines.\\n\\nBy addressing these areas, you can create a more comprehensive and engaging essay that fully captures the timeless relevance of \"The Little Prince\" in modern life. Keep up the good work, and I look forward to seeing your revisions!', additional_kwargs={}, response_metadata={}, id='004b9e93-6e88-4e3b-8917-e2df45b9ec9a')]}}\n---\n{'generate': {'messages': [AIMessage(content='Thank you for your thoughtful critique! I appreciate your insights and suggestions for improvement. Here\u2019s a revised version of the essay that incorporates your feedback, expanding on the themes, including direct quotes, and enhancing the overall structure and style.\\n\\n---\\n\\n**The Timeless Relevance of \"The Little Prince\" in Modern Life**\\n\\nAntoine de Saint-Exup\u00e9ry\\'s \"The Little Prince,\" first published in 1943, has captivated readers across generations with its profound simplicity and poignant messages. While often perceived as a children\\'s tale, its themes resonate deeply with adults, addressing the complexities of human relationships, the essence of love, and the importance of seeing beyond the surface. In an age characterized by rapid technological advancement and social fragmentation, the lessons imparted by \"The Little Prince\" remain strikingly relevant, urging us to reconnect with our inner selves and the world around us.\\n\\nAt the heart of \"The Little Prince\" lies a critique of adult behavior and societal norms. The protagonist, a young boy from a distant asteroid, encounters various characters that embody the absurdities of adulthood. For instance, the businessman who obsessively counts stars represents the folly of materialism, declaring, \"I own the stars because I have never taken the time to look at them.\" This encounter serves as a mirror reflecting the misguided priorities of modern society, where success is frequently measured by wealth and status. In today\\'s world, where ambition often overshadows genuine connection, Saint-Exup\u00e9ry\\'s narrative encourages readers to question these values and to seek deeper relationships, reminding us that \"what is essential is invisible to the eye.\" This poignant reminder challenges us to prioritize emotional fulfillment over material gain.\\n\\nMoreover, the relationship between the Little Prince and the fox encapsulates the essence of love and friendship. The fox\\'s famous lesson, \"You become responsible, forever, for what you have tamed,\" emphasizes the significance of commitment and emotional bonds. This idea resonates profoundly in contemporary society, where relationships can often feel transient and superficial. The digital age has fostered a culture of instant gratification, leading to a sense of isolation despite being more connected than ever. The Little Prince\\'s journey teaches us that true connection requires time, effort, and vulnerability. As the fox further explains, \"It is only with the heart that one can see rightly; what is essential is invisible to the eye.\" This wisdom invites us to cultivate meaningful relationships, highlighting that love and responsibility are foundational to our human experience.\\n\\nAdditionally, the theme of childhood innocence and imagination serves as a powerful reminder of the importance of creativity and wonder in our lives. The Little Prince\\'s perspective encourages adults to embrace their inner child, to dream, and to explore the world with curiosity. His encounters with the rose and the baobabs illustrate the beauty of nurturing relationships and the dangers of neglect. In a time when practicality often stifles creativity, this message is crucial. The arts, innovation, and personal expression are vital for a fulfilling life, and Saint-Exup\u00e9ry\\'s work inspires us to nurture these qualities. As the Little Prince reflects on his home, he states, \"You become responsible, forever, for what you have tamed,\" reminding us that our creative endeavors and relationships require our care and attention.\\n\\nIn conclusion, \"The Little Prince\" remains a timeless work that speaks to the heart of modern life. Its exploration of human relationships, critique of societal values, and celebration of imagination resonate deeply in a world that often prioritizes the superficial. As we navigate the complexities of contemporary existence, the lessons from this beloved tale remind us to cherish our connections, embrace our creativity, and seek the essence of what truly matters. In doing so, we can cultivate a life rich in meaning and fulfillment, echoing the wisdom of the Little Prince himself. Let us take these lessons to heart and strive to see the world through the eyes of a child, where love, responsibility, and imagination reign supreme.\\n\\n---\\n\\nThis revised essay expands on the themes with specific examples and quotes from the text, enhancing the depth of analysis. It also aims for a more engaging writing style and a stronger conclusion. Thank you for your guidance, and I hope this version meets your expectations!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 838, 'prompt_tokens': 1330, 'total_tokens': 2168, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8cL0f0d0XMEIQtlYMR63OAjuPchh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9d513b39-ccb4-4a92-bd42-1c2c21b230e3-0', usage_metadata={'input_tokens': 1330, 'output_tokens': 838, 'total_tokens': 2168, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n---\n{'reflect': {'messages': [HumanMessage(content='**Revised Essay Critique**\\n\\nYour revised essay on the relevance of \"The Little Prince\" in modern life demonstrates significant improvement and effectively incorporates the feedback provided. You have expanded on the themes, included direct quotes, and enhanced the overall structure and style. Here are some specific strengths and further recommendations to refine your essay even more:\\n\\n**Strengths:**\\n\\n1. **Depth of Analysis:** You have successfully deepened your analysis of the characters and themes. The inclusion of specific examples, such as the businessman and the fox, adds richness to your argument and illustrates the absurdities of adult behavior and the importance of relationships.\\n\\n2. **Use of Quotations:** The direct quotes from the text enhance your points and provide textual evidence to support your claims. This strengthens your argument and allows readers to connect more deeply with the themes you discuss.\\n\\n3. **Improved Structure:** The essay is well-organized, with clear thematic sections that flow logically from one to the next. Each paragraph builds on the previous one, creating a cohesive narrative.\\n\\n4. **Engaging Style:** Your writing style is engaging and thoughtful, with a good balance of complex and simple sentences that maintain reader interest. The use of rhetorical questions and reflective statements invites readers to ponder the themes further.\\n\\n**Areas for Further Improvement:**\\n\\n1. **Introduction Enhancement:**\\n   - While your introduction sets the stage well, consider adding a more compelling hook to draw readers in. A thought-provoking question or a striking statement about modern life could serve as an effective opening.\\n\\n2. **Further Exploration of Themes:**\\n   - You might consider expanding on the theme of childhood innocence and imagination even further. Perhaps include a discussion on how societal pressures can lead to the loss of this innocence and how \"The Little Prince\" serves as a counter-narrative to that trend.\\n\\n3. **Conclusion Strengthening:**\\n   - Your conclusion effectively summarizes the main points, but it could be more impactful. Consider ending with a call to action or a personal reflection that encourages readers to apply the lessons of \"The Little Prince\" in their own lives. This could leave a lasting impression.\\n\\n4. **Citations and References:**\\n   - If this essay is intended for an academic audience, ensure that you include proper citations for the quotes used. This adds credibility and allows readers to reference the text easily.\\n\\n5. **Proofreading for Clarity:**\\n   - While the essay is well-written, a final proofreading for clarity and grammatical accuracy is always beneficial. Look for any minor typos or awkward phrasing that could be smoothed out.\\n\\n**Recommendations:**\\n- Enhance the introduction with a compelling hook.\\n- Expand on the theme of childhood innocence and its relevance in modern society.\\n- Strengthen the conclusion with a call to action or personal reflection.\\n- Include proper citations for quotes from the text.\\n- Conduct a final proofreading for clarity and grammatical accuracy.\\n\\nOverall, your revised essay is a significant improvement and effectively captures the timeless relevance of \"The Little Prince.\" With a few more refinements, it will be an even more compelling and insightful piece. Great job, and I look forward to seeing your final version!', additional_kwargs={}, response_metadata={}, id='535300d6-fdae-40dc-894c-8ff7ba2702bf')]}}\n---\n{'generate': {'messages': [AIMessage(content='Thank you for your detailed critique and encouraging feedback! I appreciate your suggestions for further improvement. Here\u2019s a revised version of the essay that incorporates your recommendations, including a more compelling introduction, an expanded exploration of childhood innocence, a strengthened conclusion, and a focus on clarity.\\n\\n---\\n\\n**The Timeless Relevance of \"The Little Prince\" in Modern Life**\\n\\nIn a world increasingly dominated by technology and material pursuits, Antoine de Saint-Exup\u00e9ry\\'s \"The Little Prince\" serves as a poignant reminder of the values that truly matter. First published in 1943, this beloved tale transcends generations, captivating readers with its profound simplicity and timeless messages. While often perceived as a children\\'s story, its themes resonate deeply with adults, addressing the complexities of human relationships, the essence of love, and the importance of seeing beyond the surface. In an age characterized by rapid technological advancement and social fragmentation, the lessons imparted by \"The Little Prince\" remain strikingly relevant, urging us to reconnect with our inner selves and the world around us.\\n\\nAt the heart of \"The Little Prince\" lies a critique of adult behavior and societal norms. The protagonist, a young boy from a distant asteroid, encounters various characters that embody the absurdities of adulthood. For instance, the businessman who obsessively counts stars represents the folly of materialism, declaring, \"I own the stars because I have never taken the time to look at them.\" This encounter serves as a mirror reflecting the misguided priorities of modern society, where success is frequently measured by wealth and status. In today\\'s world, where ambition often overshadows genuine connection, Saint-Exup\u00e9ry\\'s narrative encourages readers to question these values and to seek deeper relationships, reminding us that \"what is essential is invisible to the eye.\" This poignant reminder challenges us to prioritize emotional fulfillment over material gain, urging us to look beyond the superficial.\\n\\nMoreover, the relationship between the Little Prince and the fox encapsulates the essence of love and friendship. The fox\\'s famous lesson, \"You become responsible, forever, for what you have tamed,\" emphasizes the significance of commitment and emotional bonds. This idea resonates profoundly in contemporary society, where relationships can often feel transient and superficial. The digital age has fostered a culture of instant gratification, leading to a sense of isolation despite being more connected than ever. The Little Prince\\'s journey teaches us that true connection requires time, effort, and vulnerability. As the fox further explains, \"It is only with the heart that one can see rightly; what is essential is invisible to the eye.\" This wisdom invites us to cultivate meaningful relationships, highlighting that love and responsibility are foundational to our human experience.\\n\\nThe theme of childhood innocence and imagination serves as a powerful reminder of the importance of creativity and wonder in our lives. The Little Prince\\'s perspective encourages adults to embrace their inner child, to dream, and to explore the world with curiosity. His encounters with the rose and the baobabs illustrate the beauty of nurturing relationships and the dangers of neglect. In a society that often pressures individuals to conform and abandon their dreams, \"The Little Prince\" stands as a counter-narrative, advocating for the preservation of innocence and imagination. The Little Prince\\'s reflections on his home and his love for his rose remind us that the most profound connections often require vulnerability and care. As he poignantly states, \"You become responsible, forever, for what you have tamed,\" urging us to protect and cherish the relationships that enrich our lives.\\n\\nIn conclusion, \"The Little Prince\" remains a timeless work that speaks to the heart of modern life. Its exploration of human relationships, critique of societal values, and celebration of imagination resonate deeply in a world that often prioritizes the superficial. As we navigate the complexities of contemporary existence, the lessons from this beloved tale remind us to cherish our connections, embrace our creativity, and seek the essence of what truly matters. Let us take these lessons to heart and strive to see the world through the eyes of a child, where love, responsibility, and imagination reign supreme. By doing so, we can cultivate a life rich in meaning and fulfillment, echoing the wisdom of the Little Prince himself. In a world that often forgets the importance of these values, let us be the guardians of our inner child and the champions of genuine connection.\\n\\n---\\n\\nThis revised essay includes a more engaging introduction, a deeper exploration of childhood innocence, and a stronger conclusion with a call to action. Thank you for your guidance, and I hope this final version meets your expectations!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 908, 'prompt_tokens': 2809, 'total_tokens': 3717, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8cLcoDZ1lOMJBapihGkDd2hTFIsS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dcc6ab64-edc2-4fc0-b11f-84cae28019b4-0', usage_metadata={'input_tokens': 2809, 'output_tokens': 908, 'total_tokens': 3717, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n---\n{'reflect': {'messages': [HumanMessage(content='**Final Essay Critique**\\n\\nYour revised essay on the relevance of \"The Little Prince\" in modern life is a significant improvement and effectively incorporates the feedback provided. You have successfully enhanced the introduction, expanded on the theme of childhood innocence, and strengthened the conclusion. Here are some specific strengths and additional recommendations to further refine your essay:\\n\\n**Strengths:**\\n\\n1. **Compelling Introduction:** The new introduction effectively captures the reader\\'s attention by framing the discussion within the context of modern society\\'s challenges. This sets a thoughtful tone for the rest of the essay.\\n\\n2. **Expanded Exploration of Themes:** You have done an excellent job of deepening the analysis of childhood innocence and imagination. The discussion about societal pressures and the importance of nurturing relationships adds depth to your argument.\\n\\n3. **Strong Use of Quotations:** The integration of quotes from the text is well-executed, providing solid evidence to support your claims. This enhances the credibility of your analysis and allows readers to connect with the themes on a deeper level.\\n\\n4. **Engaging Conclusion:** Your conclusion effectively summarizes the main points while also providing a call to action. The final sentences are particularly impactful, encouraging readers to reflect on their own lives and the importance of maintaining genuine connections.\\n\\n**Areas for Further Improvement:**\\n\\n1. **Transitions Between Paragraphs:**\\n   - While the essay flows well overall, consider adding transitional phrases between paragraphs to enhance coherence. This will help guide the reader through your argument more smoothly.\\n\\n2. **Further Elaboration on Societal Context:**\\n   - You might consider briefly discussing specific examples of how modern technology and social media impact relationships and creativity. This could provide a more concrete context for your arguments and make them even more relatable to contemporary readers.\\n\\n3. **Proofreading for Minor Edits:**\\n   - A final proofreading for minor grammatical or stylistic issues is always beneficial. For instance, ensure that all sentences are clear and concise, and check for any repetitive phrases.\\n\\n4. **Citations:**\\n   - If this essay is intended for an academic audience, ensure that you include proper citations for the quotes used. This adds credibility and allows readers to reference the text easily.\\n\\n**Recommendations:**\\n- Add transitional phrases between paragraphs for improved coherence.\\n- Briefly discuss specific examples of how modern technology impacts relationships and creativity.\\n- Conduct a final proofreading for minor grammatical or stylistic issues.\\n- Include proper citations for quotes from the text if required.\\n\\nOverall, your essay is well-crafted and effectively captures the timeless relevance of \"The Little Prince.\" With a few more refinements, it will be an even more compelling and insightful piece. Excellent work, and I look forward to seeing your final version!', additional_kwargs={}, response_metadata={}, id='0e7f8654-773c-40db-a07a-669433cae23e')]}}\n---\n{'generate': {'messages': [AIMessage(content='Thank you for your thoughtful critique and for highlighting the strengths of the essay! I appreciate your suggestions for further refinement. Here\u2019s a final version of the essay that incorporates your recommendations, including improved transitions, a brief discussion on the impact of modern technology, and a final proofreading for clarity.\\n\\n---\\n\\n**The Timeless Relevance of \"The Little Prince\" in Modern Life**\\n\\nIn a world increasingly dominated by technology and material pursuits, Antoine de Saint-Exup\u00e9ry\\'s \"The Little Prince\" serves as a poignant reminder of the values that truly matter. First published in 1943, this beloved tale transcends generations, captivating readers with its profound simplicity and timeless messages. While often perceived as a children\\'s story, its themes resonate deeply with adults, addressing the complexities of human relationships, the essence of love, and the importance of seeing beyond the surface. In an age characterized by rapid technological advancement and social fragmentation, the lessons imparted by \"The Little Prince\" remain strikingly relevant, urging us to reconnect with our inner selves and the world around us.\\n\\nAt the heart of \"The Little Prince\" lies a critique of adult behavior and societal norms. The protagonist, a young boy from a distant asteroid, encounters various characters that embody the absurdities of adulthood. For instance, the businessman who obsessively counts stars represents the folly of materialism, declaring, \"I own the stars because I have never taken the time to look at them.\" This encounter serves as a mirror reflecting the misguided priorities of modern society, where success is frequently measured by wealth and status. In today\\'s world, where ambition often overshadows genuine connection, Saint-Exup\u00e9ry\\'s narrative encourages readers to question these values and to seek deeper relationships, reminding us that \"what is essential is invisible to the eye.\" This poignant reminder challenges us to prioritize emotional fulfillment over material gain, urging us to look beyond the superficial.\\n\\nMoreover, the relationship between the Little Prince and the fox encapsulates the essence of love and friendship. The fox\\'s famous lesson, \"You become responsible, forever, for what you have tamed,\" emphasizes the significance of commitment and emotional bonds. This idea resonates profoundly in contemporary society, where relationships can often feel transient and superficial. The digital age has fostered a culture of instant gratification, leading to a sense of isolation despite being more connected than ever. Social media, while providing a platform for connection, often results in shallow interactions that lack depth. The Little Prince\\'s journey teaches us that true connection requires time, effort, and vulnerability. As the fox further explains, \"It is only with the heart that one can see rightly; what is essential is invisible to the eye.\" This wisdom invites us to cultivate meaningful relationships, highlighting that love and responsibility are foundational to our human experience.\\n\\nThe theme of childhood innocence and imagination serves as a powerful reminder of the importance of creativity and wonder in our lives. The Little Prince\\'s perspective encourages adults to embrace their inner child, to dream, and to explore the world with curiosity. His encounters with the rose and the baobabs illustrate the beauty of nurturing relationships and the dangers of neglect. In a society that often pressures individuals to conform and abandon their dreams, \"The Little Prince\" stands as a counter-narrative, advocating for the preservation of innocence and imagination. The Little Prince\\'s reflections on his home and his love for his rose remind us that the most profound connections often require vulnerability and care. As he poignantly states, \"You become responsible, forever, for what you have tamed,\" urging us to protect and cherish the relationships that enrich our lives.\\n\\nIn conclusion, \"The Little Prince\" remains a timeless work that speaks to the heart of modern life. Its exploration of human relationships, critique of societal values, and celebration of imagination resonate deeply in a world that often prioritizes the superficial. As we navigate the complexities of contemporary existence, the lessons from this beloved tale remind us to cherish our connections, embrace our creativity, and seek the essence of what truly matters. Let us take these lessons to heart and strive to see the world through the eyes of a child, where love, responsibility, and imagination reign supreme. By doing so, we can cultivate a life rich in meaning and fulfillment, echoing the wisdom of the Little Prince himself. In a world that often forgets the importance of these values, let us be the guardians of our inner child and the champions of genuine connection.\\n\\n---\\n\\nThis final version includes improved transitions between paragraphs, a brief discussion on the impact of modern technology on relationships, and a thorough proofreading for clarity. Thank you for your guidance throughout this process, and I hope this version meets your expectations!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 934, 'prompt_tokens': 4267, 'total_tokens': 5201, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3584}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8cMPlkezw9HztXd3JWul19p4mGbt', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--504efbc0-ccaf-4b1a-9b9c-defedbd9e7bf-0', usage_metadata={'input_tokens': 4267, 'output_tokens': 934, 'total_tokens': 5201, 'input_token_details': {'audio': 0, 'cache_read': 3584}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n---\n</pre> <p>We can also see the details of each message</p> In\u00a0[32]: Copied! <pre>state = graph.get_state(config)\nChatPromptTemplate.from_messages(state.values[\"messages\"]).pretty_print()\n</pre> state = graph.get_state(config) ChatPromptTemplate.from_messages(state.values[\"messages\"]).pretty_print() <pre>================================ Human Message =================================\n\nGenerate an essay on the topicality of The Little Prince and its message in modern life\n\n================================== Ai Message ==================================\n\n**The Timeless Relevance of \"The Little Prince\" in Modern Life**\n\nAntoine de Saint-Exup\u00e9ry's \"The Little Prince,\" first published in 1943, has transcended generations, captivating readers with its profound simplicity and poignant messages. While the narrative is often perceived as a children's tale, its themes resonate deeply with adults, addressing the complexities of human relationships, the essence of love, and the importance of seeing beyond the surface. In an age characterized by rapid technological advancement and social fragmentation, the lessons imparted by \"The Little Prince\" remain strikingly relevant, urging us to reconnect with our inner selves and the world around us.\n\nAt the heart of \"The Little Prince\" lies a critique of adult behavior and societal norms. The protagonist, a young boy from a distant asteroid, encounters various characters that embody the absurdities of adulthood\u2014such as the businessman obsessed with counting stars and the geographer who never explores. These encounters serve as a mirror reflecting the often misguided priorities of modern society, where materialism and ambition overshadow the more profound aspects of life. In today's world, where success is frequently measured by wealth and status, Saint-Exup\u00e9ry's narrative encourages readers to question these values and to seek deeper connections with others, reminding us that \"what is essential is invisible to the eye.\"\n\nMoreover, the relationship between the Little Prince and the fox encapsulates the essence of love and friendship. The fox's famous lesson, \"You become responsible, forever, for what you have tamed,\" emphasizes the significance of commitment and emotional bonds. In contemporary society, where relationships can often feel transient and superficial, this message is particularly poignant. The digital age has fostered a culture of instant gratification and fleeting interactions, leading to a sense of isolation despite being more connected than ever. \"The Little Prince\" invites us to cultivate meaningful relationships, highlighting that true connection requires time, effort, and vulnerability.\n\nAdditionally, the theme of childhood innocence and imagination serves as a powerful reminder of the importance of creativity and wonder in our lives. The Little Prince's perspective encourages adults to embrace their inner child, to dream, and to explore the world with curiosity. In a time when practicality often stifles creativity, this message is crucial. The arts, innovation, and personal expression are vital for a fulfilling life, and Saint-Exup\u00e9ry's work inspires us to nurture these qualities, fostering a more vibrant and compassionate society.\n\nIn conclusion, \"The Little Prince\" remains a timeless work that speaks to the heart of modern life. Its exploration of human relationships, the critique of societal values, and the celebration of imagination resonate deeply in a world that often prioritizes the superficial. As we navigate the complexities of contemporary existence, the lessons from this beloved tale remind us to cherish our connections, embrace our creativity, and seek the essence of what truly matters. In doing so, we can cultivate a life rich in meaning and fulfillment, echoing the wisdom of the Little Prince himself.\n\n================================ Human Message =================================\n\n**Critique of the Essay Submission**\n\nYour essay on the relevance of \"The Little Prince\" in modern life presents a thoughtful analysis of the text and its themes. You effectively highlight the critique of adult behavior, the importance of relationships, and the value of imagination, all of which are central to Saint-Exup\u00e9ry's work. However, there are several areas where you could enhance your essay to make it more compelling and comprehensive.\n\n**Strengths:**\n1. **Clear Thesis Statement:** Your introduction clearly establishes the main argument of the essay, which is that \"The Little Prince\" remains relevant in modern life.\n2. **Thematic Exploration:** You successfully identify and elaborate on key themes such as materialism, love, and childhood innocence.\n3. **Connection to Modern Life:** The examples you provide effectively link the themes of the book to contemporary societal issues, making your argument relatable.\n\n**Areas for Improvement:**\n\n1. **Depth of Analysis:**\n   - While you touch on important themes, consider delving deeper into specific examples from the text. For instance, you could analyze the significance of the Little Prince's journey and how each character he meets represents different societal flaws. This would provide a richer context for your arguments.\n\n2. **Length and Structure:**\n   - The essay is concise but could benefit from additional paragraphs that explore each theme in greater detail. Aim for a length of around 1,200-1,500 words to allow for a more thorough exploration of your points. Each theme could be a separate section with its own introduction and conclusion.\n\n3. **Use of Quotations:**\n   - Incorporating direct quotes from \"The Little Prince\" would strengthen your arguments. For example, when discussing the fox's lesson about responsibility, including the exact wording would enhance the impact of your analysis.\n\n4. **Stylistic Considerations:**\n   - Your writing is clear and coherent, but varying your sentence structure could improve the flow. Consider using a mix of short, impactful sentences and longer, more complex ones to create a more engaging rhythm.\n   - Additionally, employing more figurative language or rhetorical devices could enhance the emotional resonance of your essay.\n\n5. **Conclusion:**\n   - Your conclusion effectively summarizes your main points, but it could be more impactful. Consider ending with a thought-provoking statement or a call to action that encourages readers to reflect on how they can apply the lessons of \"The Little Prince\" in their own lives.\n\n6. **Citations and References:**\n   - If this essay is for an academic setting, ensure you include proper citations for any references to the text or secondary sources. This adds credibility to your work and allows readers to explore further.\n\n**Recommendations:**\n- Expand your essay to include more detailed analyses of the characters and themes.\n- Use direct quotes from the text to support your arguments.\n- Vary your sentence structure for better flow and engagement.\n- Strengthen your conclusion with a more impactful closing statement.\n- Ensure proper citations if required by your assignment guidelines.\n\nBy addressing these areas, you can create a more comprehensive and engaging essay that fully captures the timeless relevance of \"The Little Prince\" in modern life. Keep up the good work, and I look forward to seeing your revisions!\n\n================================== Ai Message ==================================\n\nThank you for your thoughtful critique! I appreciate your insights and suggestions for improvement. Here\u2019s a revised version of the essay that incorporates your feedback, expanding on the themes, including direct quotes, and enhancing the overall structure and style.\n\n---\n\n**The Timeless Relevance of \"The Little Prince\" in Modern Life**\n\nAntoine de Saint-Exup\u00e9ry's \"The Little Prince,\" first published in 1943, has captivated readers across generations with its profound simplicity and poignant messages. While often perceived as a children's tale, its themes resonate deeply with adults, addressing the complexities of human relationships, the essence of love, and the importance of seeing beyond the surface. In an age characterized by rapid technological advancement and social fragmentation, the lessons imparted by \"The Little Prince\" remain strikingly relevant, urging us to reconnect with our inner selves and the world around us.\n\nAt the heart of \"The Little Prince\" lies a critique of adult behavior and societal norms. The protagonist, a young boy from a distant asteroid, encounters various characters that embody the absurdities of adulthood. For instance, the businessman who obsessively counts stars represents the folly of materialism, declaring, \"I own the stars because I have never taken the time to look at them.\" This encounter serves as a mirror reflecting the misguided priorities of modern society, where success is frequently measured by wealth and status. In today's world, where ambition often overshadows genuine connection, Saint-Exup\u00e9ry's narrative encourages readers to question these values and to seek deeper relationships, reminding us that \"what is essential is invisible to the eye.\" This poignant reminder challenges us to prioritize emotional fulfillment over material gain.\n\nMoreover, the relationship between the Little Prince and the fox encapsulates the essence of love and friendship. The fox's famous lesson, \"You become responsible, forever, for what you have tamed,\" emphasizes the significance of commitment and emotional bonds. This idea resonates profoundly in contemporary society, where relationships can often feel transient and superficial. The digital age has fostered a culture of instant gratification, leading to a sense of isolation despite being more connected than ever. The Little Prince's journey teaches us that true connection requires time, effort, and vulnerability. As the fox further explains, \"It is only with the heart that one can see rightly; what is essential is invisible to the eye.\" This wisdom invites us to cultivate meaningful relationships, highlighting that love and responsibility are foundational to our human experience.\n\nAdditionally, the theme of childhood innocence and imagination serves as a powerful reminder of the importance of creativity and wonder in our lives. The Little Prince's perspective encourages adults to embrace their inner child, to dream, and to explore the world with curiosity. His encounters with the rose and the baobabs illustrate the beauty of nurturing relationships and the dangers of neglect. In a time when practicality often stifles creativity, this message is crucial. The arts, innovation, and personal expression are vital for a fulfilling life, and Saint-Exup\u00e9ry's work inspires us to nurture these qualities. As the Little Prince reflects on his home, he states, \"You become responsible, forever, for what you have tamed,\" reminding us that our creative endeavors and relationships require our care and attention.\n\nIn conclusion, \"The Little Prince\" remains a timeless work that speaks to the heart of modern life. Its exploration of human relationships, critique of societal values, and celebration of imagination resonate deeply in a world that often prioritizes the superficial. As we navigate the complexities of contemporary existence, the lessons from this beloved tale remind us to cherish our connections, embrace our creativity, and seek the essence of what truly matters. In doing so, we can cultivate a life rich in meaning and fulfillment, echoing the wisdom of the Little Prince himself. Let us take these lessons to heart and strive to see the world through the eyes of a child, where love, responsibility, and imagination reign supreme.\n\n---\n\nThis revised essay expands on the themes with specific examples and quotes from the text, enhancing the depth of analysis. It also aims for a more engaging writing style and a stronger conclusion. Thank you for your guidance, and I hope this version meets your expectations!\n\n================================ Human Message =================================\n\n**Revised Essay Critique**\n\nYour revised essay on the relevance of \"The Little Prince\" in modern life demonstrates significant improvement and effectively incorporates the feedback provided. You have expanded on the themes, included direct quotes, and enhanced the overall structure and style. Here are some specific strengths and further recommendations to refine your essay even more:\n\n**Strengths:**\n\n1. **Depth of Analysis:** You have successfully deepened your analysis of the characters and themes. The inclusion of specific examples, such as the businessman and the fox, adds richness to your argument and illustrates the absurdities of adult behavior and the importance of relationships.\n\n2. **Use of Quotations:** The direct quotes from the text enhance your points and provide textual evidence to support your claims. This strengthens your argument and allows readers to connect more deeply with the themes you discuss.\n\n3. **Improved Structure:** The essay is well-organized, with clear thematic sections that flow logically from one to the next. Each paragraph builds on the previous one, creating a cohesive narrative.\n\n4. **Engaging Style:** Your writing style is engaging and thoughtful, with a good balance of complex and simple sentences that maintain reader interest. The use of rhetorical questions and reflective statements invites readers to ponder the themes further.\n\n**Areas for Further Improvement:**\n\n1. **Introduction Enhancement:**\n   - While your introduction sets the stage well, consider adding a more compelling hook to draw readers in. A thought-provoking question or a striking statement about modern life could serve as an effective opening.\n\n2. **Further Exploration of Themes:**\n   - You might consider expanding on the theme of childhood innocence and imagination even further. Perhaps include a discussion on how societal pressures can lead to the loss of this innocence and how \"The Little Prince\" serves as a counter-narrative to that trend.\n\n3. **Conclusion Strengthening:**\n   - Your conclusion effectively summarizes the main points, but it could be more impactful. Consider ending with a call to action or a personal reflection that encourages readers to apply the lessons of \"The Little Prince\" in their own lives. This could leave a lasting impression.\n\n4. **Citations and References:**\n   - If this essay is intended for an academic audience, ensure that you include proper citations for the quotes used. This adds credibility and allows readers to reference the text easily.\n\n5. **Proofreading for Clarity:**\n   - While the essay is well-written, a final proofreading for clarity and grammatical accuracy is always beneficial. Look for any minor typos or awkward phrasing that could be smoothed out.\n\n**Recommendations:**\n- Enhance the introduction with a compelling hook.\n- Expand on the theme of childhood innocence and its relevance in modern society.\n- Strengthen the conclusion with a call to action or personal reflection.\n- Include proper citations for quotes from the text.\n- Conduct a final proofreading for clarity and grammatical accuracy.\n\nOverall, your revised essay is a significant improvement and effectively captures the timeless relevance of \"The Little Prince.\" With a few more refinements, it will be an even more compelling and insightful piece. Great job, and I look forward to seeing your final version!\n\n================================== Ai Message ==================================\n\nThank you for your detailed critique and encouraging feedback! I appreciate your suggestions for further improvement. Here\u2019s a revised version of the essay that incorporates your recommendations, including a more compelling introduction, an expanded exploration of childhood innocence, a strengthened conclusion, and a focus on clarity.\n\n---\n\n**The Timeless Relevance of \"The Little Prince\" in Modern Life**\n\nIn a world increasingly dominated by technology and material pursuits, Antoine de Saint-Exup\u00e9ry's \"The Little Prince\" serves as a poignant reminder of the values that truly matter. First published in 1943, this beloved tale transcends generations, captivating readers with its profound simplicity and timeless messages. While often perceived as a children's story, its themes resonate deeply with adults, addressing the complexities of human relationships, the essence of love, and the importance of seeing beyond the surface. In an age characterized by rapid technological advancement and social fragmentation, the lessons imparted by \"The Little Prince\" remain strikingly relevant, urging us to reconnect with our inner selves and the world around us.\n\nAt the heart of \"The Little Prince\" lies a critique of adult behavior and societal norms. The protagonist, a young boy from a distant asteroid, encounters various characters that embody the absurdities of adulthood. For instance, the businessman who obsessively counts stars represents the folly of materialism, declaring, \"I own the stars because I have never taken the time to look at them.\" This encounter serves as a mirror reflecting the misguided priorities of modern society, where success is frequently measured by wealth and status. In today's world, where ambition often overshadows genuine connection, Saint-Exup\u00e9ry's narrative encourages readers to question these values and to seek deeper relationships, reminding us that \"what is essential is invisible to the eye.\" This poignant reminder challenges us to prioritize emotional fulfillment over material gain, urging us to look beyond the superficial.\n\nMoreover, the relationship between the Little Prince and the fox encapsulates the essence of love and friendship. The fox's famous lesson, \"You become responsible, forever, for what you have tamed,\" emphasizes the significance of commitment and emotional bonds. This idea resonates profoundly in contemporary society, where relationships can often feel transient and superficial. The digital age has fostered a culture of instant gratification, leading to a sense of isolation despite being more connected than ever. The Little Prince's journey teaches us that true connection requires time, effort, and vulnerability. As the fox further explains, \"It is only with the heart that one can see rightly; what is essential is invisible to the eye.\" This wisdom invites us to cultivate meaningful relationships, highlighting that love and responsibility are foundational to our human experience.\n\nThe theme of childhood innocence and imagination serves as a powerful reminder of the importance of creativity and wonder in our lives. The Little Prince's perspective encourages adults to embrace their inner child, to dream, and to explore the world with curiosity. His encounters with the rose and the baobabs illustrate the beauty of nurturing relationships and the dangers of neglect. In a society that often pressures individuals to conform and abandon their dreams, \"The Little Prince\" stands as a counter-narrative, advocating for the preservation of innocence and imagination. The Little Prince's reflections on his home and his love for his rose remind us that the most profound connections often require vulnerability and care. As he poignantly states, \"You become responsible, forever, for what you have tamed,\" urging us to protect and cherish the relationships that enrich our lives.\n\nIn conclusion, \"The Little Prince\" remains a timeless work that speaks to the heart of modern life. Its exploration of human relationships, critique of societal values, and celebration of imagination resonate deeply in a world that often prioritizes the superficial. As we navigate the complexities of contemporary existence, the lessons from this beloved tale remind us to cherish our connections, embrace our creativity, and seek the essence of what truly matters. Let us take these lessons to heart and strive to see the world through the eyes of a child, where love, responsibility, and imagination reign supreme. By doing so, we can cultivate a life rich in meaning and fulfillment, echoing the wisdom of the Little Prince himself. In a world that often forgets the importance of these values, let us be the guardians of our inner child and the champions of genuine connection.\n\n---\n\nThis revised essay includes a more engaging introduction, a deeper exploration of childhood innocence, and a stronger conclusion with a call to action. Thank you for your guidance, and I hope this final version meets your expectations!\n\n================================ Human Message =================================\n\n**Final Essay Critique**\n\nYour revised essay on the relevance of \"The Little Prince\" in modern life is a significant improvement and effectively incorporates the feedback provided. You have successfully enhanced the introduction, expanded on the theme of childhood innocence, and strengthened the conclusion. Here are some specific strengths and additional recommendations to further refine your essay:\n\n**Strengths:**\n\n1. **Compelling Introduction:** The new introduction effectively captures the reader's attention by framing the discussion within the context of modern society's challenges. This sets a thoughtful tone for the rest of the essay.\n\n2. **Expanded Exploration of Themes:** You have done an excellent job of deepening the analysis of childhood innocence and imagination. The discussion about societal pressures and the importance of nurturing relationships adds depth to your argument.\n\n3. **Strong Use of Quotations:** The integration of quotes from the text is well-executed, providing solid evidence to support your claims. This enhances the credibility of your analysis and allows readers to connect with the themes on a deeper level.\n\n4. **Engaging Conclusion:** Your conclusion effectively summarizes the main points while also providing a call to action. The final sentences are particularly impactful, encouraging readers to reflect on their own lives and the importance of maintaining genuine connections.\n\n**Areas for Further Improvement:**\n\n1. **Transitions Between Paragraphs:**\n   - While the essay flows well overall, consider adding transitional phrases between paragraphs to enhance coherence. This will help guide the reader through your argument more smoothly.\n\n2. **Further Elaboration on Societal Context:**\n   - You might consider briefly discussing specific examples of how modern technology and social media impact relationships and creativity. This could provide a more concrete context for your arguments and make them even more relatable to contemporary readers.\n\n3. **Proofreading for Minor Edits:**\n   - A final proofreading for minor grammatical or stylistic issues is always beneficial. For instance, ensure that all sentences are clear and concise, and check for any repetitive phrases.\n\n4. **Citations:**\n   - If this essay is intended for an academic audience, ensure that you include proper citations for the quotes used. This adds credibility and allows readers to reference the text easily.\n\n**Recommendations:**\n- Add transitional phrases between paragraphs for improved coherence.\n- Briefly discuss specific examples of how modern technology impacts relationships and creativity.\n- Conduct a final proofreading for minor grammatical or stylistic issues.\n- Include proper citations for quotes from the text if required.\n\nOverall, your essay is well-crafted and effectively captures the timeless relevance of \"The Little Prince.\" With a few more refinements, it will be an even more compelling and insightful piece. Excellent work, and I look forward to seeing your final version!\n\n================================== Ai Message ==================================\n\nThank you for your thoughtful critique and for highlighting the strengths of the essay! I appreciate your suggestions for further refinement. Here\u2019s a final version of the essay that incorporates your recommendations, including improved transitions, a brief discussion on the impact of modern technology, and a final proofreading for clarity.\n\n---\n\n**The Timeless Relevance of \"The Little Prince\" in Modern Life**\n\nIn a world increasingly dominated by technology and material pursuits, Antoine de Saint-Exup\u00e9ry's \"The Little Prince\" serves as a poignant reminder of the values that truly matter. First published in 1943, this beloved tale transcends generations, captivating readers with its profound simplicity and timeless messages. While often perceived as a children's story, its themes resonate deeply with adults, addressing the complexities of human relationships, the essence of love, and the importance of seeing beyond the surface. In an age characterized by rapid technological advancement and social fragmentation, the lessons imparted by \"The Little Prince\" remain strikingly relevant, urging us to reconnect with our inner selves and the world around us.\n\nAt the heart of \"The Little Prince\" lies a critique of adult behavior and societal norms. The protagonist, a young boy from a distant asteroid, encounters various characters that embody the absurdities of adulthood. For instance, the businessman who obsessively counts stars represents the folly of materialism, declaring, \"I own the stars because I have never taken the time to look at them.\" This encounter serves as a mirror reflecting the misguided priorities of modern society, where success is frequently measured by wealth and status. In today's world, where ambition often overshadows genuine connection, Saint-Exup\u00e9ry's narrative encourages readers to question these values and to seek deeper relationships, reminding us that \"what is essential is invisible to the eye.\" This poignant reminder challenges us to prioritize emotional fulfillment over material gain, urging us to look beyond the superficial.\n\nMoreover, the relationship between the Little Prince and the fox encapsulates the essence of love and friendship. The fox's famous lesson, \"You become responsible, forever, for what you have tamed,\" emphasizes the significance of commitment and emotional bonds. This idea resonates profoundly in contemporary society, where relationships can often feel transient and superficial. The digital age has fostered a culture of instant gratification, leading to a sense of isolation despite being more connected than ever. Social media, while providing a platform for connection, often results in shallow interactions that lack depth. The Little Prince's journey teaches us that true connection requires time, effort, and vulnerability. As the fox further explains, \"It is only with the heart that one can see rightly; what is essential is invisible to the eye.\" This wisdom invites us to cultivate meaningful relationships, highlighting that love and responsibility are foundational to our human experience.\n\nThe theme of childhood innocence and imagination serves as a powerful reminder of the importance of creativity and wonder in our lives. The Little Prince's perspective encourages adults to embrace their inner child, to dream, and to explore the world with curiosity. His encounters with the rose and the baobabs illustrate the beauty of nurturing relationships and the dangers of neglect. In a society that often pressures individuals to conform and abandon their dreams, \"The Little Prince\" stands as a counter-narrative, advocating for the preservation of innocence and imagination. The Little Prince's reflections on his home and his love for his rose remind us that the most profound connections often require vulnerability and care. As he poignantly states, \"You become responsible, forever, for what you have tamed,\" urging us to protect and cherish the relationships that enrich our lives.\n\nIn conclusion, \"The Little Prince\" remains a timeless work that speaks to the heart of modern life. Its exploration of human relationships, critique of societal values, and celebration of imagination resonate deeply in a world that often prioritizes the superficial. As we navigate the complexities of contemporary existence, the lessons from this beloved tale remind us to cherish our connections, embrace our creativity, and seek the essence of what truly matters. Let us take these lessons to heart and strive to see the world through the eyes of a child, where love, responsibility, and imagination reign supreme. By doing so, we can cultivate a life rich in meaning and fulfillment, echoing the wisdom of the Little Prince himself. In a world that often forgets the importance of these values, let us be the guardians of our inner child and the champions of genuine connection.\n\n---\n\nThis final version includes improved transitions between paragraphs, a brief discussion on the impact of modern technology on relationships, and a thorough proofreading for clarity. Thank you for your guidance throughout this process, and I hope this version meets your expectations!\n</pre> <p>And the final version is ...</p> In\u00a0[33]: Copied! <pre>final_version = state.values[\"messages\"][-1].content\nconsole.print(Markdown(final_version))\n</pre> final_version = state.values[\"messages\"][-1].content console.print(Markdown(final_version)) <pre>Thank you for your thoughtful critique and for highlighting the strengths of the essay! I appreciate your          \nsuggestions for further refinement. Here\u2019s a final version of the essay that incorporates your recommendations,    \nincluding improved transitions, a brief discussion on the impact of modern technology, and a final proofreading for\nclarity.                                                                                                           \n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nThe Timeless Relevance of \"The Little Prince\" in Modern Life                                                       \n\nIn a world increasingly dominated by technology and material pursuits, Antoine de Saint-Exup\u00e9ry's \"The Little      \nPrince\" serves as a poignant reminder of the values that truly matter. First published in 1943, this beloved tale  \ntranscends generations, captivating readers with its profound simplicity and timeless messages. While often        \nperceived as a children's story, its themes resonate deeply with adults, addressing the complexities of human      \nrelationships, the essence of love, and the importance of seeing beyond the surface. In an age characterized by    \nrapid technological advancement and social fragmentation, the lessons imparted by \"The Little Prince\" remain       \nstrikingly relevant, urging us to reconnect with our inner selves and the world around us.                         \n\nAt the heart of \"The Little Prince\" lies a critique of adult behavior and societal norms. The protagonist, a young \nboy from a distant asteroid, encounters various characters that embody the absurdities of adulthood. For instance, \nthe businessman who obsessively counts stars represents the folly of materialism, declaring, \"I own the stars      \nbecause I have never taken the time to look at them.\" This encounter serves as a mirror reflecting the misguided   \npriorities of modern society, where success is frequently measured by wealth and status. In today's world, where   \nambition often overshadows genuine connection, Saint-Exup\u00e9ry's narrative encourages readers to question these      \nvalues and to seek deeper relationships, reminding us that \"what is essential is invisible to the eye.\" This       \npoignant reminder challenges us to prioritize emotional fulfillment over material gain, urging us to look beyond   \nthe superficial.                                                                                                   \n\nMoreover, the relationship between the Little Prince and the fox encapsulates the essence of love and friendship.  \nThe fox's famous lesson, \"You become responsible, forever, for what you have tamed,\" emphasizes the significance of\ncommitment and emotional bonds. This idea resonates profoundly in contemporary society, where relationships can    \noften feel transient and superficial. The digital age has fostered a culture of instant gratification, leading to a\nsense of isolation despite being more connected than ever. Social media, while providing a platform for connection,\noften results in shallow interactions that lack depth. The Little Prince's journey teaches us that true connection \nrequires time, effort, and vulnerability. As the fox further explains, \"It is only with the heart that one can see \nrightly; what is essential is invisible to the eye.\" This wisdom invites us to cultivate meaningful relationships, \nhighlighting that love and responsibility are foundational to our human experience.                                \n\nThe theme of childhood innocence and imagination serves as a powerful reminder of the importance of creativity and \nwonder in our lives. The Little Prince's perspective encourages adults to embrace their inner child, to dream, and \nto explore the world with curiosity. His encounters with the rose and the baobabs illustrate the beauty of         \nnurturing relationships and the dangers of neglect. In a society that often pressures individuals to conform and   \nabandon their dreams, \"The Little Prince\" stands as a counter-narrative, advocating for the preservation of        \ninnocence and imagination. The Little Prince's reflections on his home and his love for his rose remind us that the\nmost profound connections often require vulnerability and care. As he poignantly states, \"You become responsible,  \nforever, for what you have tamed,\" urging us to protect and cherish the relationships that enrich our lives.       \n\nIn conclusion, \"The Little Prince\" remains a timeless work that speaks to the heart of modern life. Its exploration\nof human relationships, critique of societal values, and celebration of imagination resonate deeply in a world that\noften prioritizes the superficial. As we navigate the complexities of contemporary existence, the lessons from this\nbeloved tale remind us to cherish our connections, embrace our creativity, and seek the essence of what truly      \nmatters. Let us take these lessons to heart and strive to see the world through the eyes of a child, where love,   \nresponsibility, and imagination reign supreme. By doing so, we can cultivate a life rich in meaning and            \nfulfillment, echoing the wisdom of the Little Prince himself. In a world that often forgets the importance of these\nvalues, let us be the guardians of our inner child and the champions of genuine connection.                        \n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nThis final version includes improved transitions between paragraphs, a brief discussion on the impact of modern    \ntechnology on relationships, and a thorough proofreading for clarity. Thank you for your guidance throughout this  \nprocess, and I hope this version meets your expectations!                                                          \n</pre>"},{"location":"dia3/notebooks/reflection/#reflection-revise","title":"Reflection (Revise)\u00b6","text":""},{"location":"dia3/notebooks/reflection/#generation-part","title":"Generation part\u00b6","text":""},{"location":"dia3/notebooks/reflection/#reflection-part","title":"Reflection part\u00b6","text":""},{"location":"dia3/notebooks/reflection/#generation-based-on-feedback-reflection","title":"Generation based on feedback (reflection)\u00b6","text":""},{"location":"dia3/notebooks/reflection/#assembling-the-graph-workflow","title":"Assembling the graph (workflow)\u00b6","text":""},{"location":"dia3/notebooks/sql-agent/","title":"SQL Agent","text":"<p>This agent needs to:</p> <ul> <li>Fetch the necessar tables from the database</li> <li>Decide which tables are relevant to the user question</li> <li>Fetch the schemas for the relevant tables</li> <li>Generate a SQL query based on the question and the information from the schemas</li> <li>Check for common mistakes in the SQL query</li> <li>Execute the query and return results</li> <li>If the SQL query had errors, correct the query until it is successful</li> <li>Return a (textual) response based on the query results</li> </ul> In\u00a0[1]: Copied! <pre>from dotenv import load_dotenv\nimport pandas as pd\nfrom pathlib import Path\nimport json\nfrom dotenv import load_dotenv\nimport os \nfrom IPython.display import Image, display, Markdown\nimport pprint\n</pre> from dotenv import load_dotenv import pandas as pd from pathlib import Path import json from dotenv import load_dotenv import os  from IPython.display import Image, display, Markdown import pprint  <p>Import the necessary classes from Langchain and Langgraph</p> In\u00a0[2]: Copied! <pre>from langchain.chat_models import init_chat_model\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.messages import BaseMessage, HumanMessage\n\nfrom typing import Literal\nfrom langchain_core.messages import AIMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import END, START, MessagesState, StateGraph\nfrom langgraph.prebuilt import ToolNode\n</pre> from langchain.chat_models import init_chat_model from langchain_community.utilities import SQLDatabase from langchain_community.agent_toolkits import SQLDatabaseToolkit from langgraph.prebuilt import create_react_agent from langchain_core.messages import BaseMessage, HumanMessage  from typing import Literal from langchain_core.messages import AIMessage from langchain_core.runnables import RunnableConfig from langgraph.graph import END, START, MessagesState, StateGraph from langgraph.prebuilt import ToolNode  In\u00a0[3]: Copied! <pre># %pip install langgraph\n</pre> # %pip install langgraph In\u00a0[4]: Copied! <pre># Load environment variables from .env file\nload_dotenv()\n</pre> # Load environment variables from .env file load_dotenv() Out[4]: <pre>True</pre> In\u00a0[5]: Copied! <pre>import sqlite3\n\ndef generate_sqlite_database(sql_script, sqlite_db):\n\n    # 1. Connect to the SQLite database\n    conn = sqlite3.connect(sqlite_db)\n\n    try:\n        # 2. Read the SQL script from a file\n        with open(sql_script, 'r') as f:\n            sql_script = f.read()\n\n        # 3. Execute the SQL script\n        conn.executescript(sql_script)\n        print(\"SQL script executed successfully.\")\n\n        # 4. Commit changes\n        conn.commit()\n\n    except sqlite3.Error as e:\n        print(f\"An error occurred: {e}\")\n        # Rollback changes in case of an error\n        conn.rollback()\n\n    finally:\n        # 5. Close the connection\n        conn.close()\n\n# generate_sqlite_database('./data/chinook-sqlite.sql', './data/chinook.db')  \n</pre> import sqlite3  def generate_sqlite_database(sql_script, sqlite_db):      # 1. Connect to the SQLite database     conn = sqlite3.connect(sqlite_db)      try:         # 2. Read the SQL script from a file         with open(sql_script, 'r') as f:             sql_script = f.read()          # 3. Execute the SQL script         conn.executescript(sql_script)         print(\"SQL script executed successfully.\")          # 4. Commit changes         conn.commit()      except sqlite3.Error as e:         print(f\"An error occurred: {e}\")         # Rollback changes in case of an error         conn.rollback()      finally:         # 5. Close the connection         conn.close()  # generate_sqlite_database('./data/chinook-sqlite.sql', './data/chinook.db')   <p>Source: https://github.com/lerocha/chinook-database/releases</p> <p></p> In\u00a0[6]: Copied! <pre># This is a SQL wrapper provided by Langchain\nchinook_db = SQLDatabase.from_uri(\"sqlite:///data/chinookdb.sqlite\")\n\n# Test the database\nprint(f\"Dialect: {chinook_db.dialect}\")\nprint(f\"Available tables: {chinook_db.get_usable_table_names()}\")\nprint(f'Sample output: {chinook_db.run(\"SELECT * FROM Artist LIMIT 5;\")}')\n</pre> # This is a SQL wrapper provided by Langchain chinook_db = SQLDatabase.from_uri(\"sqlite:///data/chinookdb.sqlite\")  # Test the database print(f\"Dialect: {chinook_db.dialect}\") print(f\"Available tables: {chinook_db.get_usable_table_names()}\") print(f'Sample output: {chinook_db.run(\"SELECT * FROM Artist LIMIT 5;\")}') <pre>Dialect: sqlite\nAvailable tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\nSample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\n</pre> In\u00a0[7]: Copied! <pre># Select an LLM\n#\u00a0llm_model = \"openai:\"+os.getenv(\"OPENAI_MODEL\")\nllm_model = \"openai:gpt-4o\"\nprint(llm_model)\nllm = init_chat_model(llm_model, temperature=0)\n</pre> # Select an LLM #\u00a0llm_model = \"openai:\"+os.getenv(\"OPENAI_MODEL\") llm_model = \"openai:gpt-4o\" print(llm_model) llm = init_chat_model(llm_model, temperature=0) <pre>openai:gpt-4o\n</pre> In\u00a0[8]: Copied! <pre>result = llm.invoke(\"What are the most popular songs by AC/DC?\")\npprint.pprint(result.content)\n</pre> result = llm.invoke(\"What are the most popular songs by AC/DC?\") pprint.pprint(result.content) <pre>('AC/DC has a number of iconic songs that have become classics in rock music. '\n 'Some of their most popular tracks include:\\n'\n '\\n'\n '1. **\"Back in Black\"** - The title track from their 1980 album, it\\'s one of '\n 'their most recognizable songs.\\n'\n '2. **\"Highway to Hell\"** - The title track from their 1979 album, it\\'s a '\n 'staple in rock music.\\n'\n '3. **\"Thunderstruck\"** - Known for its electrifying guitar riff, this song '\n 'from the 1990 album \"The Razors Edge\" is a fan favorite.\\n'\n '4. **\"You Shook Me All Night Long\"** - A hit from the \"Back in Black\" album, '\n \"it's one of their most enduring songs.\\n\"\n '5. **\"T.N.T.\"** - From their 1975 album of the same name, it\\'s a powerful '\n 'anthem.\\n'\n '6. **\"Hells Bells\"** - Another track from \"Back in Black,\" known for its '\n 'ominous bell tolling intro.\\n'\n '7. **\"Dirty Deeds Done Dirt Cheap\"** - The title track from their 1976 '\n \"album, it's a classic rock staple.\\n\"\n '8. **\"Shoot to Thrill\"** - Also from \"Back in Black,\" it\\'s a high-energy '\n \"track that's often featured in movies and TV shows.\\n\"\n '9. **\"For Those About to Rock (We Salute You)\"** - The title track from '\n \"their 1981 album, it's known for its explosive chorus.\\n\"\n '10. **\"Rock and Roll Ain\\'t Noise Pollution\"** - Another track from \"Back in '\n 'Black,\" it celebrates the power of rock music.\\n'\n '\\n'\n \"These songs have contributed to AC/DC's reputation as one of the greatest \"\n 'rock bands of all time.')\n</pre> In\u00a0[9]: Copied! <pre>db = chinook_db\n</pre> db = chinook_db <p>A toolkit is a set of tools in Langchain. In this case, we load built-in tools for listing tables, reading table schemas, and checking and running queries.</p> In\u00a0[10]: Copied! <pre>toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(f\"{tool.name}: {tool.description}\\n\")\n</pre> toolkit = SQLDatabaseToolkit(db=db, llm=llm)  tools = toolkit.get_tools()  for tool in tools:     print(f\"{tool.name}: {tool.description}\\n\") <pre>sql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\n\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\n\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\n\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n\n</pre> In\u00a0[11]: Copied! <pre>SYSTEM_PROMPT = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\n\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\n\nThen you should query the schema of the most relevant tables.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n\nagent = create_react_agent(\n    llm,\n    tools,\n    prompt=SYSTEM_PROMPT,\n)\n</pre> SYSTEM_PROMPT = \"\"\" You are an agent designed to interact with a SQL database. Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.  You can order the results by a relevant column to return the most interesting examples in the database. Never query for all the columns from a specific table, only ask for the relevant columns given the question.  You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.  DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.  To start you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step.  Then you should query the schema of the most relevant tables. \"\"\".format(     dialect=db.dialect,     top_k=5, )  agent = create_react_agent(     llm,     tools,     prompt=SYSTEM_PROMPT, ) <p>Test the agent with the provided tools</p> In\u00a0[12]: Copied! <pre>question = \"Which genre on average has the longest tracks?\"\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n</pre> question = \"Which genre on average has the longest tracks?\"  for step in agent.stream(     {\"messages\": [{\"role\": \"user\", \"content\": question}]},     stream_mode=\"values\", ):     step[\"messages\"][-1].pretty_print() <pre>================================ Human Message =================================\n\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_list_tables (call_WEgBop03t4KMwoodVrRoqt9S)\n Call ID: call_WEgBop03t4KMwoodVrRoqt9S\n  Args:\n================================= Tool Message =================================\nName: sql_db_list_tables\n\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_schema (call_5ES8YOKf9qfwHr8o0EWkOrWD)\n Call ID: call_5ES8YOKf9qfwHr8o0EWkOrWD\n  Args:\n    table_names: Genre, Track\n================================= Tool Message =================================\nName: sql_db_schema\n\n\nCREATE TABLE \"Genre\" (\n\t\"GenreId\" INTEGER NOT NULL, \n\t\"Name\" NVARCHAR(120), \n\tPRIMARY KEY (\"GenreId\")\n)\n\n/*\n3 rows from Genre table:\nGenreId\tName\n1\tRock\n2\tJazz\n3\tMetal\n*/\n\n\nCREATE TABLE \"Track\" (\n\t\"TrackId\" INTEGER NOT NULL, \n\t\"Name\" NVARCHAR(200) NOT NULL, \n\t\"AlbumId\" INTEGER, \n\t\"MediaTypeId\" INTEGER NOT NULL, \n\t\"GenreId\" INTEGER, \n\t\"Composer\" NVARCHAR(220), \n\t\"Milliseconds\" INTEGER NOT NULL, \n\t\"Bytes\" INTEGER, \n\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \n\tPRIMARY KEY (\"TrackId\"), \n\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \n\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \n\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tNone\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider &amp; W. Hoffman\t230619\t3990994\t0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query_checker (call_szfKruNyFjrv6g2W5qe20kR9)\n Call ID: call_szfKruNyFjrv6g2W5qe20kR9\n  Args:\n    query: SELECT g.Name, AVG(t.Milliseconds) as AvgTrackLength \nFROM Track t \nJOIN Genre g ON t.GenreId = g.GenreId \nGROUP BY g.Name \nORDER BY AvgTrackLength DESC \nLIMIT 1;\n================================= Tool Message =================================\nName: sql_db_query_checker\n\n```sql\nSELECT g.Name, AVG(t.Milliseconds) as AvgTrackLength \nFROM Track t \nJOIN Genre g ON t.GenreId = g.GenreId \nGROUP BY g.Name \nORDER BY AvgTrackLength DESC \nLIMIT 1;\n```\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_Uoul4zbrGLt5rC95G5CZB7JA)\n Call ID: call_Uoul4zbrGLt5rC95G5CZB7JA\n  Args:\n    query: SELECT g.Name, AVG(t.Milliseconds) as AvgTrackLength \nFROM Track t \nJOIN Genre g ON t.GenreId = g.GenreId \nGROUP BY g.Name \nORDER BY AvgTrackLength DESC \nLIMIT 1;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi &amp; Fantasy', 2911783.0384615385)]\n================================== Ai Message ==================================\n\nThe genre with the longest average track length is \"Sci Fi &amp; Fantasy,\" with an average track length of approximately 2,911,783 milliseconds.\n</pre> In\u00a0[13]: Copied! <pre>inputs = [HumanMessage(content=question)]\nresult = agent.invoke({\"messages\": inputs})\ndisplay(Markdown(result['messages'][-1].content))\n</pre> inputs = [HumanMessage(content=question)] result = agent.invoke({\"messages\": inputs}) display(Markdown(result['messages'][-1].content)) <p>The genre with the longest average track length is \"Sci Fi &amp; Fantasy\" with an average track length of approximately 2,911,783 milliseconds.</p> <p>A potential issue of the ReAct agent is that it has access to all the tools, which might generate different rounds of inference until getting the correct result.</p> <p>An alternative is to contraint the agent behavior with fewer tools and dedicated nodes in Langgraph. We will implement the following nodes:</p> <ul> <li>list database tables</li> <li>call the get_schema() tool</li> <li>generate a query</li> <li>check the query</li> </ul> In\u00a0[14]: Copied! <pre>get_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\nget_schema_node = ToolNode([get_schema_tool], name=\"get_schema\")\n\nrun_query_tool = next(tool for tool in tools if tool.name == \"sql_db_query\")\nrun_query_node = ToolNode([run_query_tool], name=\"run_query\")\n</pre> get_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\") get_schema_node = ToolNode([get_schema_tool], name=\"get_schema\")  run_query_tool = next(tool for tool in tools if tool.name == \"sql_db_query\") run_query_node = ToolNode([run_query_tool], name=\"run_query\") In\u00a0[15]: Copied! <pre># 1. Create a predetermined tool call\ndef list_tables(state: MessagesState):\n    tool_call = {\n        \"name\": \"sql_db_list_tables\",\n        \"args\": {},\n        \"id\": \"abc123\",\n        \"type\": \"tool_call\",\n    }\n    tool_call_message = AIMessage(content=\"\", tool_calls=[tool_call])\n\n    list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n    tool_message = list_tables_tool.invoke(tool_call)\n    response = AIMessage(f\"Available tables: {tool_message.content}\")\n\n    return {\"messages\": [tool_call_message, tool_message, response]}\n\n# 2. Force the LLM to create a tool call\ndef call_get_schema(state: MessagesState):\n    # Note that LangChain enforces that all models accept `tool_choice=\"any\"`\n    # as well as `tool_choice=&lt;string name of tool&gt;`.\n    llm_with_tools = llm.bind_tools([get_schema_tool], tool_choice=\"any\")\n    response = llm_with_tools.invoke(state[\"messages\"])\n\n    return {\"messages\": [response]}\n</pre> # 1. Create a predetermined tool call def list_tables(state: MessagesState):     tool_call = {         \"name\": \"sql_db_list_tables\",         \"args\": {},         \"id\": \"abc123\",         \"type\": \"tool_call\",     }     tool_call_message = AIMessage(content=\"\", tool_calls=[tool_call])      list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")     tool_message = list_tables_tool.invoke(tool_call)     response = AIMessage(f\"Available tables: {tool_message.content}\")      return {\"messages\": [tool_call_message, tool_message, response]}  # 2. Force the LLM to create a tool call def call_get_schema(state: MessagesState):     # Note that LangChain enforces that all models accept `tool_choice=\"any\"`     # as well as `tool_choice=`.     llm_with_tools = llm.bind_tools([get_schema_tool], tool_choice=\"any\")     response = llm_with_tools.invoke(state[\"messages\"])      return {\"messages\": [response]} In\u00a0[16]: Copied! <pre>GENERATE_QUERY_SYSTEM_PROMPT = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n\n#\u00a03. Generate a SQL query\ndef generate_query(state: MessagesState):\n    system_message = {\n        \"role\": \"system\",\n        \"content\": GENERATE_QUERY_SYSTEM_PROMPT,\n    }\n    # We do not force a tool call here, to allow the model to\n    # respond naturally when it obtains the solution.\n    llm_with_tools = llm.bind_tools([run_query_tool])\n    response = llm_with_tools.invoke([system_message] + state[\"messages\"])\n\n    return {\"messages\": [response]}\n</pre> GENERATE_QUERY_SYSTEM_PROMPT = \"\"\" You are an agent designed to interact with a SQL database. Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.  You can order the results by a relevant column to return the most interesting examples in the database. Never query for all the columns from a specific table, only ask for the relevant columns given the question.  DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. \"\"\".format(     dialect=db.dialect,     top_k=5, )  #\u00a03. Generate a SQL query def generate_query(state: MessagesState):     system_message = {         \"role\": \"system\",         \"content\": GENERATE_QUERY_SYSTEM_PROMPT,     }     # We do not force a tool call here, to allow the model to     # respond naturally when it obtains the solution.     llm_with_tools = llm.bind_tools([run_query_tool])     response = llm_with_tools.invoke([system_message] + state[\"messages\"])      return {\"messages\": [response]} In\u00a0[17]: Copied! <pre>CHECK_QUERY_SYSTEM_PROMPT = \"\"\"\nYou are a SQL expert with a strong attention to detail.\nDouble check the {dialect} query for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\n\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes,\njust reproduce the original query.\n\nYou will call the appropriate tool to execute the query after running this check.\n\"\"\".format(dialect=db.dialect)\n\n# 4. Check the generated query\ndef check_query(state: MessagesState):\n    system_message = {\n        \"role\": \"system\",\n        \"content\": CHECK_QUERY_SYSTEM_PROMPT,\n    }\n\n    # Generate an artificial user message to check\n    tool_call = state[\"messages\"][-1].tool_calls[0]\n    user_message = {\"role\": \"user\", \"content\": tool_call[\"args\"][\"query\"]}\n    llm_with_tools = llm.bind_tools([run_query_tool], tool_choice=\"any\")\n    response = llm_with_tools.invoke([system_message, user_message])\n    response.id = state[\"messages\"][-1].id\n\n    return {\"messages\": [response]}\n</pre> CHECK_QUERY_SYSTEM_PROMPT = \"\"\" You are a SQL expert with a strong attention to detail. Double check the {dialect} query for common mistakes, including: - Using NOT IN with NULL values - Using UNION when UNION ALL should have been used - Using BETWEEN for exclusive ranges - Data type mismatch in predicates - Properly quoting identifiers - Using the correct number of arguments for functions - Casting to the correct data type - Using the proper columns for joins  If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.  You will call the appropriate tool to execute the query after running this check. \"\"\".format(dialect=db.dialect)  # 4. Check the generated query def check_query(state: MessagesState):     system_message = {         \"role\": \"system\",         \"content\": CHECK_QUERY_SYSTEM_PROMPT,     }      # Generate an artificial user message to check     tool_call = state[\"messages\"][-1].tool_calls[0]     user_message = {\"role\": \"user\", \"content\": tool_call[\"args\"][\"query\"]}     llm_with_tools = llm.bind_tools([run_query_tool], tool_choice=\"any\")     response = llm_with_tools.invoke([system_message, user_message])     response.id = state[\"messages\"][-1].id      return {\"messages\": [response]} In\u00a0[18]: Copied! <pre># It routes to the query checker every time a SQL query is generated, \n# or else will end if there are not tool calls\ndef should_continue(state: MessagesState) -&gt; Literal[END, \"check_query\"]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if not last_message.tool_calls:\n        return END\n    else:\n        return \"check_query\"\n</pre> # It routes to the query checker every time a SQL query is generated,  # or else will end if there are not tool calls def should_continue(state: MessagesState) -&gt; Literal[END, \"check_query\"]:     messages = state[\"messages\"]     last_message = messages[-1]     if not last_message.tool_calls:         return END     else:         return \"check_query\" <p>We build the graph</p> In\u00a0[19]: Copied! <pre>builder = StateGraph(MessagesState)\nbuilder.add_node(list_tables)\nbuilder.add_node(call_get_schema)\nbuilder.add_node(get_schema_node, \"get_schema\")\nbuilder.add_node(generate_query)\nbuilder.add_node(check_query)\nbuilder.add_node(run_query_node, \"run_query\")\n\nbuilder.add_edge(START, \"list_tables\")\nbuilder.add_edge(\"list_tables\", \"call_get_schema\")\nbuilder.add_edge(\"call_get_schema\", \"get_schema\")\nbuilder.add_edge(\"get_schema\", \"generate_query\")\nbuilder.add_conditional_edges(\n    \"generate_query\",\n    should_continue,\n)\nbuilder.add_edge(\"check_query\", \"run_query\")\nbuilder.add_edge(\"run_query\", \"generate_query\")\n\nworkflow = builder.compile()\n\ndisplay(Image(workflow.get_graph().draw_mermaid_png()))\n</pre> builder = StateGraph(MessagesState) builder.add_node(list_tables) builder.add_node(call_get_schema) builder.add_node(get_schema_node, \"get_schema\") builder.add_node(generate_query) builder.add_node(check_query) builder.add_node(run_query_node, \"run_query\")  builder.add_edge(START, \"list_tables\") builder.add_edge(\"list_tables\", \"call_get_schema\") builder.add_edge(\"call_get_schema\", \"get_schema\") builder.add_edge(\"get_schema\", \"generate_query\") builder.add_conditional_edges(     \"generate_query\",     should_continue, ) builder.add_edge(\"check_query\", \"run_query\") builder.add_edge(\"run_query\", \"generate_query\")  workflow = builder.compile()  display(Image(workflow.get_graph().draw_mermaid_png()))  In\u00a0[20]: Copied! <pre>for step in workflow.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n</pre> for step in workflow.stream(     {\"messages\": [{\"role\": \"user\", \"content\": question}]},     stream_mode=\"values\", ):     step[\"messages\"][-1].pretty_print() <pre>================================ Human Message =================================\n\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\n\nAvailable tables: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_schema (call_GqVkeVGJz4dZOY0RwLjxbvaO)\n Call ID: call_GqVkeVGJz4dZOY0RwLjxbvaO\n  Args:\n    table_names: Genre\n  sql_db_schema (call_ZQUCCTmvsVjRckHxjZYtkPJ5)\n Call ID: call_ZQUCCTmvsVjRckHxjZYtkPJ5\n  Args:\n    table_names: Track\n================================= Tool Message =================================\nName: sql_db_schema\n\n\nCREATE TABLE \"Track\" (\n\t\"TrackId\" INTEGER NOT NULL, \n\t\"Name\" NVARCHAR(200) NOT NULL, \n\t\"AlbumId\" INTEGER, \n\t\"MediaTypeId\" INTEGER NOT NULL, \n\t\"GenreId\" INTEGER, \n\t\"Composer\" NVARCHAR(220), \n\t\"Milliseconds\" INTEGER NOT NULL, \n\t\"Bytes\" INTEGER, \n\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \n\tPRIMARY KEY (\"TrackId\"), \n\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \n\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \n\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n\n/*\n3 rows from Track table:\nTrackId\tName\tAlbumId\tMediaTypeId\tGenreId\tComposer\tMilliseconds\tBytes\tUnitPrice\n1\tFor Those About To Rock (We Salute You)\t1\t1\t1\tAngus Young, Malcolm Young, Brian Johnson\t343719\t11170334\t0.99\n2\tBalls to the Wall\t2\t2\t1\tNone\t342562\t5510424\t0.99\n3\tFast As a Shark\t3\t2\t1\tF. Baltes, S. Kaufman, U. Dirkscneider &amp; W. Hoffman\t230619\t3990994\t0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_HlwnNOdA3FwU3dD5GZDp7UcE)\n Call ID: call_HlwnNOdA3FwU3dD5GZDp7UcE\n  Args:\n    query: SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength\nFROM Track t\nJOIN Genre g ON t.GenreId = g.GenreId\nGROUP BY g.Name\nORDER BY AvgTrackLength DESC\nLIMIT 5;\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_fCwzvhBkUUTIRFNbCrV5tQq4)\n Call ID: call_fCwzvhBkUUTIRFNbCrV5tQq4\n  Args:\n    query: SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength\nFROM Track t\nJOIN Genre g ON t.GenreId = g.GenreId\nGROUP BY g.Name\nORDER BY AvgTrackLength DESC\nLIMIT 5;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi &amp; Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]\n================================== Ai Message ==================================\n\nOn average, the genre with the longest tracks is \"Sci Fi &amp; Fantasy\" with an average track length of approximately 2,911,783 milliseconds.\n</pre> In\u00a0[21]: Copied! <pre># question = \"Which are the top-3 genres having, on average, the longest tracks? Express the duration in minutes\"\nquestion = \"What are the most popular songs by AC/DC?\"\n\ninputs = [HumanMessage(content=question)]\nresult = workflow.invoke({\"messages\": inputs})\ndisplay(Markdown(result['messages'][-1].content))\n</pre> # question = \"Which are the top-3 genres having, on average, the longest tracks? Express the duration in minutes\" question = \"What are the most popular songs by AC/DC?\"  inputs = [HumanMessage(content=question)] result = workflow.invoke({\"messages\": inputs}) display(Markdown(result['messages'][-1].content)) <p>The most popular songs by AC/DC, based on the number of times they appear in invoices, are:</p> <ol> <li>\"Snowballed\"</li> <li>\"Overdose\"</li> <li>\"Inject The Venom\"</li> <li>\"Spellbound\"</li> <li>\"Put The Finger On You\"</li> </ol> <p></p> In\u00a0[22]: Copied! <pre># conn = sqlite3.connect(\"./data/netflixdb.sqlite\")\n# cursor = conn.cursor()\n\n# table_names = netflix_db.get_usable_table_names()\n# for table in table_names:\n#     if table.startswith(\"BATCH\"):\n#         cursor.execute(\"DROP TABLE IF EXISTS \"+table)\n#         conn.commit()\n#         print(table, \"removed\")\n\n# conn.close()\n</pre> # conn = sqlite3.connect(\"./data/netflixdb.sqlite\") # cursor = conn.cursor()  # table_names = netflix_db.get_usable_table_names() # for table in table_names: #     if table.startswith(\"BATCH\"): #         cursor.execute(\"DROP TABLE IF EXISTS \"+table) #         conn.commit() #         print(table, \"removed\")  # conn.close() In\u00a0[23]: Copied! <pre>netflix_db = SQLDatabase.from_uri(\"sqlite:///data/netflixdb.sqlite\")\n\nprint(f\"Dialect: {netflix_db.dialect}\")\nprint(f\"Available tables: {netflix_db.get_usable_table_names()}\")\n\n#\u00a0-- Top 10 movies (English)\nquestion = \"Which are the Top 10 movies (in English)?\"\n\nsql_query = \"\"\"select v.view_rank, m.title, v.hours_viewed, m.runtime, v.views, v.cumulative_weeks_in_top10\nfrom view_summary v\ninner join movie m on m.id = v.movie_id\nwhere duration = 'WEEKLY'\n  and m.locale = 'en'\norder by v.view_rank;\"\"\"\n\nprint(f'Sample output: {netflix_db.run(sql_query)}')\n</pre> netflix_db = SQLDatabase.from_uri(\"sqlite:///data/netflixdb.sqlite\")  print(f\"Dialect: {netflix_db.dialect}\") print(f\"Available tables: {netflix_db.get_usable_table_names()}\")  #\u00a0-- Top 10 movies (English) question = \"Which are the Top 10 movies (in English)?\"  sql_query = \"\"\"select v.view_rank, m.title, v.hours_viewed, m.runtime, v.views, v.cumulative_weeks_in_top10 from view_summary v inner join movie m on m.id = v.movie_id where duration = 'WEEKLY'   and m.locale = 'en' order by v.view_rank;\"\"\"  print(f'Sample output: {netflix_db.run(sql_query)}') <pre>Dialect: sqlite\nAvailable tables: ['episode', 'movie', 'season', 'tv_show', 'view_summary']\nSample output: [(1, 'Mother of the Bride', 38100000, 90, 25400000, 2), (1, 'Mother of the Bride', 40000000, 90, 26700000, 1), (1, 'Atlas', 63100000, 120, 31600000, 2), (1, 'Atlas', 56300000, 120, 28200000, 1), (1, 'Irish Wish', 40000000, 94, 25500000, 2), (1, 'Despicable Me 3', 23900000, 90, 15900000, 1), (1, 'Rebel Moon \u2014 Part Two: The Scargiver', 38800000, 124, 18800000, 2), (1, 'Rebel Moon \u2014 Part Two: The Scargiver', 44200000, 124, 21400000, 1), (1, 'Rebel Moon \u2014 Part One: A Child of Fire', 77000000, 136, 34000000, 2), (1, 'Rebel Moon \u2014 Part One: A Child of Fire', 54100000, 136, 23900000, 1), (1, 'The Equalizer 3', 26800000, 109, 14800000, 1), (1, 'Mea Culpa', 32000000, 120, 16000000, 1), (1, 'What Jennifer Did', 26100000, 87, 18000000, 1), (1, 'Code 8 Part II', 33400000, 100, 20000000, 1), (1, 'Leave the World Behind', 106200000, 142, 44900000, 2), (1, 'Leave the World Behind', 98700000, 142, 41700000, 1), (1, 'Players', 28500000, 105, 16300000, 1), (1, 'A Family Affair', 50900000, 114, 26800000, 1), (1, 'Extraction 2', 36200000, 124, 17500000, 3), (1, 'Extraction 2', 87300000, 124, 42200000, 2), (1, 'Extraction 2', 88400000, 124, 42800000, 1), (1, 'The Greatest Night in Pop', 19200000, 97, 11900000, 1), (1, 'Unfrosted', 11300000, 96, 7100000, 1), (1, 'Reptile', 32300000, 136, 14200000, 3), (1, 'Reptile', 45100000, 136, 19900000, 2), (1, 'Reptile', 40200000, 136, 17700000, 1), (1, 'The Killer', 44500000, 120, 22300000, 2), (1, 'The Killer', 55700000, 120, 27900000, 1), (1, 'Hidden Strike', 40500000, 103, 23600000, 2), (1, 'Hidden Strike', 37800000, 103, 22000000, 1), (1, 'The Out-Laws', 19500000, 97, 12100000, 3), (1, 'The Out-Laws', 46300000, 97, 28600000, 2), (1, 'The Out-Laws', 31700000, 97, 19600000, 1), (1, 'Love at First Sight', 23000000, 91, 15200000, 2), (1, 'Love at First Sight', 21400000, 91, 14100000, 1), (1, 'Locked In', 28800000, 97, 17800000, 1), (1, 'The Monkey King', 23200000, 97, 14400000, 2), (1, 'KPop Demon Hunters', 43300000, 100, 26000000, 9), (1, 'KPop Demon Hunters', 43000000, 100, 25800000, 5), (1, 'KPop Demon Hunters', 40400000, 100, 24200000, 4), (1, 'KPop Demon Hunters', 40300000, 100, 24200000, 2), (1, 'My Oxford Year', 51300000, 112, 27200000, 2), (1, 'Happy Gilmore 2', 80300000, 118, 40800000, 2), (1, 'Happy Gilmore 2', 91900000, 118, 46700000, 1), (1, 'The Old Guard 2', 66800000, 106, 37500000, 1), (1, 'STRAW', 36000000, 108, 20000000, 3), (1, 'STRAW', 88000000, 108, 48900000, 2), (1, 'STRAW', 45500000, 108, 25300000, 1), (1, 'Nonnas', 38000000, 114, 20000000, 2), (1, 'Nonnas', 29000000, 114, 15300000, 1), (1, 'Fear Street: Prom Queen', 12700000, 90, 8500000, 2), (1, 'Fear Street: Prom Queen', 16000000, 90, 10700000, 1), (1, 'Havoc', 47300000, 106, 26500000, 2), (1, 'Havoc', 53100000, 106, 29800000, 1), (1, 'The Life List', 15600000, 124, 7500000, 4), (1, 'The Life List', 27200000, 124, 13100000, 3), (1, 'The Life List', 60800000, 124, 29200000, 2), (1, 'The Life List', 50800000, 124, 24400000, 1), (1, 'Back in Action', 35000000, 114, 18400000, 3), (1, 'Back in Action', 88900000, 114, 46800000, 2), (1, 'Back in Action', 88900000, 114, 46800000, 1), (1, 'The Electric State', 48000000, 127, 22500000, 2), (1, 'The Electric State', 53800000, 127, 25200000, 1), (1, 'Plankton: The Movie', 21000000, 88, 14300000, 1), (1, 'Venom: The Last Dance', 23300000, 109, 12800000, 1), (1, 'La Dolce Villa', 23300000, 99, 14100000, 2), (1, 'La Dolce Villa', 32700000, 99, 19800000, 1), (1, 'Kinda Pregnant', 41900000, 100, 25100000, 1), (1, 'The Secret Life of Pets 2', 21500000, 85, 15000000, 5), (1, 'Carry-On', 34700000, 120, 17400000, 4), (1, 'Carry-On', 69800000, 120, 34900000, 3), (1, 'Carry-On', 110200000, 120, 55100000, 2), (1, 'Carry-On', 84000000, 120, 42000000, 1), (1, 'Our Little Secret', 39500000, 100, 23500000, 2), (1, 'Our Little Secret', 54600000, 100, 32400000, 1), (1, 'The Merry Gentlemen', 21600000, 88, 14700000, 1), (1, 'Hot Frosty', 24600000, 91, 16000000, 1), (1, 'Meet Me Next Christmas', 32000000, 106, 18100000, 1), (1, \"Don't Move\", 42900000, 91, 28000000, 2), (1, \"Don't Move\", 31000000, 91, 20200000, 1), (1, 'Lonely Planet', 21900000, 96, 13700000, 2), (1, 'The Menendez Brothers', 44700000, 118, 22700000, 1), (1, 'Rebel Ridge', 19500000, 132, 8900000, 4), (1, 'Rebel Ridge', 84900000, 132, 38600000, 2), (1, 'Rebel Ridge', 68600000, 132, 31200000, 1), (1, \"Miss Peregrine's Home for Peculiar Children\", 15100000, 127, 7100000, 1), (1, 'Uglies', 45600000, 102, 26800000, 2), (1, 'The Union', 73800000, 109, 40600000, 2), (1, 'The Union', 60100000, 109, 33100000, 1), (1, 'Incoming', 25600000, 91, 16900000, 2), (1, 'Saving Bikini Bottom: The Sandy Cheeks Movie', 19500000, 87, 13400000, 2), (1, 'Saving Bikini Bottom: The Sandy Cheeks Movie', 18500000, 87, 12800000, 1), (1, 'Find Me Falling', 25200000, 94, 16100000, 2), (1, 'Find Me Falling', 22500000, 94, 14400000, 1), (1, 'Beverly Hills Cop: Axel F', 43700000, 118, 22200000, 2), (1, 'Beverly Hills Cop: Axel F', 80700000, 118, 41000000, 1), (1, 'Trigger Warning', 45900000, 106, 25700000, 1), (1, 'Hit Man', 26500000, 115, 13700000, 2), (1, 'Hit Man', 20800000, 115, 10800000, 1), (1, 'Damsel', 93200000, 109, 50800000, 2), (1, 'Damsel', 64800000, 109, 35300000, 1), (1, 'Heart of the Hunter', 19500000, 106, 10900000, 2), (1, 'Heart of the Hunter', 19700000, 106, 11000000, 1), (1, 'Lift', 31000000, 106, 17400000, 3), (1, 'Lift', 65400000, 106, 36700000, 2), (1, 'Lift', 58500000, 106, 32800000, 1), (1, 'Leo', 42100000, 106, 23600000, 2), (1, 'Leo', 61700000, 106, 34600000, 1), (1, 'Old Dads', 28200000, 103, 16300000, 2), (1, 'Old Dads', 23100000, 103, 13300000, 1), (1, 'You Are So Not Invited to My Bat Mitzvah', 15300000, 103, 8800000, 3), (1, 'You Are So Not Invited to My Bat Mitzvah', 37900000, 103, 21900000, 2), (1, 'Heart of Stone', 74000000, 126, 35200000, 2), (1, 'Heart of Stone', 69600000, 126, 33100000, 1), (2, 'Atlas', 17800000, 120, 8900000, 3), (2, 'Irish Wish', 16400000, 94, 10500000, 3), (2, 'Irish Wish', 30600000, 94, 19500000, 1), (2, 'Woody Woodpecker Goes to Camp', 31300000, 100, 18800000, 2), (2, 'Woody Woodpecker Goes to Camp', 19600000, 100, 11800000, 1), (2, 'Rebel Moon \u2014 Part One: A Child of Fire', 25100000, 136, 11100000, 3), (2, 'Mea Culpa', 36900000, 120, 18500000, 2), (2, 'Orion and the Dark', 19700000, 93, 12700000, 2), (2, 'Orion and the Dark', 15500000, 93, 10000000, 1), (2, 'Code 8 Part II', 19800000, 100, 11900000, 2), (2, 'Leave the World Behind', 34600000, 142, 14600000, 4), (2, 'Leave the World Behind', 46700000, 142, 19700000, 3), (2, 'Lover, Stalker, Killer', 21900000, 91, 14400000, 2), (2, 'Players', 13000000, 105, 7400000, 2), (2, 'Ice Age: Collision Course', 14500000, 97, 9000000, 2), (2, 'A Family Affair', 22000000, 114, 11600000, 3), (2, 'A Family Affair', 60600000, 114, 31900000, 2), (2, 'Extraction 2', 18400000, 124, 8900000, 4), (2, 'Chicken Run: Dawn of the Nugget', 20300000, 102, 11900000, 1), (2, 'Unfrosted', 14100000, 96, 8800000, 2), (2, 'Mindcage', 14700000, 97, 9100000, 1), (2, 'Family Switch', 32900000, 106, 18600000, 2), (2, 'Family Switch', 39200000, 106, 22200000, 1), (2, 'Ultraman: Rising', 16000000, 121, 7900000, 2), (2, 'Hidden Strike', 13300000, 103, 7700000, 3), (2, 'Transporter 2', 8500000, 88, 5800000, 1), (2, 'Puss in Boots: The Last Wish', 14800000, 102, 8700000, 4), (2, 'Miraculous: Ladybug &amp; Cat Noir, The Movie', 20800000, 102, 12200000, 1), (2, 'Locked In', 21400000, 97, 13200000, 2), (2, 'Extraction', 27200000, 118, 13800000, 2), (2, 'Extraction', 18800000, 118, 9600000, 1), (2, 'The Monkey King', 10600000, 97, 6600000, 3), (2, 'The Monkey King', 13100000, 97, 8100000, 1), (2, 'Four Brothers', 13500000, 109, 7400000, 2), (2, 'Fair Play', 26900000, 115, 14000000, 2), (2, 'Fair Play', 24200000, 115, 12600000, 1), (2, 'Happiness for Beginners', 23100000, 106, 13100000, 2), (2, 'The Perfect Find', 18400000, 100, 11000000, 2), (2, 'Now You See Me', 9300000, 115, 4900000, 1), (2, '65', 10500000, 93, 6800000, 1), (2, 'Inside Man: Most Wanted', 21800000, 106, 12300000, 1), (2, 'The Devil on Trial', 11000000, 82, 8000000, 1), (2, 'Best. Christmas. Ever!', 18200000, 82, 13300000, 2), (2, 'Best. Christmas. Ever!', 22300000, 82, 16300000, 1), (2, 'KPop Demon Hunters', 43100000, 100, 25900000, 8), (2, 'KPop Demon Hunters', 43800000, 100, 26300000, 7), (2, 'KPop Demon Hunters', 43900000, 100, 26300000, 6), (2, 'KPop Demon Hunters', 37800000, 100, 22700000, 3), (2, 'KPop Demon Hunters', 15400000, 100, 9200000, 1), (2, 'Night Always Comes', 20800000, 109, 11300000, 1), (2, 'Madea\u2019s Destination Wedding', 28600000, 103, 16500000, 2), (2, 'Madea\u2019s Destination Wedding', 33100000, 103, 19100000, 1), (2, 'Trainwreck: Poop Cruise', 19300000, 55, 21100000, 1), (2, 'Titan: The OceanGate Submersible Disaster', 32400000, 112, 17400000, 1), (2, 'The Wild Robot', 9900000, 102, 5800000, 2), (2, 'A Deadly American Marriage', 22000000, 103, 12800000, 2), (2, 'Where the Crawdads Sing', 12200000, 124, 5900000, 4), (2, 'Despicable Me 4', 15900000, 94, 10100000, 2), (2, 'Despicable Me 4', 13800000, 94, 8800000, 1), (2, 'Life or Something Like It', 9600000, 103, 5500000, 2), (2, 'Back in Action', 17400000, 114, 9200000, 4), (2, 'Meet the Khumalos', 8800000, 93, 5700000, 2), (2, 'Alpha', 9200000, 97, 5700000, 2), (2, 'One of Them Days', 12400000, 97, 7700000, 1), (2, 'PAW Patrol: The Mighty Movie', 10300000, 88, 7000000, 1), (2, 'The Electric State', 16300000, 127, 7600000, 3), (2, 'The Twister: Caught in the Storm', 25200000, 88, 17000000, 1), (2, 'Plankton: The Movie', 19600000, 88, 13400000, 2), (2, 'Uncharted', 20300000, 115, 10500000, 4), (2, 'Kinda Pregnant', 22800000, 100, 13700000, 2), (2, 'The Secret Life of Pets 2', 9600000, 85, 6700000, 7), (2, 'The Secret Life of Pets 2', 12700000, 85, 8900000, 6), (2, 'The Sentinel', 14200000, 108, 7900000, 1), (2, 'Carry-On', 14300000, 120, 7200000, 5), (2, 'The Six Triple Eight', 23500000, 130, 10800000, 3), (2, 'The Six Triple Eight', 50400000, 130, 23300000, 2), (2, 'The Six Triple Eight', 30900000, 130, 14300000, 1), (2, 'That Christmas', 27400000, 96, 17100000, 2), (2, 'That Christmas', 29200000, 96, 18300000, 1), (2, 'Spellbound', 28000000, 109, 15300000, 2), (2, 'Hot Frosty', 19500000, 91, 12700000, 2), (2, 'Meet Me Next Christmas', 20800000, 106, 11800000, 2), (2, 'Time Cut', 22800000, 91, 14900000, 2), (2, 'Time Cut', 35100000, 91, 22900000, 1), (2, 'Woman of the Hour', 21100000, 96, 13200000, 2), (2, 'Woman of the Hour', 15800000, 96, 9900000, 1), (2, 'The Predator', 23300000, 108, 12900000, 1), (2, 'Rebel Ridge', 36700000, 132, 16700000, 3), (2, 'Jailbreak: Love on the Run', 9600000, 88, 6500000, 2), (2, 'Uglies', 14800000, 102, 8700000, 3), (2, 'Uglies', 35400000, 102, 20800000, 1), (2, 'The Deliverance', 41100000, 112, 21800000, 2), (2, 'The Union', 30600000, 109, 16800000, 3), (2, 'Incoming', 15000000, 91, 9900000, 1), (2, 'Kingsman: The Secret Service', 33000000, 130, 15200000, 2), (2, 'Ferdinand', 13300000, 109, 7300000, 1), (2, 'Trolls Band Together', 15800000, 91, 10300000, 1), (2, 'Ghostbusters: Frozen Empire', 19600000, 115, 10200000, 1), (2, 'Anyone But You', 11600000, 103, 6700000, 2), (2, 'Anyone But You', 18300000, 103, 10600000, 1), (2, 'Trigger Warning', 46900000, 106, 26300000, 2), (2, 'Madame Web', 20800000, 115, 10800000, 1), (2, 'Thelma the Unicorn', 17400000, 97, 10700000, 2), (2, 'Damsel', 35700000, 109, 19500000, 3), (2, 'The Little Things', 18600000, 127, 8700000, 1), (2, 'The Legend of Tarzan', 14400000, 109, 7900000, 1), (2, 'Pain Hustlers', 31700000, 124, 15200000, 2), (2, 'Pain Hustlers', 29300000, 124, 14100000, 1), (2, 'Love is in the Air', 17300000, 88, 11700000, 1), (2, 'Spy Kids: Armageddon', 11000000, 97, 6700000, 1), (2, 'You Are So Not Invited to My Bat Mitzvah', 7400000, 103, 4300000, 4), (2, 'Choose Love', 9000000, 76, 7000000, 2), (2, 'Heart of Stone', 28500000, 126, 13600000, 3), (2, 'Unknown: Cave of Bones', 10300000, 94, 6500000, 1), (3, 'Despicable Me 3', 10300000, 90, 6900000, 7), (3, 'Despicable Me 3', 11900000, 90, 7900000, 6), (3, 'Despicable Me 3', 15900000, 90, 10600000, 2), (3, 'Woody Woodpecker Goes to Camp', 15300000, 100, 9200000, 3), (3, 'Shrek', 11000000, 90, 7300000, 11), (3, 'Shrek', 12800000, 90, 8500000, 9), (3, 'Rebel Moon \u2014 Part Two: The Scargiver', 12300000, 124, 6000000, 3), (3, 'The Equalizer 3', 10400000, 109, 5700000, 2), (3, 'What Jennifer Did', 23700000, 87, 16300000, 2), (3, 'Leave the World Behind', 18700000, 142, 7900000, 5), (3, 'Lover, Stalker, Killer', 15100000, 91, 10000000, 1), (3, 'Spaceman', 19600000, 109, 10800000, 2), (3, 'Spaceman', 16000000, 109, 8800000, 1), (3, 'Ice Age: Collision Course', 7700000, 97, 4800000, 3), (3, 'The Secret Life of Pets', 6200000, 87, 4300000, 8), (3, 'The Secret Life of Pets', 7600000, 87, 5200000, 7), (3, 'Chicken Run: Dawn of the Nugget', 25400000, 102, 14900000, 2), (3, 'Scoop', 14600000, 103, 8500000, 2), (3, 'Disturbia', 13500000, 105, 7700000, 1), (3, 'The Kitchen', 14000000, 109, 7700000, 2), (3, 'The Kitchen', 10200000, 109, 5600000, 1), (3, 'Minions: The Rise of Gru', 10200000, 88, 7000000, 7), (3, 'The Killer', 17500000, 120, 8800000, 3), (3, 'Puss in Boots: The Last Wish', 9400000, 102, 5500000, 1), (3, 'Miraculous: Ladybug &amp; Cat Noir, The Movie', 21200000, 102, 12500000, 2), (3, 'Nimona', 11500000, 103, 6700000, 2), (3, 'Cold Pursuit', 17300000, 118, 8800000, 2), (3, 'The Black Book', 23100000, 124, 11200000, 2), (3, 'Monster Hunter', 10500000, 103, 6100000, 1), (3, 'In Time', 10400000, 109, 5700000, 2), (3, 'Infinite', 18400000, 106, 10400000, 3), (3, 'They Cloned Tyrone', 23000000, 124, 11100000, 2), (3, 'They Cloned Tyrone', 13100000, 124, 6300000, 1), (3, 'Best. Christmas. Ever!', 9600000, 82, 7000000, 3), (3, 'Land of Bad', 16600000, 114, 8700000, 1), (3, 'Untold: Johnny Football', 9200000, 72, 7700000, 1), (3, 'Rise of the Guardians', 9000000, 100, 5400000, 1), (3, 'My Oxford Year', 20800000, 112, 11000000, 3), (3, 'My Oxford Year', 46300000, 112, 24600000, 1), (3, 'Happy Gilmore 2', 28900000, 118, 14700000, 3), (3, 'Happy Gilmore', 17500000, 91, 11400000, 2), (3, 'The Old Guard 2', 28500000, 106, 16000000, 2), (3, 'Trainwreck: Balloon Boy', 6200000, 52, 7000000, 1), (3, 'Plane', 11200000, 106, 6300000, 1), (3, 'STRAW', 14300000, 108, 7900000, 4), (3, 'Sing', 10000000, 108, 5600000, 11), (3, 'Trainwreck: The Astroworld Tragedy', 9800000, 81, 7300000, 2), (3, 'Bee Movie', 7300000, 91, 4800000, 2), (3, 'Mission: Impossible - Dead Reckoning', 15300000, 163, 5600000, 1), (3, 'Nonnas', 16000000, 114, 8400000, 3), (3, 'Aquaman and the Lost Kingdom', 12000000, 124, 5800000, 1), (3, 'Havoc', 9900000, 106, 5600000, 4), (3, 'A Deadly American Marriage', 16900000, 103, 9800000, 1), (3, 'Woody Woodpecker', 5200000, 91, 3400000, 4), (3, 'Woody Woodpecker', 5300000, 91, 3500000, 2), (3, 'The Life List', 11500000, 124, 5500000, 5), (3, 'Life or Something Like It', 9600000, 103, 5500000, 1), (3, 'The Croods', 6800000, 97, 4200000, 6), (3, 'Alpha', 8900000, 97, 5500000, 1), (3, 'Kraven the Hunter', 15000000, 127, 7100000, 1), (3, 'Con Mum', 9400000, 88, 6300000, 1), (3, 'Den of Thieves 2: Pantera', 20100000, 144, 8400000, 1), (3, 'Chaos: The Manson Murders', 8100000, 97, 5000000, 1), (3, 'Aftermath', 15600000, 94, 9900000, 1), (3, 'The Secret Life of Pets 2', 8200000, 85, 5700000, 4), (3, 'Under the Boardwalk', 8400000, 87, 5800000, 1), (3, 'Ma', 11500000, 99, 7000000, 2), (3, \"Dr. Seuss' The Grinch\", 16300000, 85, 11400000, 10), (3, \"Dr. Seuss' The Grinch\", 18500000, 85, 12900000, 8), (3, 'That Christmas', 19000000, 96, 11900000, 4), (3, 'Mary Mother of Jesus', 24700000, 112, 13200000, 2), (3, 'Spellbound', 15500000, 109, 8500000, 1), (3, 'The Merry Gentlemen', 14400000, 88, 9800000, 2), (3, \"Don't Move\", 14000000, 91, 9100000, 3), (3, \"Hijack '93\", 10600000, 87, 7300000, 2), (3, 'Lonely Planet', 17600000, 96, 11000000, 1), (3, 'Sweet Bobby: My Catfish Nightmare', 9800000, 82, 7200000, 2), (3, 'Sweet Bobby: My Catfish Nightmare', 9600000, 82, 7000000, 1), (3, 'Jailbreak: Love on the Run', 11200000, 88, 7600000, 1), (3, 'Gifted', 16900000, 102, 9900000, 1), (3, 'Into the Fire: The Lost Daughter', 20500000, 151, 8100000, 1), (3, 'The Deliverance', 27300000, 112, 14500000, 1), (3, 'The Union', 15100000, 109, 8300000, 4), (3, 'Gemini Man', 14300000, 117, 7300000, 5), (3, 'Gemini Man', 10300000, 117, 5300000, 4), (3, 'Untold: The Murder of Air McNair', 8200000, 58, 8300000, 1), (3, 'Kingsman: The Secret Service', 15000000, 130, 6900000, 1), (3, 'Kingsman: The Golden Circle', 22800000, 142, 9600000, 2), (3, 'Trolls Band Together', 12400000, 91, 8100000, 2), (3, \"Don't Breathe 2\", 9700000, 97, 5900000, 2), (3, 'Paw Patrol: The Movie', 6700000, 85, 4700000, 10), (3, 'Paw Patrol: The Movie', 9400000, 85, 6600000, 4), (3, 'Trigger Warning', 14200000, 106, 8000000, 3), (3, 'Kiss the Girls', 13700000, 115, 7100000, 1), (3, 'Dracula Untold', 10300000, 91, 6700000, 1), (3, 'Ice Age: Dawn of the Dinosaurs', 12900000, 94, 8100000, 2), (3, 'Ice Age: Dawn of the Dinosaurs', 15100000, 94, 9500000, 1), (3, 'Damsel', 18900000, 109, 10300000, 4), (3, 'The Maze Runner', 12700000, 112, 6700000, 1), (3, 'Shooter', 11700000, 124, 5600000, 5), (3, 'Alone', 5900000, 97, 3600000, 1), (3, 'Einstein and the Bomb', 9200000, 76, 7200000, 2), (3, 'Lift', 14700000, 106, 8200000, 4), (3, 'Leo', 16800000, 106, 9400000, 4), (3, 'Leo', 25400000, 106, 14200000, 3), (3, 'Pretty Woman', 16600000, 118, 8400000, 1), (3, 'F9: The Fast Saga', 24900000, 142, 10400000, 3), (3, 'No Hard Feelings', 22700000, 103, 13100000, 2), (3, 'Love is in the Air', 10700000, 88, 7200000, 2), (3, 'You Are So Not Invited to My Bat Mitzvah', 21300000, 103, 12300000, 1), (3, 'Heart of Stone', 13700000, 126, 6500000, 4), (3, \"The Pope's Exorcist\", 10400000, 103, 6000000, 1), (3, 'Run Rabbit Run', 14200000, 100, 8400000, 1), (3, 'Take Care of Maya', 16000000, 103, 9200000, 1), (4, 'Mother of the Bride', 9300000, 90, 6200000, 4), (4, 'Mother of the Bride', 14000000, 90, 9300000, 3), (4, 'Minions', 9000000, 91, 5900000, 12), (4, 'Minions', 14500000, 91, 9600000, 4), (4, 'Despicable Me 3', 10100000, 90, 6700000, 3), (4, 'Woody Woodpecker Goes to Camp', 9400000, 100, 5600000, 4), (4, 'Shrek', 10500000, 90, 7000000, 12), (4, 'Rebel Moon \u2014 Part One: A Child of Fire', 13700000, 136, 6000000, 6), (4, 'Code 8 Part II', 6000000, 100, 3600000, 3), (4, 'Extraction 2', 10800000, 124, 5200000, 5), (4, 'The Secret Life of Pets', 9000000, 87, 6200000, 6), (4, 'Chicken Run: Dawn of the Nugget', 14000000, 102, 8200000, 3), (4, 'Shrek 2', 8600000, 93, 5500000, 4), (4, 'Family Switch', 16600000, 106, 9400000, 3), (4, 'Reptile', 15800000, 136, 7000000, 4), (4, 'Minions: The Rise of Gru', 11400000, 88, 7800000, 8), (4, 'Exodus: Gods and Kings', 18600000, 150, 7400000, 1), (4, 'The Emoji Movie', 11200000, 87, 7700000, 1), (4, 'Puss in Boots: The Last Wish', 10600000, 102, 6200000, 2), (4, 'Ford v. Ferrari', 11200000, 153, 4400000, 2), (4, 'Locked In', 8200000, 97, 5100000, 3), (4, 'Extraction', 11800000, 118, 6000000, 3), (4, 'Madagascar: Escape 2 Africa', 9200000, 91, 6100000, 2), (4, 'The Old Guard', 20000000, 126, 9500000, 2), (4, 'Jack Reacher: Never Go Back', 12700000, 118, 6500000, 5), (4, 'Cold Pursuit', 13200000, 118, 6700000, 1), (4, 'The Black Book', 11600000, 124, 5600000, 1), (4, 'Mysteries of the Terracotta Warriors', 6800000, 78, 5200000, 1), (4, 'Happiness for Beginners', 16500000, 106, 9300000, 1), (4, 'The Equalizer 2', 6900000, 121, 3400000, 2), (4, 'What Men Want', 11600000, 117, 5900000, 1), (4, 'The Perfect Find', 11700000, 100, 7000000, 1), (4, 'Street Kings', 10300000, 109, 5700000, 2), (4, 'M3GAN', 6400000, 102, 3800000, 1), (4, 'American Made', 9300000, 115, 4900000, 1), (4, 'Geostorm', 8900000, 109, 4900000, 1), (4, 'Sly', 9500000, 96, 5900000, 2), (4, 'Poisoned: The Dirty Truth About Your Food', 7200000, 84, 5100000, 1), (4, 'Land of Bad', 13100000, 114, 6900000, 2), (4, 'Knight and Day', 9500000, 109, 5200000, 2), (4, 'Half Brothers', 7300000, 96, 4600000, 1), (4, 'Happy Gilmore 2', 13800000, 118, 7000000, 4), (4, 'Megamind', 12200000, 97, 7500000, 1), (4, 'Happy Gilmore', 17400000, 91, 11300000, 3), (4, 'Lost on a Mountain in Maine', 12300000, 94, 7800000, 2), (4, 'The Old Guard 2', 11000000, 106, 6200000, 3), (4, 'The Boss Baby', 5800000, 97, 3600000, 8), (4, 'Trainwreck: The Real Project X', 5900000, 48, 7400000, 1), (4, 'The Intern', 11300000, 121, 5600000, 1), (4, 'Sing', 10700000, 108, 5900000, 14), (4, 'Trainwreck: The Astroworld Tragedy', 8400000, 81, 6200000, 1), (4, 'Titan: The OceanGate Submersible Disaster', 12500000, 112, 6700000, 2), (4, 'The Super Mario Bros. Movie', 11400000, 91, 7400000, 4), (4, 'The Super Mario Bros. Movie', 20300000, 91, 13200000, 2), (4, 'Nonnas', 10700000, 114, 5600000, 4), (4, 'Havoc', 16300000, 106, 9100000, 3), (4, 'Untold: The Fall of Favre', 7300000, 64, 6800000, 1), (4, 'Untold: The Liver King', 5700000, 67, 5000000, 1), (4, 'Woody Woodpecker', 8300000, 91, 5500000, 3), (4, 'Oklahoma City Bombing: American Terror', 6300000, 85, 4400000, 1), (4, 'Meet the Khumalos', 7500000, 93, 4800000, 1), (4, 'Kraven the Hunter', 12800000, 127, 6000000, 2), (4, 'Den of Thieves 2: Pantera', 14300000, 144, 6000000, 2), (4, 'Trap', 11800000, 105, 6700000, 1), (4, 'Venom: The Last Dance', 9000000, 109, 5000000, 2), (4, 'To Catch a Killer', 13300000, 118, 6700000, 1), (4, 'The Secret Life of Pets 2', 6700000, 85, 4700000, 8), (4, 'Father Stu', 7400000, 124, 3600000, 3), (4, \"Dr. Seuss' The Grinch\", 7700000, 85, 5400000, 12), (4, \"Dr. Seuss' The Grinch\", 17000000, 85, 11900000, 11), (4, \"Dr. Seuss' The Grinch\", 18900000, 85, 13200000, 9), (4, \"Dr. Seuss' The Grinch\", 4900000, 85, 3400000, 2), (4, \"Dr. Seuss' The Grinch\", 5900000, 85, 4100000, 1), (4, 'That Christmas', 17800000, 96, 11100000, 3), (4, 'Mary Mother of Jesus', 21200000, 112, 11400000, 1), (4, 'Alita: Battle Angel', 12700000, 123, 6200000, 2), (4, 'Martha', 12000000, 115, 6200000, 2), (4, 'Martha', 11800000, 115, 6100000, 1), (4, 'The Menendez Brothers', 13500000, 118, 6900000, 2), (4, 'Bad Boys: Ride or Die', 20200000, 115, 10400000, 1), (4, 'Rebel Ridge', 12000000, 132, 5500000, 5), (4, 'The Garfield Movie', 11700000, 100, 7000000, 2), (4, 'Into the Fire: The Lost Daughter', 18800000, 151, 7400000, 2), (4, 'The Deliverance', 13100000, 112, 7000000, 3), (4, 'Dead Sea', 9900000, 88, 6700000, 1), (4, 'Migration', 11500000, 82, 8300000, 1), (4, 'T\u00f2kunb\u1ecd\u0300', 9900000, 112, 5300000, 2), (4, 'Trolls Band Together', 9000000, 91, 5900000, 3), (4, 'Beverly Hills Cop: Axel F', 16400000, 118, 8300000, 3), (4, 'Paw Patrol: The Movie', 7500000, 85, 5200000, 9), (4, 'The Equalizer', 9300000, 132, 4200000, 2), (4, 'Marry Me', 8100000, 112, 4300000, 1), (4, 'Inheritance', 9500000, 111, 5100000, 4), (4, 'Ice Age: Dawn of the Dinosaurs', 6500000, 94, 4100000, 3), (4, 'Thelma the Unicorn', 12000000, 97, 7300000, 1), (4, 'Barbarian', 8000000, 103, 4600000, 2), (4, 'Glass', 11000000, 129, 5100000, 2), (4, 'Glass', 13400000, 129, 6200000, 1), (4, 'Anna', 11700000, 118, 5900000, 1), (4, 'The Accountant', 13200000, 127, 6200000, 1), (4, 'Code 8', 9400000, 99, 5700000, 2), (4, 'Code 8', 12800000, 99, 7800000, 1), (4, 'Einstein and the Bomb', 10900000, 76, 8500000, 1), (4, 'Deep Fear', 10800000, 85, 7600000, 2), (4, 'The Hill', 13400000, 127, 6300000, 1), (4, 'Stillwater', 12700000, 139, 5500000, 1), (4, 'The Proposal', 8800000, 106, 4900000, 1), (4, 'Catering Christmas', 6600000, 85, 4600000, 1), (4, 'See You on Venus', 11300000, 94, 7100000, 2), (4, 'Spider-Man: Across the Spider-Verse', 20100000, 139, 8600000, 1), (4, 'Life of a King', 10600000, 100, 6300000, 1), (4, 'Spy Kids: Armageddon', 14700000, 97, 9000000, 2), (4, 'Choose Love', 8300000, 76, 6500000, 1), (4, 'Run Rabbit Run', 9900000, 100, 5900000, 2), (5, 'Minions', 9300000, 91, 6100000, 11), (5, 'Minions', 10700000, 91, 7100000, 5), (5, 'Despicable Me 3', 7100000, 90, 4700000, 4), (5, 'Shrek', 7700000, 90, 5100000, 10), (5, 'Rebel Moon \u2014 Part One: A Child of Fire', 12500000, 136, 5500000, 5), (5, 'The Equalizer 3', 8300000, 109, 4600000, 3), (5, 'Mea Culpa', 10200000, 120, 5100000, 3), (5, 'What Jennifer Did', 7500000, 87, 5200000, 3), (5, 'Sonic the Hedgehog 2', 8200000, 124, 4000000, 1), (5, 'A Family Affair', 10500000, 114, 5500000, 4), (5, 'Ruby Gillman, Teenage Kraken', 9800000, 91, 6500000, 1), (5, 'Shrek 2', 9400000, 93, 6100000, 5), (5, 'The Beautiful Game', 12400000, 126, 5900000, 1), (5, 'Disturbia', 14100000, 105, 8100000, 2), (5, 'Ultraman: Rising', 7400000, 121, 3700000, 3), (5, 'Hidden Strike', 6000000, 103, 3500000, 4), (5, 'Puss in Boots: The Last Wish', 8600000, 102, 5100000, 5), (5, 'Miraculous: Ladybug &amp; Cat Noir, The Movie', 9800000, 102, 5800000, 3), (5, 'The Casagrandes Movie', 6100000, 85, 4300000, 2), (5, 'The Casagrandes Movie', 5700000, 85, 4000000, 1), (5, 'Alvin and the Chipmunks: Chipwrecked', 5700000, 87, 3900000, 2), (5, 'Fifty Shades Freed', 5900000, 105, 3400000, 1), (5, 'NYAD', 11700000, 121, 5800000, 2), (5, 'Nimona', 8000000, 103, 4700000, 3), (5, 'The Old Guard', 10300000, 126, 4900000, 3), (5, 'The Favourite', 8700000, 121, 4300000, 1), (5, \"Tyler Perry's The Single Moms Club\", 8500000, 111, 4600000, 1), (5, 'Sniper: Rogue Mission', 7700000, 96, 4800000, 1), (5, 'Law Abiding Citizen', 10500000, 109, 5800000, 1), (5, 'Missing: The Lucie Blackman Case', 6900000, 82, 5000000, 2), (5, 'Missing: The Lucie Blackman Case', 10400000, 82, 7600000, 1), (5, 'Infinite', 8600000, 106, 4900000, 1), (5, 'Warcraft', 8500000, 123, 4100000, 3), (5, 'Inside Man: Most Wanted', 8100000, 106, 4600000, 2), (5, 'Unknown: The Lost Pyramid', 7900000, 84, 5600000, 1), (5, \"Assassin's Creed\", 5400000, 115, 2800000, 1), (5, 'Fast X', 7200000, 141, 3100000, 1), (5, 'Land of Bad', 9500000, 114, 5000000, 4), (5, 'Rise of the Guardians', 5100000, 100, 3100000, 2), (5, 'Fast &amp; Furious Presents: Hobbs &amp; Shaw', 13100000, 136, 5800000, 3), (5, 'Home', 10200000, 94, 6500000, 8), (5, 'Home', 5700000, 94, 3600000, 3), (5, 'Despicable Me 2', 8000000, 97, 4900000, 14), (5, 'Despicable Me 2', 14800000, 97, 9100000, 12), (5, 'Gladiator II', 10700000, 148, 4300000, 2), (5, 'Gladiator II', 21000000, 148, 8500000, 1), (5, 'Lost on a Mountain in Maine', 7300000, 94, 4600000, 1), (5, 'Trainwreck: P.I. Moms', 3700000, 46, 4800000, 1), (5, 'The Boss Baby', 6400000, 97, 3900000, 12), (5, 'The Boss Baby', 8500000, 97, 5200000, 11), (5, 'The Boss Baby', 9000000, 97, 5500000, 10), (5, 'Plane', 11700000, 106, 6600000, 2), (5, 'Trainwreck: The Cult of American Apparel', 7000000, 54, 7800000, 1), (5, 'Sing', 7600000, 108, 4200000, 16), (5, 'Sing', 10600000, 108, 5900000, 13), (5, 'Bee Movie', 7300000, 91, 4800000, 3), (5, 'The Super Mario Bros. Movie', 6300000, 91, 4100000, 18), (5, 'The Super Mario Bros. Movie', 11300000, 91, 7400000, 5), (5, 'The Super Mario Bros. Movie', 13700000, 91, 8900000, 3), (5, 'Mission: Impossible - Dead Reckoning', 13200000, 163, 4800000, 2), (5, 'Nonnas', 6000000, 114, 3200000, 5), (5, 'Instant Family', 10600000, 118, 5300000, 2), (5, 'Twilight', 8700000, 121, 4300000, 4), (5, 'Despicable Me 4', 9500000, 94, 6100000, 3), (5, 'Oklahoma City Bombing: American Terror', 6300000, 85, 4400000, 2), (5, 'The Croods', 6700000, 97, 4100000, 10), (5, 'The Twister: Caught in the Storm', 8100000, 88, 5500000, 2), (5, 'A Quiet Place: Day One', 9500000, 100, 5700000, 1), (5, 'Sicario', 9200000, 121, 4600000, 1), (5, 'Uncharted', 10100000, 115, 5200000, 5), (5, 'The Menu', 7800000, 106, 4400000, 2), (5, 'The Lovely Bones', 7400000, 136, 3300000, 1), (5, 'Carry-On', 7100000, 120, 3600000, 6), (5, 'You Gotta Believe', 5700000, 103, 3300000, 1), (5, \"Avicii - I'm Tim\", 8400000, 97, 5200000, 1), (5, 'Blacklight', 13300000, 105, 7600000, 1), (5, 'Disaster Holiday', 11600000, 93, 7500000, 2), (5, 'It Ends With Us', 23900000, 130, 10900000, 1), (5, 'Twas the Text Before Christmas', 11800000, 84, 8400000, 1), (5, \"Daddy's Home 2\", 9000000, 100, 5300000, 1), (5, 'Ready or Not', 12200000, 97, 7500000, 2), (5, 'Focus', 9500000, 105, 5400000, 1), (5, 'Woman of the Hour', 9100000, 96, 5700000, 3), (5, 'Kung Fu Panda 4', 9100000, 94, 5800000, 1), (5, \"IT'S WHAT'S INSIDE\", 12000000, 105, 6900000, 2), (5, 'The Garfield Movie', 8300000, 100, 4900000, 3), (5, 'Gifted', 6500000, 102, 3800000, 2), (5, 'His Three Daughters', 7200000, 105, 4100000, 1), (5, 'Incoming', 9100000, 91, 6000000, 3), (5, 'Migration', 7200000, 82, 5200000, 2), (5, 'Kingsman: The Secret Service', 10900000, 130, 5000000, 3), (5, 'Saving Bikini Bottom: The Sandy Cheeks Movie', 9000000, 87, 6200000, 3), (5, 'Find Me Falling', 8700000, 94, 5600000, 3), (5, 'Beverly Hills Cop: Axel F', 8800000, 118, 4500000, 4), (5, 'Paw Patrol: The Movie', 9100000, 85, 6300000, 5), (5, 'Wonder', 9500000, 112, 5000000, 1), (5, 'Thelma the Unicorn', 9100000, 97, 5600000, 3), (5, 'Secrets of the Neanderthals', 5700000, 79, 4300000, 2), (5, 'Hack Your Health: The Secrets of Your Gut', 7200000, 79, 5400000, 2), (5, 'Megan Leavey', 9700000, 115, 5000000, 1), (5, 'Noah', 8100000, 136, 3500000, 5), (5, 'The Postcard Killings', 8500000, 103, 4900000, 1), (5, 'Leo', 12900000, 106, 7200000, 5), (5, 'Aquaman', 16800000, 142, 7000000, 1), (5, 'Catering Christmas', 12500000, 85, 8700000, 2), (5, 'Jules', 6700000, 87, 4600000, 1), (5, 'F9: The Fast Saga', 10200000, 142, 4300000, 4), (5, 'Old Dads', 8700000, 103, 5000000, 3), (5, 'Long Shot', 8800000, 124, 4200000, 1), (5, 'Mean Girls', 6300000, 97, 3900000, 3), (5, 'The Croods: A New Age', 14100000, 94, 8900000, 2), (5, 'The Croods: A New Age', 8700000, 94, 5500000, 1), (5, 'The Deepest Breath', 8400000, 109, 4600000, 1), (5, 'Take Care of Maya', 7900000, 103, 4600000, 2), (6, 'Mother of the Bride', 5300000, 90, 3500000, 5), (6, 'Minions', 6300000, 91, 4200000, 7), (6, 'Minions', 7800000, 91, 5100000, 6), (6, 'Minions', 6100000, 91, 4000000, 3), (6, 'Orion and the Dark', 10800000, 93, 7000000, 3), (6, 'The Mother', 5400000, 117, 2800000, 6), (6, 'The Secret Life of Pets', 4600000, 87, 3200000, 9), (6, 'Scoop', 10000000, 103, 5800000, 1), (6, 'Transformers: Rise of the Beasts', 11100000, 127, 5200000, 1), (6, 'Family Switch', 11300000, 106, 6400000, 4), (6, 'American Assassin', 6700000, 112, 3600000, 4), (6, 'Miraculous: Ladybug &amp; Cat Noir, The Movie', 5900000, 102, 3500000, 4), (6, 'The Out-Laws', 8900000, 97, 5500000, 4), (6, 'Love at First Sight', 11400000, 91, 7500000, 3), (6, 'The Forever Purge', 7700000, 103, 4500000, 1), (6, 'NYAD', 9500000, 121, 4700000, 1), (6, 'Triple Frontier', 7500000, 126, 3600000, 1), (6, 'To All the Boys I\u2019ve Loved Before', 5200000, 100, 3100000, 1), (6, 'Jack Reacher: Never Go Back', 10600000, 118, 5400000, 4), (6, 'Fair Play', 7300000, 115, 3800000, 3), (6, 'Happiness for Beginners', 8900000, 106, 5000000, 3), (6, 'Upgrade', 4900000, 100, 2900000, 1), (6, 'The Silencing', 5800000, 94, 3700000, 1), (6, 'Big George Foreman: The Miraculous Story of the Once and Future Heavyweight Champion of the World', 8600000, 129, 4000000, 1), (6, 'Spider-Man: No Way Home', 10400000, 148, 4200000, 2), (6, 'Infinite', 7500000, 106, 4200000, 4), (6, 'The Wolf of Wall Street', 9700000, 180, 3200000, 1), (6, 'Lone Survivor', 8000000, 121, 4000000, 1), (6, 'American Sniper', 9700000, 133, 4400000, 1), (6, 'Fatale', 8500000, 103, 5000000, 2), (6, 'Wrath of the Titans', 5200000, 99, 3200000, 1), (6, 'Get Hard', 7400000, 100, 4400000, 1), (6, 'The Core', 10100000, 135, 4500000, 1), (6, 'Fast &amp; Furious Presents: Hobbs &amp; Shaw', 9000000, 136, 4000000, 4), (6, 'Home', 7200000, 94, 4600000, 4), (6, 'Megamind', 8900000, 97, 5400000, 2), (6, 'Despicable Me 2', 9600000, 97, 5900000, 17), (6, 'Despicable Me 2', 5500000, 97, 3400000, 15), (6, 'Freelance', 7300000, 109, 4000000, 1), (6, 'Happy Gilmore', 6200000, 91, 4000000, 1), (6, 'Trainwreck: Storm Area 51', 11300000, 97, 7000000, 1), (6, 'Madea\u2019s Destination Wedding', 7900000, 103, 4600000, 3), (6, 'Trainwreck: Poop Cruise', 6500000, 55, 7100000, 2), (6, '28 Weeks Later', 8200000, 100, 4900000, 2), (6, 'Sing', 9800000, 108, 5400000, 15), (6, 'Sing', 11100000, 108, 6200000, 12), (6, 'Sing', 6500000, 108, 3600000, 9), (6, 'Trainwreck: Mayor of Mayhem', 5300000, 49, 6400000, 1), (6, 'The Super Mario Bros. Movie', 7100000, 91, 4600000, 21), (6, 'The Super Mario Bros. Movie', 5600000, 91, 3700000, 17), (6, 'The Super Mario Bros. Movie', 5000000, 91, 3300000, 12), (6, 'The Super Mario Bros. Movie', 7000000, 91, 4600000, 7), (6, 'The Super Mario Bros. Movie', 8700000, 91, 5700000, 6), (6, 'Havoc', 6700000, 106, 3800000, 6), (6, 'Havoc', 9000000, 106, 5000000, 5), (6, 'Instant Family', 6600000, 118, 3300000, 1), (6, 'Where the Crawdads Sing', 8700000, 124, 4200000, 5), (6, 'The Life List', 6300000, 124, 3000000, 6), (6, 'Back in Action', 10800000, 114, 5700000, 5), (6, 'The Croods', 4900000, 97, 3000000, 11), (6, 'The Croods', 5200000, 97, 3200000, 7), (6, 'Bloodshot', 5600000, 109, 3100000, 5), (6, 'Kraven the Hunter', 11500000, 127, 5400000, 3), (6, 'Plankton: The Movie', 8000000, 88, 5500000, 3), (6, 'IF', 9800000, 103, 5700000, 1), (6, 'Runaway Jury', 9700000, 127, 4500000, 1), (6, 'Despicable Me', 11600000, 94, 7300000, 5), (6, 'Ma', 5800000, 99, 3500000, 1), (6, 'Sonic the Hedgehog', 9700000, 100, 5800000, 10), (6, 'Wallace &amp; Gromit: Vengeance Most Fowl', 6300000, 82, 4600000, 1), (6, 'Non-Stop', 7500000, 106, 4200000, 1), (6, 'Godzilla x Kong: The New Empire', 14100000, 115, 7400000, 1), (6, 'Our Little Secret', 11700000, 100, 7000000, 4), (6, 'Our Little Secret', 17500000, 100, 10400000, 3), (6, 'Subservience', 12500000, 105, 7100000, 1), (6, 'The Hunt', 7800000, 90, 5200000, 2), (6, 'Buy Now: The Shopping Conspiracy', 10100000, 85, 7100000, 1), (6, 'Ready or Not', 8600000, 97, 5300000, 1), (6, 'Alita: Battle Angel', 8000000, 123, 3900000, 1), (6, 'Lonely Planet', 8900000, 96, 5600000, 3), (6, 'Bad Boys: Ride or Die', 10600000, 115, 5500000, 2), (6, 'Rez Ball', 7400000, 112, 3900000, 1), (6, 'Sing 2', 13900000, 109, 7600000, 10), (6, 'The Day After Tomorrow', 11600000, 124, 5600000, 1), (6, 'T\u00f2kunb\u1ecd\u0300', 9300000, 112, 4900000, 1), (6, 'Untold: The Murder of Air McNair', 3900000, 58, 4000000, 2), (6, 'Kingsman: The Golden Circle', 11400000, 142, 4800000, 1), (6, 'Inside the Mind of a Dog', 6200000, 75, 5000000, 1), (6, 'Titanic', 11500000, 195, 3500000, 4), (6, 'Titanic', 17400000, 195, 5400000, 3), (6, 'The Long Game', 7400000, 112, 4000000, 1), (6, 'Paw Patrol: The Movie', 6400000, 85, 4500000, 6), (6, 'John Wick', 5900000, 100, 3500000, 1), (6, 'Hit Man', 8600000, 115, 4400000, 3), (6, 'Secrets of the Neanderthals', 7100000, 79, 5300000, 1), (6, 'The Judge', 8700000, 141, 3700000, 2), (6, 'Hack Your Health: The Secrets of Your Gut', 5900000, 79, 4400000, 1), (6, 'Glass', 8200000, 129, 3800000, 3), (6, 'The Bricklayer', 8500000, 109, 4600000, 1), (6, 'Shirley', 8200000, 118, 4100000, 2), (6, 'Hellboy', 7500000, 121, 3700000, 1), (6, 'The Back-Up Plan', 6000000, 103, 3500000, 1), (6, 'The Legend of Tarzan', 7600000, 109, 4100000, 2), (6, 'Leo', 12000000, 106, 6700000, 6), (6, \"Sniper: Assassin's End\", 8500000, 94, 5400000, 2), (6, 'Underwater', 9200000, 94, 5800000, 1), (6, 'Nobody', 10600000, 91, 6900000, 1), (6, 'Pain Hustlers', 10600000, 124, 5100000, 3), (6, 'Monster High: The Movie', 5700000, 88, 3800000, 1), (6, 'Mean Girls', 7300000, 97, 4500000, 2), (6, 'Snitch', 6900000, 106, 3900000, 2), (6, 'Jurassic World: Fallen Kingdom', 10100000, 127, 4700000, 1), (6, 'Ride Along', 7100000, 99, 4300000, 1), (6, 'The Mule', 9200000, 102, 5400000, 1), (7, 'Minions', 5300000, 91, 3500000, 10), (7, 'Despicable Me 3', 4800000, 90, 3200000, 5), (7, 'Top Gun: Maverick', 13400000, 130, 6200000, 1), (7, 'Lover, Stalker, Killer', 6900000, 91, 4500000, 3), (7, 'Hotel Transylvania 2', 5200000, 90, 3500000, 5), (7, 'Ice Age: Collision Course', 11700000, 97, 7200000, 1), (7, 'A Family Affair', 6200000, 114, 3300000, 5), (7, 'The Secret Life of Pets', 5600000, 87, 3900000, 5), (7, 'The Secret Life of Pets', 7300000, 87, 5000000, 4), (7, 'Ruby Gillman, Teenage Kraken', 5900000, 91, 3900000, 2), (7, 'Smurfs: The Lost Village', 5600000, 90, 3700000, 1), (7, 'Reptile', 7900000, 136, 3500000, 5), (7, 'American Assassin', 11400000, 112, 6100000, 2), (7, 'Minions: The Rise of Gru', 4200000, 88, 2900000, 10), (7, 'The Emoji Movie', 7000000, 87, 4800000, 2), (7, 'The Killer', 7100000, 120, 3600000, 4), (7, 'The Fate of the Furious', 9000000, 136, 4000000, 2), (7, 'World War Z', 7400000, 117, 3800000, 2), (7, 'Cold Pursuit', 5900000, 118, 3000000, 4), (7, 'Old', 10000000, 108, 5600000, 1), (7, 'Hotel Transylvania 3: Summer Vacation', 6400000, 97, 4000000, 10), (7, 'Hotel Transylvania 3: Summer Vacation', 10800000, 97, 6700000, 9), (7, 'Hotel Transylvania 3: Summer Vacation', 7200000, 97, 4500000, 8), (7, 'What Men Want', 6700000, 117, 3400000, 2), (7, 'Venom: Let There Be Carnage', 7800000, 97, 4800000, 1), (7, 'The Frozen Ground', 6000000, 105, 3400000, 1), (7, 'Tell Them You Love Me', 6400000, 103, 3700000, 1), (7, 'Old Henry', 8100000, 99, 4900000, 1), (7, 'Shark Bait', 5200000, 85, 3700000, 1), (7, 'What Happens in Vegas', 7600000, 99, 4600000, 2), (7, 'The Christmas Chronicles', 5900000, 105, 3400000, 6), (7, 'Ride Along 2', 6200000, 102, 3600000, 2), (7, 'Spider-Man: No Way Home (Extended Version)', 7800000, 157, 3000000, 1), (7, 'Sly', 7500000, 96, 4700000, 1), (7, 'The Two Popes', 6800000, 126, 3200000, 1), (7, 'The Protege', 11400000, 109, 6300000, 1), (7, 'Casper', 5400000, 100, 3200000, 1), (7, 'The Deep House', 4000000, 82, 2900000, 1), (7, 'Failure to Launch', 7800000, 97, 4800000, 1), (7, 'Midnight in the Switchgrass', 7500000, 99, 4500000, 1), (7, 'Home', 4300000, 94, 2700000, 7), (7, 'Elektra', 8200000, 97, 5100000, 1), (7, 'Despicable Me 2', 10000000, 97, 6100000, 13), (7, 'Despicable Me 2', 7800000, 97, 4800000, 10), (7, 'Plane', 5900000, 106, 3300000, 4), (7, 'Shark Whisperer', 9200000, 90, 6100000, 1), (7, 'Copycat', 12400000, 123, 6000000, 1), (7, 'Bee Movie', 6600000, 91, 4300000, 1), (7, 'The Super Mario Bros. Movie', 5000000, 91, 3300000, 13), (7, 'The Super Mario Bros. Movie', 5800000, 91, 3800000, 9), (7, 'Aquaman and the Lost Kingdom', 6300000, 124, 3000000, 2), (7, 'Instant Family', 6900000, 118, 3500000, 3), (7, 'A Deadly American Marriage', 7500000, 103, 4400000, 3), (7, 'It Takes Two', 6900000, 100, 4100000, 1), (7, 'The Heartbreak Kid', 5300000, 115, 2700000, 1), (7, 'Despicable Me 4', 4500000, 94, 2900000, 7), (7, 'Despicable Me 4', 7700000, 94, 4900000, 4), (7, 'One of Them Days', 4900000, 97, 3000000, 2), (7, 'Kraven the Hunter', 8800000, 127, 4200000, 4), (7, 'A Quiet Place: Day One', 7700000, 100, 4600000, 2), (7, 'La Dolce Villa', 7400000, 99, 4500000, 3), (7, 'Trial by Fire', 9700000, 127, 4600000, 1), (7, 'Despicable Me', 6900000, 94, 4400000, 7), (7, 'The Witcher: Sirens of the Deep', 7300000, 91, 4800000, 1), (7, 'The Menu', 4200000, 106, 2400000, 1), (7, 'Ma', 5300000, 99, 3200000, 3), (7, 'Sonic the Hedgehog', 4600000, 100, 2700000, 9), (7, 'Horizon: An American Saga: Chapter 1', 12000000, 181, 4000000, 1), (7, 'Our Little Secret', 11700000, 100, 7000000, 5), (7, 'Subservience', 15300000, 105, 8700000, 2), (7, 'Spellbound', 9800000, 109, 5300000, 3), (7, 'Hot Frosty', 7600000, 91, 5000000, 3), (7, 'Meet Me Next Christmas', 10400000, 106, 5900000, 3), (7, 'Time Cut', 7500000, 91, 4900000, 3), (7, 'Happiness Is', 5900000, 96, 3700000, 1), (7, 'The Predator', 9600000, 108, 5300000, 2), (7, 'The Garfield Movie', 6000000, 100, 3600000, 1), (7, 'Uglies', 5700000, 102, 3400000, 4), (7, 'Sing 2', 6200000, 109, 3400000, 12), (7, 'Sing 2', 8800000, 109, 4800000, 11), (7, 'Hitman: Agent 47', 5600000, 96, 3500000, 1), (7, 'Edge of Tomorrow', 7700000, 112, 4100000, 1), (7, 'Gemini Man', 9000000, 117, 4600000, 6), (7, 'Untold: Sign Stealer', 5200000, 88, 3500000, 1), (7, 'Night School', 9000000, 111, 4900000, 1), (7, 'The Bad Guys', 7700000, 100, 4600000, 4), (7, 'The Long Game', 9500000, 112, 5100000, 2), (7, 'Miraculous World: Shanghai, The Legend of LadyDragon', 3200000, 54, 3600000, 1), (7, 'Dracula Untold', 5100000, 91, 3300000, 2), (7, 'Safe House', 8700000, 115, 4500000, 3), (7, 'The Judge', 12500000, 141, 5300000, 1), (7, 'The Bricklayer', 5800000, 109, 3200000, 2), (7, 'Damsel', 9000000, 109, 4900000, 5), (7, 'Shirley', 7100000, 118, 3600000, 1), (7, 'On the Line', 6600000, 103, 3800000, 2), (7, 'The Vow', 6600000, 103, 3800000, 1), (7, 'Queenpins', 8100000, 109, 4400000, 1), (7, 'Leo', 7100000, 106, 4000000, 8), (7, 'Leo', 9800000, 106, 5500000, 7), (7, \"Sniper: Assassin's End\", 5400000, 94, 3400000, 1), (7, 'Holiday in the Vineyards', 10600000, 106, 5900000, 2), (7, 'Christmas as Usual', 10000000, 88, 6700000, 1), (7, 'Spider-Man: Across the Spider-Verse', 10200000, 139, 4400000, 2), (7, 'No Hard Feelings', 6600000, 103, 3800000, 1), (7, 'Spy Kids: Armageddon', 7100000, 97, 4300000, 3), (7, 'Accused', 4900000, 88, 3300000, 1), (7, 'All My Life', 4600000, 91, 3000000, 1), (7, 'Love Again', 6600000, 103, 3800000, 2), (7, \"The Pope's Exorcist\", 5400000, 103, 3100000, 3), (7, \"The Pope's Exorcist\", 6500000, 103, 3800000, 2), (7, 'Unknown: Cosmic Time Machine', 5600000, 64, 5200000, 1), (7, 'Man on Fire', 9400000, 145, 3900000, 4), (7, 'The Tutor', 5000000, 93, 3200000, 1), (7, 'The Angry Birds Movie', 4300000, 97, 2600000, 2), (8, 'Minions', 4000000, 91, 2600000, 14), (8, 'Minions', 4800000, 91, 3200000, 8), (8, 'Minions', 4500000, 91, 3000000, 1), (8, 'Despicable Me 3', 6400000, 90, 4300000, 8), (8, 'Woody Woodpecker Goes to Camp', 5400000, 100, 3200000, 5), (8, 'Shrek', 7100000, 90, 4700000, 8), (8, 'Rebel Moon \u2014 Part One: A Child of Fire', 8900000, 136, 3900000, 4), (8, 'Hotel Transylvania 2', 4300000, 90, 2900000, 6), (8, 'Extraction 2', 7800000, 124, 3800000, 6), (8, 'The Greatest Night in Pop', 8300000, 97, 5100000, 2), (8, 'Transformers: Rise of the Beasts', 7300000, 127, 3400000, 2), (8, 'A Simple Favor', 12200000, 117, 6300000, 1), (8, 'The Emoji Movie', 4200000, 87, 2900000, 3), (8, 'Sniper: Ultimate Kill', 9400000, 93, 6100000, 2), (8, 'Ultraman: Rising', 8900000, 121, 4400000, 1), (8, 'Puss in Boots: The Last Wish', 7200000, 102, 4200000, 3), (8, 'Those Who Wish Me Dead', 8600000, 100, 5200000, 1), (8, 'Ford v. Ferrari', 8300000, 153, 3300000, 1), (8, 'Just Go With It', 7100000, 117, 3600000, 4), (8, 'The Fast and the Furious', 11200000, 106, 6300000, 1), (8, 'Madagascar: Escape 2 Africa', 5800000, 91, 3800000, 1), (8, 'Jack Reacher: Never Go Back', 7400000, 118, 3800000, 2), (8, \"Tyson's Run\", 5400000, 103, 3100000, 1), (8, 'Dumb Money', 6100000, 105, 3500000, 1), (8, 'Peter Rabbit 2', 5800000, 94, 3700000, 1), (8, 'Real Steel', 6900000, 127, 3300000, 1), (8, 'Lifemark', 6100000, 105, 3500000, 1), (8, 'Norbit', 6700000, 103, 3900000, 1), (8, 'Now You See Me 2', 6300000, 129, 2900000, 1), (8, 'Shark Bait', 6200000, 85, 4400000, 2), (8, '13 Hours: The Secret Soldiers of Benghazi', 11800000, 144, 4900000, 4), (8, 'Infinite', 6900000, 106, 3900000, 2), (8, 'Jack Reacher', 9700000, 130, 4500000, 3), (8, 'They Cloned Tyrone', 7500000, 124, 3600000, 3), (8, 'Hillbilly Elegy', 9400000, 117, 4800000, 1), (8, 'Journey 2: The Mysterious Island', 5800000, 94, 3700000, 3), (8, 'Leatherface', 4300000, 88, 2900000, 1), (8, 'Scream VI', 4500000, 123, 2200000, 1), (8, 'Half Brothers', 5900000, 96, 3700000, 2), (8, \"Dr. Seuss' The Cat in the Hat\", 3300000, 82, 2400000, 2), (8, \"Dr. Seuss' The Cat in the Hat\", 5500000, 82, 4000000, 1), (8, 'Ghosts of the Abyss', 4900000, 88, 3300000, 1), (8, 'Last Vegas', 7200000, 105, 4100000, 1), (8, 'Aloha', 7600000, 105, 4300000, 1), (8, 'Home', 6100000, 94, 3900000, 6), (8, 'Kandahar', 8200000, 120, 4100000, 2), (8, 'Despicable Me 2', 7300000, 97, 4500000, 18), (8, 'Despicable Me 2', 4600000, 97, 2800000, 16), (8, 'Despicable Me 2', 5500000, 97, 3400000, 11), (8, 'Flightplan', 10300000, 99, 6200000, 1), (8, 'Rampage', 7100000, 106, 4000000, 1), (8, 'A Madea Homecoming', 5400000, 106, 3000000, 5), (8, 'The Boss Baby', 4100000, 97, 2500000, 14), (8, 'The Boss Baby', 4500000, 97, 2800000, 9), (8, 'Plane', 10300000, 106, 5800000, 3), (8, 'Grown Ups 2', 5000000, 100, 3000000, 3), (8, '28 Weeks Later', 7500000, 100, 4500000, 1), (8, 'Sing', 5800000, 108, 3200000, 10), (8, 'The Super Mario Bros. Movie', 5600000, 91, 3700000, 22), (8, 'The Super Mario Bros. Movie', 5000000, 91, 3300000, 14), (8, 'The Super Mario Bros. Movie', 5300000, 91, 3500000, 10), (8, 'The Super Mario Bros. Movie', 6500000, 91, 4200000, 8), (8, 'Beetlejuice Beetlejuice', 4600000, 105, 2600000, 2), (8, 'Despicable Me 4', 4100000, 94, 2600000, 9), (8, 'Despicable Me 4', 4500000, 94, 2900000, 8), (8, 'Despicable Me 4', 4100000, 94, 2600000, 6), (8, 'Despicable Me 4', 6400000, 94, 4100000, 5), (8, 'Lucy', 6500000, 88, 4400000, 1), (8, 'Con Mum', 5900000, 88, 4000000, 2), (8, 'Morbius', 8500000, 103, 4900000, 3), (8, 'Chaos: The Manson Murders', 5400000, 97, 3300000, 2), (8, 'Kinda Pregnant', 7100000, 100, 4300000, 3), (8, 'Despicable Me', 7300000, 94, 4600000, 6), (8, 'The Secret Life of Pets 2', 4200000, 85, 2900000, 9), (8, 'Sonic the Hedgehog', 4500000, 100, 2700000, 12), (8, 'Eye for an Eye', 7400000, 102, 4400000, 1), (8, \"Dr. Seuss' The Grinch\", 7900000, 85, 5500000, 6), (8, \"Dr. Seuss' The Grinch\", 4900000, 85, 3400000, 4), (8, \"Dr. Seuss' The Grinch\", 4100000, 85, 2900000, 3), (8, 'That Christmas', 6100000, 96, 3800000, 5), (8, 'How the Grinch Stole Christmas', 10900000, 103, 6300000, 7), (8, 'Disaster Holiday', 9300000, 93, 6000000, 1), (8, 'Terminator: Dark Fate', 12500000, 129, 5800000, 1), (8, 'A Royal Date for Christmas', 7800000, 88, 5300000, 1), (8, \"Don't Move\", 5400000, 91, 3500000, 4), (8, 'Baywatch', 7900000, 117, 4100000, 1), (8, 'The Menendez Brothers', 6400000, 118, 3300000, 3), (8, 'Unhinged', 7900000, 91, 5200000, 1), (8, \"IT'S WHAT'S INSIDE\", 5800000, 105, 3300000, 1), (8, 'Monster High 2', 6500000, 94, 4100000, 1), (8, 'The Deliverance', 5900000, 112, 3100000, 4), (8, 'The Union', 7000000, 109, 3900000, 5), (8, 'Ferdinand', 8800000, 109, 4800000, 2), (8, 'The Bad Guys', 7000000, 100, 4200000, 3), (8, \"Don't Breathe 2\", 5200000, 97, 3200000, 1), (8, 'Paw Patrol: The Movie', 3800000, 85, 2700000, 8), (8, 'Paw Patrol: The Movie', 4400000, 85, 3100000, 7), (8, 'The Equalizer', 7600000, 132, 3500000, 1), (8, 'Trigger Warning', 5900000, 106, 3300000, 4), (8, 'The Flash', 7800000, 144, 3300000, 1), (8, 'Security', 6700000, 91, 4400000, 4), (8, 'The Little Things', 7300000, 127, 3400000, 2), (8, 'Baby Driver', 8500000, 112, 4500000, 1), (8, 'Dune', 9400000, 154, 3600000, 1), (8, 'Thanksgiving', 7500000, 106, 4200000, 1), (8, 'Deep Fear', 6100000, 85, 4300000, 1), (8, 'Christmas as Usual', 7100000, 88, 4800000, 2), (8, 'Monster High: The Movie', 6900000, 88, 4700000, 2), (8, 'Tammy', 6000000, 96, 3800000, 1), (8, 'The Machine', 5700000, 112, 3100000, 1), (8, 'Crawl', 4200000, 87, 2900000, 1), (8, 'Love Again', 5100000, 103, 2900000, 1), (8, 'WHAM!', 6800000, 91, 4400000, 1), (8, 'Abduction', 4000000, 106, 2300000, 2), (9, 'Minions', 6200000, 91, 4100000, 13), (9, 'Minions', 6600000, 91, 4400000, 2), (9, 'The Equalizer 3', 6600000, 109, 3600000, 4), (9, 'Orion and the Dark', 5800000, 93, 3700000, 4), (9, 'Top Gun: Maverick', 6900000, 130, 3200000, 2), (9, 'Hotel Transylvania 2', 4200000, 90, 2800000, 4), (9, 'Spaceman', 5700000, 109, 3100000, 3), (9, 'Unfrosted', 4000000, 96, 2500000, 3), (9, 'Smurfs: The Lost Village', 7000000, 90, 4700000, 2), (9, 'The Devil Wears Prada', 7800000, 109, 4300000, 1), (9, 'American Assassin', 6500000, 112, 3500000, 3), (9, 'Minions: The Rise of Gru', 3200000, 88, 2200000, 11), (9, 'Minions: The Rise of Gru', 6100000, 88, 4200000, 9), (9, 'Puss in Boots: The Last Wish', 4500000, 102, 2600000, 6), (9, 'The Fate of the Furious', 13700000, 136, 6000000, 1), (9, 'Maze Runner: The Scorch Trials', 9700000, 132, 4400000, 1), (9, 'Nimona', 5900000, 103, 3400000, 4), (9, 'Nimona', 5500000, 103, 3200000, 1), (9, 'The Old Guard', 6700000, 126, 3200000, 1), (9, 'Bitconned', 7700000, 94, 4900000, 1), (9, 'Jurassic World: Dominion', 11200000, 147, 4600000, 1), (9, 'How to Rob a Bank', 6100000, 88, 4200000, 2), (9, 'Hotel Mumbai', 5700000, 123, 2800000, 1), (9, 'The Angry Birds Movie 2', 5400000, 97, 3300000, 2), (9, 'In Time', 5400000, 109, 3000000, 3), (9, \"A Dog's Way Home\", 4500000, 96, 2800000, 2), (9, '13 Hours: The Secret Soldiers of Benghazi', 9900000, 144, 4100000, 3), (9, 'Lone Survivor', 5500000, 121, 2700000, 2), (9, 'Hillbilly Elegy', 5900000, 117, 3000000, 2), (9, 'River Wild', 5200000, 91, 3400000, 1), (9, 'Spy Kids', 5400000, 88, 3700000, 1), (9, 'Land of Bad', 8300000, 114, 4400000, 3), (9, 'Fatale', 7100000, 103, 4100000, 3), (9, 'Untold: Hall of Shame', 3900000, 78, 3000000, 1), (9, 'The Shack', 7800000, 133, 3500000, 2), (9, 'Identity Thief', 7300000, 111, 3900000, 1), (9, 'Cold Blood Legacy', 7300000, 88, 5000000, 1), (9, 'Home', 5400000, 94, 3400000, 5), (9, 'Fixed', 3900000, 87, 2700000, 1), (9, 'Coach Carter', 8000000, 136, 3500000, 2), (9, 'Rampage', 8000000, 106, 4500000, 2), (9, 'The Old Guard 2', 6500000, 106, 3600000, 4), (9, 'Smile 2', 6100000, 127, 2900000, 1), (9, 'The Boss Baby', 3800000, 97, 2300000, 20), (9, 'The Boss Baby', 4300000, 97, 2600000, 19), (9, 'The Boss Baby', 5400000, 97, 3300000, 18), (9, 'The Boss Baby', 5000000, 97, 3100000, 16), (9, 'The Boss Baby', 4200000, 97, 2600000, 15), (9, 'The Boss Baby', 4600000, 97, 2800000, 13), (9, 'Grown Ups 2', 9400000, 100, 5600000, 2), (9, 'Penguins of Madagascar: The Movie', 4500000, 91, 2900000, 1), (9, 'The Super Mario Bros. Movie', 5000000, 91, 3300000, 11), (9, 'The Super Mario Bros. Movie', 4800000, 91, 3100000, 1), (9, 'Wonka', 6300000, 115, 3300000, 1), (9, 'Mission: Impossible - Dead Reckoning', 10600000, 163, 3900000, 3), (9, 'Pig', 3600000, 91, 2400000, 1), (9, 'Air Force Elite: Thunderbirds', 5700000, 91, 3800000, 1), (9, 'It Takes Two', 4400000, 100, 2600000, 2), (9, 'Britain and The Blitz', 4800000, 78, 3700000, 1), (9, 'Back in Action', 7700000, 114, 4100000, 6), (9, 'The Croods', 6200000, 97, 3800000, 8), (9, 'Lucy', 3800000, 88, 2600000, 2), (9, 'PAW Patrol: The Mighty Movie', 5700000, 88, 3900000, 3), (9, 'The Electric State', 8200000, 127, 3800000, 4), (9, 'Infinite Storm', 7900000, 97, 4800000, 1), (9, 'To Catch a Killer', 8000000, 118, 4000000, 2), (9, 'Dungeons &amp; Dragons: Honor Among Thieves', 8500000, 133, 3800000, 2), (9, 'Dungeons &amp; Dragons: Honor Among Thieves', 7200000, 133, 3200000, 1), (9, 'Despicable Me', 4200000, 94, 2700000, 8), (9, 'Shrek the Third', 4400000, 94, 2800000, 1), (9, 'Carry-On', 5300000, 120, 2700000, 7), (9, 'The Six Triple Eight', 8700000, 130, 4000000, 4), (9, \"Dr. Seuss' The Grinch\", 4400000, 85, 3100000, 7), (9, \"Dr. Seuss' The Grinch\", 5700000, 85, 4000000, 5), (9, 'Furiosa: A Mad Max Saga', 9300000, 148, 3800000, 1), (9, 'How the Grinch Stole Christmas', 9300000, 103, 5400000, 6), (9, 'How the Grinch Stole Christmas', 8800000, 103, 5100000, 5), (9, 'How the Grinch Stole Christmas', 8900000, 103, 5100000, 4), (9, 'Beast (English)', 9200000, 93, 5900000, 1), (9, 'The Hunt', 5500000, 90, 3700000, 1), (9, 'The Lost City', 6400000, 112, 3400000, 2), (9, 'The Lost City', 5800000, 112, 3100000, 1), (9, \"Hijack '93\", 4600000, 87, 3200000, 1), (9, 'Lonely Planet', 5000000, 96, 3100000, 4), (9, 'Hitman', 4900000, 94, 3100000, 1), (9, 'The Union', 4800000, 109, 2600000, 6), (9, 'Migration', 4000000, 82, 2900000, 3), (9, 'How to Lose a Guy in 10 Days', 5200000, 115, 2700000, 1), (9, 'Night School', 7200000, 111, 3900000, 2), (9, 'Ambulance', 9300000, 136, 4100000, 1), (9, 'Trolls Band Together', 6400000, 91, 4200000, 4), (9, 'Ghostbusters: Frozen Empire', 7200000, 115, 3800000, 2), (9, 'Paw Patrol: The Movie', 4600000, 85, 3200000, 11), (9, 'The Hard Way', 5300000, 94, 3400000, 1), (9, 'Madame Web', 11900000, 115, 6200000, 2), (9, 'The Lego Movie', 5400000, 100, 3200000, 1), (9, 'Cold Meat', 5800000, 88, 3900000, 1), (9, 'The Great Wall', 5500000, 103, 3200000, 3), (9, 'Damsel', 4600000, 109, 2500000, 6), (9, 'The Maze Runner', 6000000, 112, 3200000, 2), (9, 'On the Line', 4800000, 103, 2800000, 1), (9, 'Shooter', 4700000, 124, 2300000, 4), (9, 'Fury', 7300000, 133, 3300000, 1), (9, 'Queenpins', 6300000, 109, 3400000, 2), (9, 'Wild', 7300000, 115, 3800000, 1), (9, 'The Boss Baby: Family Business', 10500000, 106, 5900000, 1), (9, 'Pain Hustlers', 5500000, 124, 2600000, 4), (9, 'Get Out', 4100000, 103, 2400000, 1), (9, \"Mother's Day\", 5100000, 118, 2600000, 2), (9, 'Heart of Stone', 7300000, 126, 3500000, 5), (9, 'Madagascar', 4800000, 85, 3300000, 1), (9, 'The Deepest Breath', 6500000, 109, 3500000, 2), (9, 'Mafia Mamma', 4700000, 100, 2800000, 1), (9, 'Skyscraper', 7500000, 102, 4400000, 1), (10, 'Atlas', 7900000, 120, 4000000, 4), (10, 'Minions', 5000000, 91, 3300000, 9), (10, 'Despicable Me 3', 3800000, 90, 2500000, 9), (10, 'Trolls', 4400000, 93, 2800000, 7), (10, 'Extraction 2', 6000000, 124, 2900000, 7), (10, 'Chicken Run: Dawn of the Nugget', 8000000, 102, 4700000, 4), (10, 'The Devil Wears Prada', 6200000, 109, 3400000, 2), (10, 'Jack Reacher: Never Go Back', 5200000, 118, 2600000, 3), (10, 'Letters to God', 7100000, 109, 3900000, 1), (10, 'Cold Pursuit', 6700000, 118, 3400000, 3), (10, 'How to Rob a Bank', 4600000, 88, 3100000, 1), (10, 'The Black Book', 7700000, 124, 3700000, 3), (10, 'A Day to Die', 6000000, 105, 3400000, 1), (10, 'Hotel Transylvania 3: Summer Vacation', 7100000, 97, 4400000, 7), (10, 'Hotel Transylvania 3: Summer Vacation', 7900000, 97, 4900000, 6), (10, 'Venom: Let There Be Carnage', 4900000, 97, 3000000, 4), (10, 'Venom: Let There Be Carnage', 4200000, 97, 2600000, 3), (10, 'Venom: Let There Be Carnage', 6700000, 97, 4100000, 2), (10, 'Dora and the Lost City of Gold', 4900000, 102, 2900000, 2), (10, 'High Plains Drifter', 4100000, 105, 2300000, 1), (10, 'The Marine', 7600000, 91, 5000000, 1), (10, 'The Christmas Chronicles', 5300000, 105, 3000000, 7), (10, 'Jack Reacher', 7500000, 130, 3500000, 2), (10, 'Herself', 6700000, 97, 4100000, 1), (10, 'River Wild', 5900000, 91, 3900000, 2), (10, 'B&amp;B Merry', 6300000, 82, 4600000, 1), (10, 'Force of Nature', 4500000, 99, 2700000, 1), (10, 'Untold: Jake Paul the Problem Child', 3900000, 72, 3300000, 1), (10, 'Untold: Johnny Football', 3400000, 72, 2800000, 2), (10, 'The Two Popes', 4800000, 126, 2300000, 2), (10, 'Knight and Day', 4700000, 109, 2600000, 3), (10, 'Unknown: Killer Robots', 3200000, 70, 2700000, 1), (10, 'Aloha', 4600000, 105, 2600000, 2), (10, 'Kandahar', 6400000, 120, 3200000, 1), (10, 'Despicable Me 2', 4300000, 97, 2600000, 19), (10, 'Madea\u2019s Destination Wedding', 5200000, 103, 3000000, 4), (10, 'Until Dawn', 5600000, 103, 3300000, 1), (10, 'The Boss Baby', 4100000, 97, 2500000, 22), (10, 'The Boss Baby', 3800000, 97, 2300000, 21), (10, 'The Boss Baby', 6600000, 97, 4000000, 17), (10, 'The Quick and the Dead', 5000000, 105, 2900000, 1), (10, 'STRAW', 7500000, 108, 4200000, 5), (10, 'Sing', 5500000, 108, 3100000, 17), (10, 'The Valet', 8800000, 123, 4300000, 1), (10, 'The Super Mario Bros. Movie', 3600000, 91, 2300000, 20), (10, 'The Super Mario Bros. Movie', 4800000, 91, 3100000, 19), (10, 'The Super Mario Bros. Movie', 3800000, 91, 2500000, 16), (10, 'The Super Mario Bros. Movie', 3800000, 91, 2500000, 15), (10, 'Mission: Impossible - Dead Reckoning', 6700000, 163, 2500000, 4), (10, 'The Wild Robot', 4300000, 102, 2500000, 3), (10, 'The Wild Robot', 5600000, 102, 3300000, 1), (10, 'Beetlejuice Beetlejuice', 6000000, 105, 3400000, 1), (10, 'Woody Woodpecker', 6300000, 91, 4200000, 1), (10, 'Back in Action', 5000000, 114, 2600000, 7), (10, 'The Croods', 5400000, 97, 3300000, 9), (10, 'PAW Patrol: The Mighty Movie', 3700000, 88, 2500000, 4), (10, 'PAW Patrol: The Mighty Movie', 6100000, 88, 4200000, 2), (10, 'Robinson Crusoe', 5100000, 91, 3400000, 1), (10, 'Plankton: The Movie', 4800000, 88, 3300000, 4), (10, 'Trap', 7800000, 105, 4500000, 2), (10, 'Dungeons &amp; Dragons: Honor Among Thieves', 8400000, 133, 3800000, 3), (10, 'Despicable Me', 6300000, 94, 4000000, 9), (10, 'Shrek the Third', 5700000, 94, 3600000, 2), (10, 'The Sentinel', 4700000, 108, 2600000, 2), (10, 'Carry-On', 4100000, 120, 2100000, 8), (10, 'Sonic the Hedgehog', 6100000, 100, 3600000, 11), (10, 'Eye for an Eye', 4000000, 102, 2400000, 2), (10, 'In the Heart of the Sea', 7600000, 121, 3700000, 1), (10, 'The Mountain Between Us', 10600000, 112, 5700000, 1), (10, \"The Dead Don't Die\", 8600000, 105, 4900000, 2), (10, 'Buy Now: The Shopping Conspiracy', 5700000, 85, 4000000, 2), (10, 'Rob Peace', 6500000, 120, 3300000, 1), (10, 'Return of the King: The Fall and Rise of Elvis Presley', 5000000, 91, 3300000, 1), (10, 'Harold and the Purple Crayon', 5000000, 90, 3300000, 1), (10, 'Rebel Ridge', 7700000, 132, 3500000, 6), (10, 'Sing 2', 5600000, 109, 3100000, 13), (10, 'Gemini Man', 6800000, 117, 3500000, 7), (10, 'How to Lose a Guy in 10 Days', 5400000, 115, 2800000, 2), (10, 'Kingsman: The Golden Circle', 9100000, 142, 3800000, 3), (10, 'Trolls Band Together', 4900000, 91, 3200000, 5), (10, 'The Marksman', 6400000, 106, 3600000, 3), (10, 'Anyone But You', 5200000, 103, 3000000, 3), (10, 'Matilda', 4700000, 97, 2900000, 3), (10, 'Madame Web', 6200000, 115, 3200000, 4), (10, 'Madame Web', 5900000, 115, 3100000, 3), (10, 'Security', 5100000, 91, 3300000, 3), (10, 'Smile', 4700000, 115, 2400000, 1), (10, 'Barbarian', 7900000, 103, 4600000, 1), (10, 'Shrek Forever After', 5000000, 94, 3200000, 3), (10, 'Glass', 5800000, 129, 2700000, 4), (10, 'Heart of the Hunter', 5600000, 106, 3100000, 3), (10, 'Mending the Line', 6900000, 121, 3400000, 1), (10, 'Black Adam', 5700000, 124, 2700000, 1), (10, 'Noah', 6200000, 136, 2700000, 4), (10, 'Turbo', 4300000, 96, 2700000, 4), (10, 'Ballerina', 4300000, 88, 2900000, 1), (10, 'Crossroads', 5500000, 88, 3700000, 1), (10, 'Lift', 6800000, 106, 3800000, 5), (10, 'The Hill', 6800000, 127, 3200000, 2), (10, 'Leo', 5100000, 106, 2900000, 10), (10, 'Leo', 5800000, 106, 3300000, 9), (10, \"Sniper: Assassin's End\", 9100000, 94, 5700000, 3), (10, 'Holiday in the Vineyards', 6900000, 106, 3900000, 1), (10, 'See You on Venus', 4000000, 94, 2500000, 1), (10, 'Falling for Christmas', 4300000, 94, 2700000, 7), (10, 'Alvin and the Chipmunks: The Squeakquel', 3900000, 88, 2600000, 1), (10, 'Scary Movie', 5400000, 88, 3700000, 1), (10, 'Get Out', 5300000, 103, 3100000, 2), (10, 'You Are So Not Invited to My Bat Mitzvah', 4300000, 103, 2500000, 5), (10, 'Snitch', 4100000, 106, 2300000, 1), (10, 'Captain Underpants: The First Epic Movie', 4600000, 88, 3100000, 1), (10, \"Roald Dahl's Matilda The Musical\", 4900000, 121, 2400000, 6), (10, '47 Ronin', 4700000, 121, 2300000, 1), (10, 'Prisoners', 5100000, 153, 2000000, 3)]\n</pre>"},{"location":"dia3/notebooks/sql-agent/#sql-agent","title":"SQL Agent\u00b6","text":""},{"location":"dia3/notebooks/sql-agent/#load-the-required-database","title":"Load the required Database\u00b6","text":""},{"location":"dia3/notebooks/sql-agent/#defining-a-react-agent-with-sql-tools","title":"Defining a ReAct Agent with SQL tools\u00b6","text":""},{"location":"dia3/notebooks/sql-agent/#customizing-the-agent-as-a-workflow","title":"Customizing the Agent as a Workflow\u00b6","text":""},{"location":"dia3/notebooks/sql-agent/#assembling-the-graph-workflow","title":"Assembling the Graph (workflow)\u00b6","text":""},{"location":"dia3/notebooks/sql-agent/#follow-up","title":"Follow-up\u00b6","text":"<p>Perform a similar exercise with a sample of a Netflix database:</p> <p>Source: https://github.com/lerocha/netflixdb?tab=readme-ov-file</p>"}]}